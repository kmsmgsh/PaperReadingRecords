<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 14 Dirichlet-Laplace Priors for Optimal Shrinkage | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 14 Dirichlet-Laplace Priors for Optimal Shrinkage | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 14 Dirichlet-Laplace Priors for Optimal Shrinkage | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2019-10-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html">
<link rel="next" href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><i class="fa fa-check"></i><b>11</b> Bayesian Modeling of Joint Regressions for the Mean and Covariance Matrix</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#abstract-7"><i class="fa fa-check"></i><b>11.1</b> Abstract</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#introduction-9"><i class="fa fa-check"></i><b>11.2</b> Introduction</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#the-model"><i class="fa fa-check"></i><b>11.3</b> The model</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#bayesian-methodology"><i class="fa fa-check"></i><b>11.4</b> Bayesian Methodology</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#centering-and-order-methods"><i class="fa fa-check"></i><b>11.5</b> Centering and Order Methods</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><i class="fa fa-check"></i><b>12</b> MAXIMUM LIKELIHOOD ESTIMATION IN TRANSFORMED LINEAR REGRESSION WITH NONNORMAL ERRORS</a><ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#abstract-8"><i class="fa fa-check"></i><b>12.1</b> Abstract</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#introduction-10"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#parameter-estimation-and-inference-procedure"><i class="fa fa-check"></i><b>12.3</b> Parameter estimation and inference procedure</a><ul>
<li class="chapter" data-level="12.3.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#mle"><i class="fa fa-check"></i><b>12.3.1</b> MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><i class="fa fa-check"></i><b>13</b> Homogeneity tests of covariance matrices with high-dimensional longitudinal data</a><ul>
<li class="chapter" data-level="13.1" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#introduction-background"><i class="fa fa-check"></i><b>13.1</b> Introduction &amp; Background</a></li>
<li class="chapter" data-level="13.2" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#basic-setting"><i class="fa fa-check"></i><b>13.2</b> Basic setting</a></li>
<li class="chapter" data-level="13.3" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#homogeneity-tests-of-covariance-matrices"><i class="fa fa-check"></i><b>13.3</b> Homogeneity tests of covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html"><i class="fa fa-check"></i><b>14</b> Dirichlet-Laplace Priors for Optimal Shrinkage</a><ul>
<li class="chapter" data-level="14.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#abstract-9"><i class="fa fa-check"></i><b>14.1</b> Abstract</a></li>
<li class="chapter" data-level="14.2" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#introduction-11"><i class="fa fa-check"></i><b>14.2</b> Introduction</a></li>
<li class="chapter" data-level="14.3" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#a-new-class-of-shrinkage-priors"><i class="fa fa-check"></i><b>14.3</b> A NEW CLASS OF SHRINKAGE PRIORS:</a><ul>
<li class="chapter" data-level="14.3.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#bayesian-sparsity-priors-in-normal-means-problem"><i class="fa fa-check"></i><b>14.3.1</b> Bayesian Sparsity Priors in Normal Means Problem</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#global-local-shrinkage-rules"><i class="fa fa-check"></i><b>14.4</b> Global-Local shrinkage Rules</a></li>
<li class="chapter" data-level="14.5" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#dirichlet-kernel-priors"><i class="fa fa-check"></i><b>14.5</b> Dirichlet-Kernel Priors</a></li>
<li class="chapter" data-level="14.6" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#posterior-computation"><i class="fa fa-check"></i><b>14.6</b> Posterior Computation</a></li>
<li class="chapter" data-level="14.7" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#concentration-properties-of-dirichlet-laplace-priors"><i class="fa fa-check"></i><b>14.7</b> Concentration properties of dirichlet-laplace priors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><i class="fa fa-check"></i><b>15</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a><ul>
<li class="chapter" data-level="15.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#abstract-10"><i class="fa fa-check"></i><b>15.1</b> Abstract</a></li>
<li class="chapter" data-level="15.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#introduction-12"><i class="fa fa-check"></i><b>15.2</b> Introduction</a></li>
<li class="chapter" data-level="15.3" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#the-model-and-prior"><i class="fa fa-check"></i><b>15.3</b> The model and prior:</a><ul>
<li class="chapter" data-level="15.3.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#likelihood"><i class="fa fa-check"></i><b>15.3.1</b> Likelihood</a></li>
<li class="chapter" data-level="15.3.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#prior-specifiction"><i class="fa fa-check"></i><b>15.3.2</b> Prior specifiction</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#inference-and-simulation-method"><i class="fa fa-check"></i><b>15.4</b> Inference and Simulation method</a><ul>
<li class="chapter" data-level="15.4.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#bayesian-inference."><i class="fa fa-check"></i><b>15.4.1</b> Bayesian inference.</a></li>
<li class="chapter" data-level="15.4.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>15.4.2</b> Markov Chain Monte Carlo Sampling</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#simulation-analysis"><i class="fa fa-check"></i><b>15.5</b> Simulation Analysis</a></li>
<li class="chapter" data-level="15.6" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#real-data-analysis"><i class="fa fa-check"></i><b>15.6</b> Real data analysis</a></li>
<li class="chapter" data-level="15.7" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#conclusion"><i class="fa fa-check"></i><b>15.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><i class="fa fa-check"></i><b>16</b> Modeling local dependence in latent vector autoregressive models</a><ul>
<li class="chapter" data-level="16.1" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html#section-16.1"><i class="fa fa-check"></i><b>16.1</b> 英文摘要简介</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><i class="fa fa-check"></i><b>17</b> BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R</a><ul>
<li class="chapter" data-level="17.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#movel-averaged-estimations-and-predictions"><i class="fa fa-check"></i><b>17.1</b> 6 Movel averaged estimations and predictions</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#estimation"><i class="fa fa-check"></i><b>17.1.1</b> 6.1 Estimation</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#prediction"><i class="fa fa-check"></i><b>17.1.2</b> 6.2 Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><i class="fa fa-check"></i><b>18</b> Criteria for Bayesian Model Choice With Application to Variable Selection</a><ul>
<li class="chapter" data-level="18.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#abstract-11"><i class="fa fa-check"></i><b>18.1</b> Abstract:</a></li>
<li class="chapter" data-level="18.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-13"><i class="fa fa-check"></i><b>18.2</b> Introduction</a><ul>
<li class="chapter" data-level="18.2.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#background"><i class="fa fa-check"></i><b>18.2.1</b> Background</a></li>
<li class="chapter" data-level="18.2.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#notation."><i class="fa fa-check"></i><b>18.2.2</b> Notation.</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#criteria-for-objective-model-selection-priors."><i class="fa fa-check"></i><b>18.3</b> Criteria for objective model selection priors.</a><ul>
<li class="chapter" data-level="18.3.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#basic-criteria"><i class="fa fa-check"></i><b>18.3.1</b> Basic criteria:</a></li>
<li class="chapter" data-level="18.3.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#consistency-criteria"><i class="fa fa-check"></i><b>18.3.2</b> Consistency criteria</a></li>
<li class="chapter" data-level="18.3.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#predictive-matching-criteria."><i class="fa fa-check"></i><b>18.3.3</b> 2.4 Predictive matching criteria.</a></li>
<li class="chapter" data-level="18.3.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#invariance-criteria"><i class="fa fa-check"></i><b>18.3.4</b> Invariance criteria</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#objective-prior-distributions-for-variable-selection-in-normal-linear-models."><i class="fa fa-check"></i><b>18.4</b> Objective prior distributions for variable selection in normal linear models.</a><ul>
<li class="chapter" data-level="18.4.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-14"><i class="fa fa-check"></i><b>18.4.1</b> Introduction</a></li>
<li class="chapter" data-level="18.4.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#proposed-prior-the-robust-prior"><i class="fa fa-check"></i><b>18.4.2</b> Proposed prior (the “robust prior”)</a></li>
<li class="chapter" data-level="18.4.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#choosing-the-hyperparameters-for-p_irg"><i class="fa fa-check"></i><b>18.4.3</b> Choosing the hyperparameters for <span class="math inline">\(p_i^R(g)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#section-18.5"><i class="fa fa-check"></i><b>18.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><i class="fa fa-check"></i><b>19</b> Objective Bayesian Methods for Model Selection: Introduction and Comparison</a><ul>
<li class="chapter" data-level="19.0.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#the-fractional-bayes-factor-fbf-approach"><i class="fa fa-check"></i><b>19.0.1</b> 2.3 The Fractional Bayes Factor (FBF) Approach</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><i class="fa fa-check"></i><b>20</b> A Review of Bayesian Variable Selection Methods: What, How and Which</a><ul>
<li class="chapter" data-level="20.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#abstract-12"><i class="fa fa-check"></i><b>20.1</b> Abstract</a></li>
<li class="chapter" data-level="20.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#introduction-15"><i class="fa fa-check"></i><b>20.2</b> Introduction</a><ul>
<li class="chapter" data-level="20.2.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#notation"><i class="fa fa-check"></i><b>20.2.1</b> Notation:</a></li>
<li class="chapter" data-level="20.2.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#concepts-and-properties"><i class="fa fa-check"></i><b>20.2.2</b> Concepts and Properties</a></li>
<li class="chapter" data-level="20.2.3" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#variable-selection-methods"><i class="fa fa-check"></i><b>20.2.3</b> Variable Selection Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dirichlet-laplace-priors-for-optimal-shrinkage" class="section level1">
<h1><span class="header-section-number">Paper 14</span> Dirichlet-Laplace Priors for Optimal Shrinkage</h1>
<p><span class="citation">(Bhattacharya et al. <a href="#ref-Bhattacharya:2015cq">2015</a>)</span></p>
<div id="abstract-9" class="section level2">
<h2><span class="header-section-number">14.1</span> Abstract</h2>
<p>Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated continuous shrinkage priors.</p>
<p>This article proposed a new class of Dirichlet-Laplace priors relative to alternatives is assessed in simulated and real data example.</p>
</div>
<div id="introduction-11" class="section level2">
<h2><span class="header-section-number">14.2</span> Introduction</h2>
<p>However, in many applications, it is crucial to obtain a realistic characterization of uncertainty in estimates of parameters, function of parameters, and predictions.
Frequentist approaches using constructing asymptotic confidence regions or using the bootstrap to characterize uncertainty. But these may be broken down in high-dimensional settings.</p>
<p>Example: In regression, one cannot naively appeal to asymptotic normality and resampling from the data when p is equal or larger than n. Resampling of data may not provide an adequate characterization of uncertainty.</p>
<p>Most penalized estimators correspond to the mode of a Bayesian posterior distribution. Lasso is equivelant to a MAP estimation under a Gaussian linear regression model with double exponential (Laplace) prior.</p>
<p>A natural question is whether we can use the entire posterior distribution to provide a probabilistic measure of uncertainty.</p>
<p>Bayesian perspective also have distinct advantages in terms of tuning parameter choice. In this case, the key tuning parameters can be marginalized over the posterior distribution instead of relying on cross-validation.</p>
<p>“However, instead of showing that a penalized estimator obtains the minimax rate under sparsity assumption, we would like to show that the entire posterior distribution concentrates at the optimal rate, that is, the posterior probability assigned to a shrinking neighborhood of the true parameter value converges to one, with the neighborhood size proportional to the requentist minimax rate.”</p>
<p>An amazing variety of shrinkage priors has been proposed in the Bayesian literature.</p>
<ul>
<li><p>Ghosal(1999),Bontemps(2011): Provided conditions on the prior for asymptotic normality of linear regression coefficients allowing the number of predictors p to increase with sample size n. Ghosal approach requiring a very slow rate of growth. Bontemps assuming <span class="math inline">\(p\leq n\)</span>.</p></li>
<li><p>Armagan, Dunson, and Lee(2013) considered shrinkage priors in providing simple sufficient conditions for posterior consistency in linear regression where the number of variables grows slower than the sample size, though no rate of contraction was provided.</p></li>
</ul>
<p>Studying “posterior contraction” in high-dimensions, …
Studying these properties of shrinkage pirors is challenging due to the lack of exact zeros, with the prior draws being sparse in only an approximate sense.</p>
<p>Prior selection remains an art.</p>
<p>The overarching goal is to obtain theory allowing design of novel priors, which are appealing from a Bayesian perspective while having frequentist optimality properties.</p>
</div>
<div id="a-new-class-of-shrinkage-priors" class="section level2">
<h2><span class="header-section-number">14.3</span> A NEW CLASS OF SHRINKAGE PRIORS:</h2>
<div id="bayesian-sparsity-priors-in-normal-means-problem" class="section level3">
<h3><span class="header-section-number">14.3.1</span> Bayesian Sparsity Priors in Normal Means Problem</h3>
<p>For concreteness (?), this paper focus on the normal means problem. (not the generalized model like GLM).</p>
<p>Based on a single observation corrupted with iid standard normal noise:
<span class="math display">\[
y_{i}=\theta_{i}+\epsilon_{i}, \quad \epsilon_{i} \sim \mathrm{N}(0,1), \quad 1 \leq i \leq n
\]</span></p>
<p>Let <span class="math inline">\(\ell_0[q;n]\)</span> denote the subset of <span class="math inline">\(\mathbb R^n\)</span> given by
<span class="math display">\[
l_{0}[q ; n]=\left\{\theta \in \mathbb{R}^{n} : \#\left(1 \leq j \leq n : \theta_{j} \neq 0\right) \leq q\right\}
\]</span>
<span class="math inline">\(l_0[q;n]\)</span> 是<span class="math inline">\(\mathbb R^n\)</span> 的子集，这个#是啥，子集中非零元的个数吗。</p>
<p>A vector <span class="math inline">\(x\in \mathbb R^r\)</span>, let <span class="math inline">\(||x||_2\)</span> is Euclidean norm. If <span class="math inline">\(\theta_0\)</span> is <span class="math inline">\(q_n\)</span>-sparse, that is, <span class="math inline">\(\theta_0\in l_0[q_n;n]\)</span>, with <span class="math inline">\(q_n=o(n)\)</span>, the <em>squared minimax rate in estimating <span class="math inline">\(\theta_0\)</span> in <span class="math inline">\(\ell_2\)</span> norm is <span class="math inline">\(2 q_{n} \log \left(n / q_{n}\right)(1+o(1))\)</span></em>. (Donoho et al. 1992);</p>
<blockquote>
<p>Squared minimax rate是啥？</p>
</blockquote>
<p>(given sequences <span class="math inline">\(a_n\)</span>,<span class="math inline">\(b_n\)</span>, we denote <span class="math inline">\(a_n=O(b_n)\)</span> or <span class="math inline">\(a_{n} \lesssim b_{n}\)</span> if there exists a global constant <span class="math inline">\(C\)</span> such that <span class="math inline">\(a_{n} \leq C b_{n}\)</span> and <span class="math inline">\(a_n=o(b_n)\)</span> if <span class="math inline">\(a_n/b_n\rightarrow0\)</span>) as <span class="math inline">\(n\rightarrow 0\)</span></p>
<p><span class="math display">\[
\inf _{\hat{\theta}} \sup _{\theta_{0} \in l_{0}\left[q_{n} ; n\right]} E_{\theta_{0}}\left\|\hat{\theta}-\theta_{0}\right\|_{2}^{2} \asymp q_{n} \log \left(n / q_{n}\right)
\]</span>
In the above display, <span class="math inline">\(E_{\theta_0}\)</span> denotes an expectation with respect to an <span class="math inline">\(N_n(\theta_0,I_n)\)</span> density.</p>
<blockquote>
<p>这玩意的意义没看懂。</p>
</blockquote>
<p>For a subset <span class="math inline">\(S \subset\{1, \ldots, n\}\)</span>, let <span class="math inline">\(|S|\)</span> denote the cardinality of <span class="math inline">\(S\)</span> and define <span class="math inline">\(\theta_{S}=\left(\theta_{j} : j \in S\right)\)</span> for a vector <span class="math inline">\(\theta\in \mathbb R^n\)</span>. Denote <span class="math inline">\(supp(\theta)\)</span> to be the <em>support</em> of <span class="math inline">\(\theta\)</span>, the subset of <span class="math inline">\(\{1, \ldots, n\}\)</span> corresponding to the nonzero entries of <span class="math inline">\(\theta\)</span>. For a vector <span class="math inline">\(\theta \in \mathbb R^n\)</span>, a nature way to incorporate sparsity is to use <strong>point mass mixture priors</strong>:
<span class="math display">\[
\theta_{j} \sim(1-\pi) \delta_{0}+\pi g_{\theta}, \quad j=1, \ldots, n
\]</span>
where <span class="math inline">\(\pi=\operatorname{Pr}\left(\theta_{j} \neq 0\right)\)</span>, <span class="math inline">\(\mathbb{E}\{|\operatorname{supp}(\theta)| | \pi\}=n \pi\)</span> is the prior guess on model size (sparsity level), and <span class="math inline">\(g_\theta\)</span> is an absolutely continuous density on <span class="math inline">\(\mathbb R\)</span>.</p>
<p>These priors are highly appealing in allowing separate control of the level of sparsity and the size of the signal coefficients. If the sparsity parameter <span class="math inline">\(\pi\)</span> is estimated via empirical Bayes the posterior median of <span class="math inline">\(\theta\)</span> is estimated via empirical Bayes, the posterior median of <span class="math inline">\(\theta\)</span> is a minimax-optimal estimator, which can adapt to arbitrary sparsity levels as long as <span class="math inline">\(q_n=o(n)\)</span>.</p>
<blockquote>
<p>反正就是说这个先验很好的意思？ 能满足一些性质，而且同样的，<span class="math inline">\(\lambda\)</span>可以用empirical bayes选</p>
</blockquote>
<p>In fully bayesian framework, it is common to place a beta prior on <span class="math inline">\(\pi\)</span>, leading to a beta-Bernoulli prior on the model size, which conveys an automatic multiplicity adjustment.</p>
<blockquote>
<p>multiplicity adjustment这又是啥</p>
</blockquote>
<p>In recent article, Castillo and van der Vaart (2012) established that prior with an appropriate beta prior on <span class="math inline">\(\pi\)</span> and suitable tail conditions on <span class="math inline">\(g_\theta\)</span> leads to a minimax optimal rate of <span class="math inline">\(posteiror contraction\)</span>.</p>
<blockquote>
<p>posterior contraction 是啥?</p>
</blockquote>
<p>That is, the posterior concentrates most of its mass on a ball around <span class="math inline">\(\theta_0\)</span> of squared radius of the order of <span class="math inline">\(q_n log (n/q_n)\)</span>:</p>
<p><span class="math display">\[
E_{\theta_{0}} \mathbb{P}\left(\left\|\theta-\theta_{0}\right\|_{2}&lt;M s_{n} | y\right) \rightarrow 1, \text { as } n \rightarrow \infty
\]</span>
.</p>
<p>Narisetty et al.(2014) obtained consistency in model selection using point mass mixture priors with appropriate data-driven hyperparameters.</p>
</div>
</div>
<div id="global-local-shrinkage-rules" class="section level2">
<h2><span class="header-section-number">14.4</span> Global-Local shrinkage Rules</h2>
<p>Although point mass mixture priors are intuitively appealing and possess attractive theoretical properties, posterior sampling requires a stochastic search over an enormous space, leading to slow mixing and convergence. Thus, continuous shrinkage priors and promoted by several authors. [<strong>literatues</strong>]</p>
<p>Polson and Scott (2010) noted that essentially all such shrinkage priors can be represented as global-local (GL) mixtures of Gaussians,
<span class="math display">\[
\theta_{j} \sim \mathrm{N}\left(0, \psi_{j} \tau\right), \quad \psi_{j} \sim f, \quad \tau \sim g
\]</span></p>
<blockquote>
<p>Bayes lasso确实就是这种的一个特殊形式</p>
</blockquote>
<p><span class="math inline">\(\tau\)</span> controls global shrinkage toward the origin while the local scales <span class="math inline">\(\{\psi_j\}\)</span> allow deviations in the degree of shrinkage. This can be an approximate point mass priors through a continuous density concentrated near zero with heavy tails.</p>
<p>GL priors are more convenient in computation compare to point mass priors. Because the Normal priors allows for conjugate updating for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\psi\)</span> in a block. The frequentist regularization procedures correspond to posterior modes under GL priors with appropriate choice of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>.</p>
<p>However, many aspects of shrinkagepriors are poorly understood, with the lack of exact zeros compounding the difficulty in studying basic properties, such as prior expectation, tail bounds for the number of large signals, and prior concentration around sparse vectors.</p>
<p>Hence, subjective Bayesian face difficulties in incorporating prior information regarding sparsity, and frequentists tend to be skeptical due to the lack of theoretical justification.</p>
<p>This is warranted, reasonable seeming priors can have poor performance in high dimensional settings.</p>
<blockquote>
<p>这段话没看懂：</p>
</blockquote>
<p>For example, choosing <span class="math inline">\(\pi=\frac{1}{2}\)</span> in point mass mixture priors leads to an exponentially small prior probability of <span class="math inline">\(2^{-n}\)</span> assigned to the null model, so that it becomes literally impossible to override that prior informativeness with the information in the data to pick the null model.</p>
<blockquote>
<p>也就是说，如果取<span class="math inline">\(\pi=\frac{1}{2}\)</span>,那么就有<span class="math inline">\(\theta_j\sim \frac{1}{2}\delta_0+\frac{1}{2}g_\theta\)</span>,但是这个为啥是会leads to an exponential small prior probability of <span class="math inline">\(2^{-n}\)</span>？ 感觉对这个point mass mixture prior理解不够深入。两个最简单的问题，<span class="math inline">\(\delta_0\)</span> 和<span class="math inline">\(g_\theta\)</span>应该如何选取</p>
</blockquote>
<p>However, with a beta prior on <span class="math inline">\(\pi\)</span>, this problem can be avoided. In the same vein, if one places iid <span class="math inline">\(N(0,1)\)</span> priors on the entries of <span class="math inline">\(\theta\)</span>, then the induced prior on <span class="math inline">\(\|\theta\|\)</span> is highly concentrated around <span class="math inline">\(\sqrt n\)</span> leading to mmisleadin ginferences on <span class="math inline">\(\theta\)</span> almost everywhere. Although these are simple examples, similar multiplicity problems can transpire more subtly in cases where complicated models/prios are involved and hence it is fundmentally important to understand properties of the prior and the posterior in the setting of the normal mean model.</p>
<p>There has been a recent awareness of these issues, motivating a basic assessment of the marginal properties of shrinkage priors for a single <span class="math inline">\(\theta_j\)</span>. Recent priors such as the horseshoe and generalized double Pareto are carefully formulated to obtain marginals having a high concentration around zero with heavy tails. This is well justified,but as we will se below, such marginal behavior alone is not sufficient; it is necessary to study the joint distribution of <span class="math inline">\(\theta\)</span> on <span class="math inline">\(\mathbb R^n\)</span>. With such motivation, we propose a class of Dirichlet-kernel priors in the next subsection.</p>
</div>
<div id="dirichlet-kernel-priors" class="section level2">
<h2><span class="header-section-number">14.5</span> Dirichlet-Kernel Priors</h2>
<p>Let <span class="math inline">\(\phi_0\)</span> denote the standard normal density on <span class="math inline">\(\mathbb R\)</span>. Also, let <span class="math inline">\(DE(\tau)\)</span> denote a zero mean double-exponential or Laplace distribution or Laplace distribution with density <span class="math inline">\(f(y)=(2 \tau)^{-1} e^{-|y| / \tau}\)</span> for <span class="math inline">\(y\in \mathbb R\)</span>. Integrating out the local scales <span class="math inline">\(\psi_i\)</span>’s, the global and local prior can be equivalently represented as a global scale mixture of a kernel <span class="math inline">\(\mathcal{K}(\cdot)\)</span>,
<span class="math display">\[
\theta_{j} \stackrel{\mathrm{iid}}{\sim} \mathcal{K}(\cdot, \tau), \quad \tau \sim g
\]</span>
where <span class="math inline">\(\mathcal{K}(x)=\int \psi^{-1 / 2} \phi_{0}(x / \sqrt{\psi}) g(\psi) d \psi\)</span> is a symmetrix unimodal density on <span class="math inline">\(\mathbb R\)</span> and <span class="math inline">\(\mathcal{K}(x, \tau) :=\tau^{-1 / 2} \mathcal{K}(x / \sqrt{\tau})\)</span>. For example, <span class="math inline">\(\psi_j\sim Exp(1/2)\)</span> corresponds to a double-exponential kernel <span class="math inline">\(\mathcal{K} \equiv \mathrm{DE}(1)\)</span>, while <span class="math inline">\(\psi_{j} \sim \mathrm{IG}(1 / 2,1 / 2)\)</span> results in a standard Cauchy kernel <span class="math inline">\(\mathcal{K} \equiv \mathrm{Ca}(0,1)\)</span>.</p>
<blockquote>
<p>上面这几个等价得检查研究一下</p>
</blockquote>
<p>These choices lead to a kernel that is bounded in a neighborhood of zero.</p>
<p>However, if one instead uses a half Cauchy prior <span class="math inline">\(\psi_j^{\frac{1}{2}}\sim Ca_{+}(0,1)\)</span>, then the resulting horseshoe kernel (Carvalho,Polson, and Scott 2009,2010) is unbounded with a singularity at zero. This phenomenon coupled with tail robustness properties leads to excellent empirical performance of the horseshoe.
However, the joint distribution of <span class="math inline">\(\theta\)</span> under a horseshoe prior is understudied and further theoretical investigation is required to understand its operating characteristics. One can imagine that it concentrates more along sparse regions of the parameter space compared to common shrinkage priors isnce the singularity at zero potentially allows most of the entries to be concentrated around zero with the heavy tails ensuring concentration around the relatively small number of signals.</p>
<p>The above class of priors rely on obtaining a suitable kernel <span class="math inline">\(\mathcal K\)</span> through appropriate normal scale mixtures. In this article, alleviate the requirements on the kernel, while having attractive theoretical properties. In particular, our proposedclass of Dirichlet-kernel (Dk) priors replaces the single global scale <span class="math inline">\(\tau\)</span> by a vector scales <span class="math inline">\((\phi_1\tau,...\phi_n\tau)\)</span>, where <span class="math inline">\(\phi=(\phi_1,...,\phi_n)\)</span> is constrained to lie in the <span class="math inline">\((n-1)\)</span>-dimensional simplex <span class="math inline">\(\mathcal S^{n-1}=\{x=(x_1,...,x_n)^T:x_j\geq 0,\sum_{j=1}^n x_j=1\}\)</span> and is assigned a <span class="math inline">\(Dir(a,...,a)\)</span> prior:
<span class="math display">\[
\theta_{j} | \phi_{j}, \tau \sim \mathcal{K}\left(\cdot, \phi_{j} \tau\right), \quad \phi \sim \operatorname{Dir}(a, \ldots, a)
\]</span></p>
<p><span class="math inline">\(\mathcal K\)</span> is any symmetric (about zero) unimodal density with exponential or heavier tails; for computational purposes, we restrict attention to the class of kernels that can be represented as scale mixture of normals. While previous shrinkage priors obtain marginal behavior similar to the point mass mixture priors, our construction aims at resembling the joint distribution of <span class="math inline">\(\theta\)</span> under a two-component mixture prior.</p>
<blockquote>
<p>没太懂这个kernel是啥，然后kernel的重要性在哪里。 然后那个积分感觉怪怪的，需要更细致的讲解。 然后这个用Dirichlet分布放在tau上。有点奇怪，需要更深的解释。</p>
</blockquote>
<p>Focus on the Laplace kernel from now on for concreteness, noting that all the results stated below can be generalized to other choices. The corresponding hierachical prior given <span class="math inline">\(\tau\)</span>.,</p>
<p><span class="math display">\[
\theta_{j} | \phi, \tau \sim \operatorname{DE}\left(\phi_{j} \tau\right), \quad \phi \sim \operatorname{Dir}(a, \ldots, a)
\]</span>
is referred to as a Dirichlet-Laplace prior, denoted <span class="math inline">\(\theta|\tau\sim DL_a(\tau)\)</span>.</p>
<p>To understand the role of <span class="math inline">\(\phi\)</span>, we undertake a study of the marginal properties of <span class="math inline">\(\theta_j\)</span> conditional on <span class="math inline">\(\tau\)</span>, integrating out <span class="math inline">\(\phi_j\)</span>. The results are summarized in Proposition 2.1.</p>
<p>Proposition 2.1. If <span class="math inline">\(\theta | \tau \sim \operatorname{DL}_{a}(\tau)\)</span>, then the marginal distribution of <span class="math inline">\(\theta_j\)</span> given <span class="math inline">\(\tau\)</span> is unbounded with a singularity at zero for any <span class="math inline">\(a&lt;1\)</span>. Further, in the special case <span class="math inline">\(a=1/n\)</span>, the marginal distribution is a wrapped Gamma distribution <span class="math inline">\(WG(\tau^{-1},1/n)\)</span>, where <span class="math inline">\(WG(\lambda,\alpha)\)</span> has a density <span class="math inline">\(f(x ; \lambda, \alpha) \propto|x|^{\alpha-1} e^{-\lambda|x|}\)</span> on <span class="math inline">\(\mathbb R\)</span>.</p>
<p>Thus,marginalizing over <span class="math inline">\(\phi\)</span>, we obtain an unbounded kernel <span class="math inline">\(\mathcal K\)</span>, so that the marginal density of <span class="math inline">\(\theta_j|\tau\)</span> has singularity at 0 while retaining exponential tails.</p>
<p>The parameter <span class="math inline">\(\tau\)</span> plays a critical role in determining the tails of the marginal distribution of <span class="math inline">\(\theta_j\)</span>’s. We consider a fully Bayesian framework where <span class="math inline">\(\tau\)</span> is assigned a prior <span class="math inline">\(g\)</span> on the positive real line and learnt from the data through the posterior. Specifically, we assume a <span class="math inline">\(gamma(\lambda,1/2)\)</span> prior on <span class="math inline">\(\tau\)</span> with <span class="math inline">\(\lambda=na\)</span>. We continue to refer to the induced prior on <span class="math inline">\(\theta\)</span> implied by the hierarchical structure,
<span class="math display">\[
\begin{array}{l}{\theta_{j} | \phi, \tau \sim \operatorname{DE}\left(\phi_{j} \tau\right), \quad \phi \sim \operatorname{Dir}(a, \ldots, a), \quad \tau \sim}  {\operatorname{gamma}(n a, 1 / 2)}\end{array}
\]</span>
as a Dirichlet-Laplace prior, denoted <span class="math inline">\(\theta\sim DL_a\)</span>.</p>
<p>There is a frequentist literature on including a local penalty specific to each coefficient. The adaptive lasso relies on empirically estimated weights that are plugged in. Leng instead sampled the penalty parameters from a posterior, with a sparse point estimate obtained for each draw. These approaches do not produce a full posterior distribution but focus on sparse point estimates.</p>
</div>
<div id="posterior-computation" class="section level2">
<h2><span class="header-section-number">14.6</span> Posterior Computation</h2>
<p>The proposed class ofo DL prior leads to straightforward posterior computation via an efficient data augmented Gibbs sampler. The <span class="math inline">\(DL_a\)</span> prior can be equivalently represented as</p>
<p><span class="math display">\[
\begin{array}{l}{\theta_{j} \sim \mathrm{N}\left(0, \psi_{j} \phi_{j}^{2} \tau^{2}\right), \psi_{j} \sim \operatorname{Exp}(1 / 2), \phi \sim \operatorname{Dir}(a, \ldots, a)}  {,\tau \sim \operatorname{gamma}(n a, 1 / 2)}\end{array}
\]</span></p>
<p>We detail the steps in the normal means setting noting that the algorithm is trivially modified to accomodate normal linear regression, robust regression with heavy tailed residuals, probit models, logistic regression, factor models, and other hierarchical Gaussian cases. To reduce auto-correlation, we rely on marginalization and blocking as much as possible. Our sampler cycles through <span class="math inline">\(\text { (i) } \theta | \psi, \phi, \tau, y,(\text { ii) } \psi | \phi, \tau, \theta, \text { (iii) } \tau | \phi, \theta, \text { and (iv) } \phi|\theta\)</span>. We use the fact that the joint posterior of <span class="math inline">\((\psi, \phi, \tau)\)</span> is conditionally independent of <span class="math inline">\(y\)</span> given <span class="math inline">\(\theta\)</span>. Steps (ii)-(iv) together give us a draw from the conditional distribution of <span class="math inline">\((\psi, \phi, \tau) | \theta\)</span> since
<span class="math display">\[
[\psi, \phi, \tau | \theta]=[\psi | \phi, \tau, \theta][\tau | \phi, \theta][\phi | \theta]
\]</span>
Steps (i)-(iii) are standard and hence not derived. Step (iv) is nontrivial and we develop an efficient sampling algorithm for jointly sampling <span class="math inline">\(\phi\)</span>. Usual one at a time updates of a Dirichlet vector lead to tremendously slow mixing and convergence, and hence the joint updaate in Theorem 2.1 is an important feature of our proposed prior. Consider the following parameterization for the three-parameter generalized inverse Gaussian (giG) distribution:
<span class="math inline">\(Y \sim \operatorname{giG}(\lambda, \rho, \chi)\)</span> if <span class="math inline">\(f(y) \propto y^{\lambda-1} e^{-0.5(\rho y+\chi / y)}\)</span> for <span class="math inline">\(y&gt;0\)</span>.</p>
<p>Theorem 2.1. The joint posterior of <span class="math inline">\(\phi|\theta\)</span> has the same distribution as <span class="math inline">\(\left(T_{1} / T, \ldots, T_{n} / T\right)\)</span>, where <span class="math inline">\(T_j\)</span> are independently distributed according to a <span class="math inline">\(giG(a01,1,2|\theta_j|)\)</span> distribution, and <span class="math inline">\(T=\sum_{j=1}^{n} T_{j}\)</span>.</p>
<p>Summaries of each step are provided below:</p>
<ol style="list-style-type: decimal">
<li><p>To sample <span class="math inline">\(\theta | \psi, \phi, \tau, y\)</span>, draw <span class="math inline">\(\theta_j\)</span> independently from an <span class="math inline">\(N(\mu_j,\sigma_j^2)\)</span> distribution with
<span class="math display">\[\sigma_{j}^{2}=\left\{1+1 /\left(\psi_{j} \phi_{j}^{2} \tau^{2}\right)\right\}^{-1}, \quad \mu_{j}=\left\{1+1 /\left(\psi_{j} \phi_{j}^{2} \tau^{2}\right)\right\}^{-1} y\]</span>.</p></li>
<li><p>The conditional posterior of <span class="math inline">\(\psi | \phi, \tau, \theta\)</span> can be sampled efficiently in a block by independently sampling <span class="math inline">\(\tilde{\psi}_{j} | \phi, \theta\)</span> from an inverse-Gaussian distribution <span class="math inline">\(\mathrm{i} \mathrm{G}\left(\mu_{j}, \lambda\right)\)</span> with <span class="math inline">\(\mu_{j}=\phi_{j} \tau /\left|\theta_{j}\right|, \lambda=1\)</span> and setting <span class="math inline">\(\psi_{j}=1 / \tilde{\psi}_{j}\)</span>.</p></li>
<li><p>Sample the conditional posterior of <span class="math inline">\(\tau|\phi,\theta\)</span> from a <span class="math inline">\(\operatorname{giG}\left(\lambda-n, 1,2 \sum_{j=1}^{n}\left|\theta_{j}\right| / \phi_{j}\right)\)</span> distribution.</p></li>
<li><p>To sample <span class="math inline">\(\phi|\theta\)</span>, draw <span class="math inline">\(T_{1}, \ldots, T_{n}\)</span> independently with <span class="math inline">\(T_{j} \sim \operatorname{giG}\left(a-1,1,2\left|\theta_{j}\right|\right)\)</span> and set <span class="math inline">\(\phi_j=T_j/T\)</span> with <span class="math inline">\(T=\sum_{j=1}^n T_j\)</span>.</p></li>
</ol>
</div>
<div id="concentration-properties-of-dirichlet-laplace-priors" class="section level2">
<h2><span class="header-section-number">14.7</span> Concentration properties of dirichlet-laplace priors</h2>
<p>In this section, we study a number of properties of the joint density of the Dirichlet-Laplace prior <span class="math inline">\(DL_a\)</span> on <span class="math inline">\(\mathbb R^n\)</span> and investigate the implied rate of posterior contraction in the normal means setting.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bhattacharya:2015cq">
<p>Bhattacharya, Anirban, Debdeep Pati, Natesh S Pillai, and David B Dunson. 2015. “Dirichlet-Laplace priors for optimal shrinkage.” <em>Journal of the American Statistical Association</em> 110 (512): 1479–90.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
