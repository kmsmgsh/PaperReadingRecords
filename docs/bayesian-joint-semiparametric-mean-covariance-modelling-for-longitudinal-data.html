<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 10 Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 10 Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 10 Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2019-10-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html">
<link rel="next" href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><i class="fa fa-check"></i><b>11</b> Bayesian Modeling of Joint Regressions for the Mean and Covariance Matrix</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#abstract-7"><i class="fa fa-check"></i><b>11.1</b> Abstract</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#introduction-9"><i class="fa fa-check"></i><b>11.2</b> Introduction</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#the-model"><i class="fa fa-check"></i><b>11.3</b> The model</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#bayesian-methodology"><i class="fa fa-check"></i><b>11.4</b> Bayesian Methodology</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#centering-and-order-methods"><i class="fa fa-check"></i><b>11.5</b> Centering and Order Methods</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><i class="fa fa-check"></i><b>12</b> MAXIMUM LIKELIHOOD ESTIMATION IN TRANSFORMED LINEAR REGRESSION WITH NONNORMAL ERRORS</a><ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#abstract-8"><i class="fa fa-check"></i><b>12.1</b> Abstract</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#introduction-10"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#parameter-estimation-and-inference-procedure"><i class="fa fa-check"></i><b>12.3</b> Parameter estimation and inference procedure</a><ul>
<li class="chapter" data-level="12.3.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#mle"><i class="fa fa-check"></i><b>12.3.1</b> MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><i class="fa fa-check"></i><b>13</b> Homogeneity tests of covariance matrices with high-dimensional longitudinal data</a><ul>
<li class="chapter" data-level="13.1" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#introduction-background"><i class="fa fa-check"></i><b>13.1</b> Introduction &amp; Background</a></li>
<li class="chapter" data-level="13.2" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#basic-setting"><i class="fa fa-check"></i><b>13.2</b> Basic setting</a></li>
<li class="chapter" data-level="13.3" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#homogeneity-tests-of-covariance-matrices"><i class="fa fa-check"></i><b>13.3</b> Homogeneity tests of covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html"><i class="fa fa-check"></i><b>14</b> Dirichlet-Laplace Priors for Optimal Shrinkage</a><ul>
<li class="chapter" data-level="14.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#abstract-9"><i class="fa fa-check"></i><b>14.1</b> Abstract</a></li>
<li class="chapter" data-level="14.2" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#introduction-11"><i class="fa fa-check"></i><b>14.2</b> Introduction</a></li>
<li class="chapter" data-level="14.3" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#a-new-class-of-shrinkage-priors"><i class="fa fa-check"></i><b>14.3</b> A NEW CLASS OF SHRINKAGE PRIORS:</a><ul>
<li class="chapter" data-level="14.3.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#bayesian-sparsity-priors-in-normal-means-problem"><i class="fa fa-check"></i><b>14.3.1</b> Bayesian Sparsity Priors in Normal Means Problem</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#global-local-shrinkage-rules"><i class="fa fa-check"></i><b>14.4</b> Global-Local shrinkage Rules</a></li>
<li class="chapter" data-level="14.5" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#dirichlet-kernel-priors"><i class="fa fa-check"></i><b>14.5</b> Dirichlet-Kernel Priors</a></li>
<li class="chapter" data-level="14.6" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#posterior-computation"><i class="fa fa-check"></i><b>14.6</b> Posterior Computation</a></li>
<li class="chapter" data-level="14.7" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#concentration-properties-of-dirichlet-laplace-priors"><i class="fa fa-check"></i><b>14.7</b> Concentration properties of dirichlet-laplace priors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><i class="fa fa-check"></i><b>15</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a><ul>
<li class="chapter" data-level="15.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#abstract-10"><i class="fa fa-check"></i><b>15.1</b> Abstract</a></li>
<li class="chapter" data-level="15.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#introduction-12"><i class="fa fa-check"></i><b>15.2</b> Introduction</a></li>
<li class="chapter" data-level="15.3" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#the-model-and-prior"><i class="fa fa-check"></i><b>15.3</b> The model and prior:</a><ul>
<li class="chapter" data-level="15.3.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#likelihood"><i class="fa fa-check"></i><b>15.3.1</b> Likelihood</a></li>
<li class="chapter" data-level="15.3.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#prior-specifiction"><i class="fa fa-check"></i><b>15.3.2</b> Prior specifiction</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#inference-and-simulation-method"><i class="fa fa-check"></i><b>15.4</b> Inference and Simulation method</a><ul>
<li class="chapter" data-level="15.4.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#bayesian-inference."><i class="fa fa-check"></i><b>15.4.1</b> Bayesian inference.</a></li>
<li class="chapter" data-level="15.4.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>15.4.2</b> Markov Chain Monte Carlo Sampling</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#simulation-analysis"><i class="fa fa-check"></i><b>15.5</b> Simulation Analysis</a></li>
<li class="chapter" data-level="15.6" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#real-data-analysis"><i class="fa fa-check"></i><b>15.6</b> Real data analysis</a></li>
<li class="chapter" data-level="15.7" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#conclusion"><i class="fa fa-check"></i><b>15.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><i class="fa fa-check"></i><b>16</b> Modeling local dependence in latent vector autoregressive models</a><ul>
<li class="chapter" data-level="16.1" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html#section-16.1"><i class="fa fa-check"></i><b>16.1</b> 英文摘要简介</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><i class="fa fa-check"></i><b>17</b> BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R</a><ul>
<li class="chapter" data-level="17.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#movel-averaged-estimations-and-predictions"><i class="fa fa-check"></i><b>17.1</b> 6 Movel averaged estimations and predictions</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#estimation"><i class="fa fa-check"></i><b>17.1.1</b> 6.1 Estimation</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#prediction"><i class="fa fa-check"></i><b>17.1.2</b> 6.2 Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><i class="fa fa-check"></i><b>18</b> Criteria for Bayesian Model Choice With Application to Variable Selection</a><ul>
<li class="chapter" data-level="18.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#abstract-11"><i class="fa fa-check"></i><b>18.1</b> Abstract:</a></li>
<li class="chapter" data-level="18.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-13"><i class="fa fa-check"></i><b>18.2</b> Introduction</a><ul>
<li class="chapter" data-level="18.2.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#background"><i class="fa fa-check"></i><b>18.2.1</b> Background</a></li>
<li class="chapter" data-level="18.2.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#notation."><i class="fa fa-check"></i><b>18.2.2</b> Notation.</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#criteria-for-objective-model-selection-priors."><i class="fa fa-check"></i><b>18.3</b> Criteria for objective model selection priors.</a><ul>
<li class="chapter" data-level="18.3.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#basic-criteria"><i class="fa fa-check"></i><b>18.3.1</b> Basic criteria:</a></li>
<li class="chapter" data-level="18.3.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#consistency-criteria"><i class="fa fa-check"></i><b>18.3.2</b> Consistency criteria</a></li>
<li class="chapter" data-level="18.3.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#predictive-matching-criteria."><i class="fa fa-check"></i><b>18.3.3</b> 2.4 Predictive matching criteria.</a></li>
<li class="chapter" data-level="18.3.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#invariance-criteria"><i class="fa fa-check"></i><b>18.3.4</b> Invariance criteria</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#objective-prior-distributions-for-variable-selection-in-normal-linear-models."><i class="fa fa-check"></i><b>18.4</b> Objective prior distributions for variable selection in normal linear models.</a><ul>
<li class="chapter" data-level="18.4.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-14"><i class="fa fa-check"></i><b>18.4.1</b> Introduction</a></li>
<li class="chapter" data-level="18.4.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#proposed-prior-the-robust-prior"><i class="fa fa-check"></i><b>18.4.2</b> Proposed prior (the “robust prior”)</a></li>
<li class="chapter" data-level="18.4.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#choosing-the-hyperparameters-for-p_irg"><i class="fa fa-check"></i><b>18.4.3</b> Choosing the hyperparameters for <span class="math inline">\(p_i^R(g)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#section-18.5"><i class="fa fa-check"></i><b>18.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><i class="fa fa-check"></i><b>19</b> Objective Bayesian Methods for Model Selection: Introduction and Comparison</a><ul>
<li class="chapter" data-level="19.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#bayes-factors"><i class="fa fa-check"></i><b>19.1</b> Bayes Factors</a><ul>
<li class="chapter" data-level="19.1.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#basic-framework"><i class="fa fa-check"></i><b>19.1.1</b> Basic Framework</a></li>
<li class="chapter" data-level="19.1.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-the-bayesian-approach-to-model-selection"><i class="fa fa-check"></i><b>19.1.2</b> Motivation for the Bayesian Approach to Model Selection</a></li>
<li class="chapter" data-level="19.1.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#utility-functions-and-prediction"><i class="fa fa-check"></i><b>19.1.3</b> Utility Functions and Prediction</a></li>
<li class="chapter" data-level="19.1.4" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.4</b> Motivation for Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.5" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#difficulties-in-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.5</b> Difficulties in Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.6" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#preview"><i class="fa fa-check"></i><b>19.1.6</b> Preview</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#objective-bayesian-model-selection-methods-with-illustrations-in-the-linear-model"><i class="fa fa-check"></i><b>19.2</b> Objective Bayesian Model Selection Methods, with Illustrations in the Linear Model</a><ul>
<li class="chapter" data-level="19.2.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#conventional-prior-approach"><i class="fa fa-check"></i><b>19.2.1</b> Conventional Prior Approach</a></li>
<li class="chapter" data-level="19.2.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#intrinsic-bayes-factor-ibf-approach"><i class="fa fa-check"></i><b>19.2.2</b> 2.2 Intrinsic Bayes Factor (IBF) approach</a></li>
<li class="chapter" data-level="19.2.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#the-fractional-bayes-factor-fbf-approach"><i class="fa fa-check"></i><b>19.2.3</b> 2.3 The Fractional Bayes Factor (FBF) Approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><i class="fa fa-check"></i><b>20</b> A Review of Bayesian Variable Selection Methods: What, How and Which</a><ul>
<li class="chapter" data-level="20.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#abstract-12"><i class="fa fa-check"></i><b>20.1</b> Abstract</a></li>
<li class="chapter" data-level="20.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#introduction-15"><i class="fa fa-check"></i><b>20.2</b> Introduction</a><ul>
<li class="chapter" data-level="20.2.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#notation"><i class="fa fa-check"></i><b>20.2.1</b> Notation:</a></li>
<li class="chapter" data-level="20.2.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#concepts-and-properties"><i class="fa fa-check"></i><b>20.2.2</b> Concepts and Properties</a></li>
<li class="chapter" data-level="20.2.3" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#variable-selection-methods"><i class="fa fa-check"></i><b>20.2.3</b> Variable Selection Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html"><i class="fa fa-check"></i><b>21</b> Marginal Likelihood From the Gibbs Output</a><ul>
<li class="chapter" data-level="21.1" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#abstract-13"><i class="fa fa-check"></i><b>21.1</b> Abstract</a></li>
<li class="chapter" data-level="21.2" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#introduction-16"><i class="fa fa-check"></i><b>21.2</b> Introduction</a></li>
<li class="chapter" data-level="21.3" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#notation-1"><i class="fa fa-check"></i><b>21.3</b> Notation:</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><i class="fa fa-check"></i><b>22</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a><ul>
<li class="chapter" data-level="22.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#abstract-14"><i class="fa fa-check"></i><b>22.1</b> Abstract</a></li>
<li class="chapter" data-level="22.2" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#introduction-17"><i class="fa fa-check"></i><b>22.2</b> Introduction</a></li>
<li class="chapter" data-level="22.3" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#the-method"><i class="fa fa-check"></i><b>22.3</b> The Method</a></li>
<li class="chapter" data-level="22.4" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#implementation-and-examples"><i class="fa fa-check"></i><b>22.4</b> 5 Implementation and Examples</a><ul>
<li class="chapter" data-level="22.4.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#iteratively-reweighted-least-squares"><i class="fa fa-check"></i><b>22.4.1</b> Iteratively Reweighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="22.5" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#using-samples-from-posterior-to-evaluate-the-marginal-likelihood"><i class="fa fa-check"></i><b>22.5</b> Using Samples From Posterior To Evaluate The Marginal Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html"><i class="fa fa-check"></i><b>23</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a></li>
<li class="chapter" data-level="24" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html"><i class="fa fa-check"></i><b>24</b> Density estimation in R</a><ul>
<li class="chapter" data-level="24.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#abstract-15"><i class="fa fa-check"></i><b>24.1</b> Abstract</a></li>
<li class="chapter" data-level="24.2" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#motivation"><i class="fa fa-check"></i><b>24.2</b> Motivation</a></li>
<li class="chapter" data-level="24.3" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#theoretical-approaches"><i class="fa fa-check"></i><b>24.3</b> Theoretical approaches</a><ul>
<li class="chapter" data-level="24.3.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#histogram"><i class="fa fa-check"></i><b>24.3.1</b> Histogram</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#kernel-density-estimation"><i class="fa fa-check"></i><b>24.4</b> Kernel density estimation</a><ul>
<li class="chapter" data-level="24.4.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#section-24.4.1"><i class="fa fa-check"></i><b>24.4.1</b> 罚似然函数方法</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#section-24.5"><i class="fa fa-check"></i><b>24.5</b> 密度估计的包</a><ul>
<li class="chapter" data-level="24.5.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#histogram"><i class="fa fa-check"></i><b>24.5.1</b> Histogram柱状图</a></li>
<li class="chapter" data-level="24.5.2" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#kernel-density-estimation-1"><i class="fa fa-check"></i><b>24.5.2</b> Kernel Density estimation</a></li>
<li class="chapter" data-level="24.5.3" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#penalized-approaches"><i class="fa fa-check"></i><b>24.5.3</b> Penalized approaches</a></li>
<li class="chapter" data-level="24.5.4" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#taut-strings-approach"><i class="fa fa-check"></i><b>24.5.4</b> Taut strings approach</a></li>
<li class="chapter" data-level="24.5.5" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#other-packages"><i class="fa fa-check"></i><b>24.5.5</b> Other packages</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#density-estimation-computation-speed"><i class="fa fa-check"></i><b>24.6</b> Density estimation computation speed</a></li>
<li class="chapter" data-level="24.7" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#accuracy-of-density-estimates"><i class="fa fa-check"></i><b>24.7</b> Accuracy of density estimates</a></li>
<li class="chapter" data-level="24.8" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#speed-vs.accuracy"><i class="fa fa-check"></i><b>24.8</b> Speed vs. accuracy</a></li>
<li class="chapter" data-level="24.9" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#conclusion-1"><i class="fa fa-check"></i><b>24.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data" class="section level1">
<h1><span class="header-section-number">Paper 10</span> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</h1>
<p><span class="citation">Liu, Zhang, and Chen (<a href="#ref-Liu:2018wn">2018</a>)</span></p>
<div id="abstract-6" class="section level2">
<h2><span class="header-section-number">10.1</span> Abstract</h2>
<p>In this paper, a Bayesian approach is proposed for modelling the mean and covariance simultaneously by using semiparametric models and the modified Cholesky decomposition. Using a generalized prior to avoid the knots selection while using B-spline to approximate the nonlinear part. And using Metropolis-Hastings algorithm for computations.</p>
</div>
<div id="introduction-8" class="section level2">
<h2><span class="header-section-number">10.2</span> Introduction</h2>
<p>Based on Leng,Zhang,Pan那篇, 基本方法还是来源于 <span class="citation">Cepeda and Gamerman (<a href="#ref-Cepeda:2000vz">2000</a>)</span> . 然后semi-parametric的部分是用了B-spline。Knots的选取使用一个generalized prior代替了。</p>
</div>
<div id="models-and-bayesian-estimation-methods" class="section level2">
<h2><span class="header-section-number">10.3</span> Models and Bayesian Estimation Methods</h2>
<div id="models" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Models</h3>
<p>Suppose <span class="math inline">\(y_{i j} | x_{i j}, t_{i j} \sim N\left(\mu_{i j}, \Sigma_{i}\right)\)</span>, with
MCD <span class="math inline">\(\Phi_{i} \Sigma_{i} \Phi_{i}^{\prime}=D_{i}\)</span>.
Have autoregressive coefficients <span class="math inline">\(\phi_{ijk}\)</span> is negative entry of <span class="math inline">\(\Phi_i\)</span></p>
<p><span class="math display">\[
y_{i j}=\mu_{i j}+\sum_{k=1}^{j-1} \phi_{i j k}\left(y_{i k}-\mu_{i k}\right)+\epsilon_{i j}
\]</span></p>
<p>And the slightly different 3 model</p>
<p><span class="math display">\[
\mu_{i j}=x_{i j}^{\prime} \beta+f_{0}\left(t_{i j}\right), \quad \phi_{i j k}=w_{i j k}^{\prime} \gamma, \quad \log \left(\sigma_{i j}^{2}\right)=z_{i j}^{\prime} \lambda+f_{1}\left(t_{i j}\right)
\]</span></p>
<p><span class="math inline">\(\beta\)</span>,<span class="math inline">\(\gamma\)</span>,<span class="math inline">\(\lambda\)</span>是线性部分，然后<span class="math inline">\(f_0(\cdot)\)</span> 和<span class="math inline">\(f_1(\cdot)\)</span> 是smooth function。</p>
</div>
<div id="smoothing-splines-and-priors" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Smoothing Splines and Priors</h3>
<p><span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1\)</span> 用regression splines来决定。假设<span class="math inline">\(f_0\)</span>和<span class="math inline">\(f_1\)</span>有同样的smoothness的形式，令<span class="math inline">\(0=s_{0}&lt;s_{1}&lt;\cdots&lt;s_{k_{n}}&lt;s_{k_{n}+1}=1\)</span> be a partition of the interval <span class="math inline">\([0,1]\)</span>. <span class="math inline">\(s_i\)</span> be the interval knots, then we have <span class="math inline">\(L=k_n+l\)</span> normalized B-spline basis function of order <span class="math inline">\(l\)</span> that form a basis for the linear spline space, the nonlinear function f can be linearized as
<span class="math display">\[
f(t)=\sum_{l=1}^{L} \theta_{l} b_{l}(t)
\]</span>
Where <span class="math inline">\(B(t)=\left(b_{1}(t), \ldots, b_{L}(t)\right)\)</span> are the B-spline basis functions and <span class="math inline">\(\psi=\left(\theta_{1}, \cdots, \theta_{L}\right)^{\prime}\)</span> are the associated coefficients.</p>
<p>Follow the Bayesian p-spline method by Lang and Brezger, place a first-order random walk prior distribution on the coefficients, so that</p>
<p><span class="math display">\[
p\left(\theta_{1}\right) \propto 1, \quad \theta_{l}=\theta_{l-1}+e_{l}, \quad l=2, \ldots, L ; \quad e_{2}, \ldots, e_{L} | \tau \stackrel{i i d}{\sim} N\left(0, \tau^{2}\right)
\]</span></p>
<p>Here the random walk variance <span class="math inline">\(\tau^2\)</span> can be interpreted as a smoothing parameter. Thus, the resulting conditional prior density for <span class="math inline">\(\psi\)</span> satisfies
<span class="math display">\[
p(\psi | \tau) \propto \exp \left(-\frac{1}{2 \tau^{2}} \psi^{\prime} \Omega \psi\right)
\]</span>
with a banded penalty matrix</p>
<blockquote>
<p>这个random walk prior感觉很有意思诶，不知道为啥要那么弄，但是感觉很强的样子，ar(1)或者ma(1)的结构，然后最后条件分布是一个正态</p>
</blockquote>
<p><span class="math display">\[
\Omega=\left( \begin{array}{cccccc}{1} &amp; {-1} &amp; {0} &amp; {\cdots} &amp; {0} &amp; {0} \\ {-1} &amp; {2} &amp; {-1} &amp; {\cdots} &amp; {0} &amp; {0} \\ {0} &amp; {-1} &amp; {2} &amp; {\cdots} &amp; {0} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} &amp; {\vdots} \\ {0} &amp; {0} &amp; {0} &amp; {\cdots} &amp; {2} &amp; {-1} \\ {0} &amp; {0} &amp; {0} &amp; {\cdots} &amp; {-1} &amp; {1}\end{array}\right)
\]</span></p>
<p>This prior structure is a Bayesian interpretation of the work of Eilers and Marx(1996) which constructed a different penalty from the ranodm walk prior and adopted a penalized maximum likelihood approach.</p>
<p>More in Lang and Brezger(2004).</p>
<p>对于两个非参数的部分<span class="math inline">\(f_{k}(t) \quad(k=0,1)\)</span> 可以写成线性形式<span class="math inline">\(f_{k}\left(t_{i j}\right)=B_{k}\left(t_{i j}\right) \psi_{k}\)</span> at time <span class="math inline">\(t_{ij}\)</span>. 设<span class="math inline">\(n_i\times L\)</span>矩阵
<span class="math inline">\(B_{k i}=\left(B_{k}^{\prime}\left(t_{i 1}\right), \ldots, B_{k}^{\prime}\left(t_{i n_{i}}\right)\right)^{\prime}\)</span> and <span class="math inline">\(f_{k}\left(t_{i}\right)=\left(f_{k}\left(t_{i} 1\right), \ldots, f_{k}\left(t_{i n_{i}}\right)\right)^{\prime}\)</span>, then <span class="math inline">\(f_{k}\left(t_{i}\right)=B_{k i} \psi_{k}\)</span> for k=0,1.
The prior density of <span class="math inline">\(\psi_k\)</span> is</p>
<p><span class="math display">\[
p\left(\psi_{k} | \tau_{k}\right) \propto \exp \left(-\frac{1}{2 \tau_{k}^{2}} \psi_{k}^{\prime} \Omega \psi_{k}\right), \quad k=0,1
\]</span>
.</p>
<p>For fully Bayesian inference, the unknown variance parameters <span class="math inline">\(\tau_k\)</span> can be consideredas random and assigned an inverse gamma hyper-prior, namely <span class="math inline">\(\tau^2_k\sim\Gamma^{-1}(a_k,b_k)\)</span> with some known constants <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_k\)</span>. Also, a vague prior can be imposed on <span class="math inline">\(\tau_k\)</span>. Smaller <span class="math inline">\(\tau_k\)</span> means larger penalty on parameters and meanwhile we assigned large knumber of knos. So a smaller <span class="math inline">\(\tau_k\)</span> is preferred to get the efficient estimation of nonlinear function.</p>
<p>Conjugate prior for <span class="math inline">\(\beta\)</span>,<span class="math inline">\(\gamma\)</span>,<span class="math inline">\(\lambda\)</span> with easily implement MCMC, that is,
<span class="math display">\[
\beta \sim N_{p}\left(\beta_{0}, \Delta_{0}\right), \quad \gamma \sim N_{q}\left(\gamma_{0}, \Gamma_{0}\right), \quad \lambda \sim N_{d}\left(\lambda_{0}, \Lambda_{0}\right)
\]</span></p>
<p>.</p>
</div>
</div>
<div id="mcmc-sampling" class="section level2">
<h2><span class="header-section-number">10.4</span> MCMC Sampling</h2>
<p><span class="math display">\[
\pi\left(\beta | Y, X, f_{0}, \Sigma\right)=N_{p}\left(\Delta_{1}^{-1}\left(\Delta_{0}^{-1} \beta_{0}+\sum_{i=1}^{m} X_{i}^{\prime} \Sigma_{i}^{-1}\left(Y_{i}-f_{0}\left(t_{i}\right)\right)\right), \Delta_{1}^{-1}\right)
\]</span>
with <span class="math inline">\(\Delta_{1}=\Delta_{0}^{-1}+\sum_{i=1}^{m} X_{i}^{\prime} \Sigma_{i}^{-1} X_{i}\)</span>.</p>
<p>The posterior full conditional distribution for B-spline coefficient <span class="math inline">\(\psi_0\)</span> is
<span class="math display">\[
\pi\left(\psi_{0} | \beta, \gamma, \lambda, f_{1}\right)=N_{L}\left(P_{0}^{-1} \sum_{i=1}^{m} B_{0 i}^{\prime} \Sigma_{i}^{-1}\left(Y_{i}-X_{i} \beta\right), P_{0}^{-1}\right)
\]</span>
where
<span class="math inline">\(P_{0}=\sum_{i=1}^{m} B_{0 i}^{\prime} \Sigma_{i}^{-1} B_{0 i}+\Omega / \tau_{0}^{2}\)</span>.</p>
<p>Let <span class="math inline">\(W_{i}=\left(0,\left(y_{i 1}-\mu_{i 1}\right) w_{i 21}, \ldots, \sum_{k=1}^{n_{i}-1}\left(y_{i k}-\mu_{i k}\right) w_{i j k}\right)^{\prime}\)</span>, then we have
<span class="math display">\[
Y_{i}-\mu_{i}=W_{i} \gamma+\epsilon_{i}, \quad i=1, \ldots, m
\]</span>
&gt; 也就是和zp的操作一致。</p>
<p><span class="math display">\[
\pi\left(\gamma | Y, X, \beta, \lambda, f_{0}, f_{1}\right)=N_{q}\left(\Gamma_{1}^{-1}\left(\Gamma_{0}^{-1} \gamma_{0}+\sum_{i=1}^{m} W_{i}^{\prime} D_{i}^{-1}\left(Y_{i}-\mu_{i}\right)\right), \Gamma_{1}^{-1}\right)
\]</span>
where <span class="math inline">\(\Gamma_{1}=\Gamma_{0}^{-1}+\sum_{i=1}^{m} W_{i}^{\prime} D_{i}^{-1} W_{i}\)</span>.</p>
<p>However, unlike the full conditional distributions of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span>, the full conditional distributions of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\psi_1\)</span> are given by</p>
<p><span class="math display">\[
\begin{aligned} \pi\left(\lambda | \beta, \gamma, f_{0}, f_{1}\right) \propto \prod_{i=1}^{m}\left|D_{i}\right|^{-1 / 2} \exp \left\{-\frac{1}{2} \sum_{i=1}^{m}\left(y_{i}-\mu_{i}-W_{i} \gamma\right)^{\prime} D_{i}^{-1}\left(y_{i}-\mu_{i}-W_{i} \gamma\right)\right.\\-\frac{1}{2}\left(\lambda-\lambda_{0}\right)^{\prime} \Lambda_{0}^{-1}\left(\lambda-\lambda_{0}\right) \} \end{aligned}
\]</span>
and
<span class="math display">\[
\begin{aligned} \pi\left(\psi_{1} | \beta, \gamma, \lambda, f_{0}\right) \propto \prod_{i=1}^{m}\left|D_{i}\right|^{-1 / 2} \exp \left\{-\frac{1}{2} \sum_{i=1}^{m}\left(y_{i}-\mu_{i}-W_{i} \gamma\right)^{\prime} D_{i}^{-1}\left(y_{i}-\mu_{i}-W_{i} \gamma\right)\right.\\ -\frac{1}{2 \tau_{1}^{2}} \psi_{1}^{\prime} \Omega \psi_{1} \} \end{aligned}
\]</span></p>
<p>According to the standard Bayesian theory, we have</p>

<div class="theorem">
<span id="thm:unnamed-chunk-10" class="theorem"><strong>Theorem 10.1  </strong></span>Let the Bayesian estimation of <span class="math inline">\(\left(\beta, \psi_{0}, \gamma, \lambda, \psi_{1}\right)\)</span> be the mean of posterior distributions. Then the resultant estimators are known as the minimum mean square error estimators.
</div>

<p>These final two full conditional posterior ditributions are analytically intractable, then we use Metropolis-Hastings instead. Empirical evidence suggests that the more it interrelates with the objective distribution the faster it converges. This paper use the methodologyproposed by [Gamerman,D. 1997 Sampling from the posterior distribution in generalized linear mixed models.] Using fisher scoring algorithm to construct suitable proposals as those of Cepeda and Gamerman. This algorithm requires working variables to approximate transformation of the observations aroune the current parameter estimates. At the step of <span class="math inline">\(\lambda\)</span> iteration, <span class="math inline">\(\beta,\gamma\)</span> and <span class="math inline">\(f_k\)</span> are fixed at their current values <span class="math inline">\(\beta^{(c)}, \lambda^{(c)} \text { and } f_{k}^{(c)}\)</span>, given the working observation model is
<span class="math display">\[
h_{i j}=\left(y_{i j}-x_{i j}^{\prime} \beta^{(c)}-f_{0}^{(c)}-w_{i}^{(c)} \gamma^{(c)}\right)^{2} \sim \sigma_{i j}^{2} \chi_{1}^{2}, \quad i=1, \ldots, m ; j=1, \ldots, n_{i}
\]</span>
where <span class="math inline">\(w_i^{(c)}\)</span> is the ith row of <span class="math inline">\(W_i^{(c)}\)</span>. Therefore, the observation <span class="math inline">\(h_{ij}\)</span> has respective mean <span class="math inline">\(E h_{i j}=\sigma_{i j}^{2}\)</span> and variance <span class="math inline">\(\operatorname{Var}\left(h_{i j}\right)=2 \sigma_{i j}^{4}\)</span>, and is related to the regression parameter <span class="math inline">\(\lambda\)</span> through <span class="math inline">\(\log \left(E h_{i j}\right)=z_{i j}^{\prime} \lambda+f_{1}\left(t_{i j}\right)\)</span>. By Taylor expansion,</p>
<p><span class="math display">\[
\begin{aligned} \log \left(h_{i j}\right) &amp; \approx \log \left(E h_{i j}\right)+\left(h_{i j}-E h_{i j}\right)\left(\log \left(E h_{i i j}\right)\right)^{\prime} \\ &amp;=\log \left(\sigma_{i j}^{2}\right)+\frac{h_{i j}}{\sigma_{i j}^{2}}-1 \sim N\left(z_{i j}^{\prime} \lambda+f_{1}\left(t_{i j}\right), 2\right) \end{aligned}
\]</span>
Similar, we can get the vector of appropriate working observation <span class="math inline">\(\tilde Y=\left(\tilde{y}_{11}, \ldots, \tilde{y}_{m n_{i}}\right)\)</span> with
<span class="math display">\[
\begin{array}{c}{\tilde{y}_{i j}=z_{i j}^{\prime} \lambda^{(c)}+f_{1}^{(c)}\left(t_{i j}\right)+\frac{\left(y_{i j}-x_{i j}^{\prime} \beta^{(c)}-f_{0}^{(c)}-w_{i}^{(c)} \gamma^{(c)}\right)^{2}}{\exp \left(z_{i j}^{\prime} \lambda^{(c)}+f_{1}^{(c)}\left(t_{i j}\right)\right)}-1} \\ {i=1, \ldots, m ; j=1, \ldots, n_{i} ;}\end{array}
\]</span></p>
<p>Therefore, the normal transition kernel for <span class="math inline">\(\lambda\)</span> based on Fisher scoring methods is obtained as
<span class="math display">\[
q_{\lambda}\left(\lambda^{(c)}, \lambda^{(n)}\right)=N\left(g_{b}, G_{b}\right)
\]</span>
where <span class="math inline">\(\lambda^{(n)}\)</span> is the next possible value, and
<span class="math display">\[
g_{b}=G_{b}\left[\Lambda_{0}^{-1} \lambda_{0}+2^{-1} \sum_{i=1}^{m} Z_{i}^{\prime}\left(\tilde{Y}_{i}-f_{1}^{(c)}\left(t_{i}\right)\right)\right], \quad G_{b}=\left(\Lambda_{0}^{-1}+2^{-1} \sum_{i=1}^{m} Z_{i}^{\prime} Z_{i}\right)^{-1}
\]</span></p>
<p>Similarly, the normal transition kernel for <span class="math inline">\(\psi_1\)</span> can be obtained as
<span class="math display">\[
q_{\psi_{1}}\left(\psi_{1}^{(c)}, \psi_{1}^{(n)}\right)=N\left(v_{b}, V_{b}\right)
\]</span></p>
<p>where <span class="math inline">\(\psi_1^{(n)}\)</span> is the next possible value, and
<span class="math display">\[
v_{b}=2^{-1} V_{b} \sum_{i=1}^{m} B_{1 i}^{\prime}\left(\tilde{Y}_{i}-Z_{i} \lambda^{(c)}\right), \quad V_{b}=\left(\tau_{1}^{-2} \Omega+2^{-1} \sum_{i=1}^{m} B_{1 i}^{\prime} B_{1 i}\right)^{-1}
\]</span></p>
<p>Note that instead of sampling all the parameters in <span class="math inline">\(\theta=\left(\beta, \gamma, \lambda, \psi_{0}, \psi_{1}\right)\)</span> at the same time, we should use the conditional independent structure of the model to divided the into blocks and draw samples step by step. Specifically, it is advisable to sample <span class="math inline">\(\beta,\gamma,\psi_0\)</span> from their full posterior conditional distributions directly, and draw sample of <span class="math inline">\(\lambda,\phi_1\)</span> using the Metropolis-Hastings method. The algorithm could be described as follows:</p>
<ol style="list-style-type: decimal">
<li>Start with <span class="math inline">\(\beta^{(0)}, \gamma^{(0)}, \lambda^{(0)}, \psi_{0}^{(0)}, \psi_{1}^{(0)}\)</span></li>
<li>Generate new <span class="math inline">\(\beta^{(j)}\)</span></li>
<li>Based on <span class="math inline">\(\beta^{(j)}\)</span> generate the new value <span class="math inline">\(\psi_0^{(j)}\)</span> and update.</li>
<li>Basedon <span class="math inline">\(\beta^{(j)}\)</span>, <span class="math inline">\(\psi_0^{(j)}\)</span>,generate the new value <span class="math inline">\(\gamma^{(j)}\)</span> and update</li>
<li>Based on step2-4, propose a new value <span class="math inline">\(\lambda^*\)</span> generated from transition kernel. Calculate the acceptance probability of the movement <span class="math inline">\(\alpha_{j}\left(\lambda^{(j-1)}, \lambda^{*}\right)\)</span>. If the movement is accepted, then <span class="math inline">\(\lambda^{(j)}=\lambda^{*}\)</span>, otherwise <span class="math inline">\(\lambda^{(j)}=\lambda^{(j-1)}\)</span>.</li>
<li>Based step 2-5, propose a new value <span class="math inline">\(\psi_1^*\)</span>, and follow the MH algorithm,update <span class="math inline">\(\psi_1^{(j)}\)</span></li>
<li>Next iteration j+1.</li>
</ol>
<p>The acceptance probability mentioned in step 5 could be expressed as
<span class="math display">\[
\alpha_{j}\left(\lambda^{(j-1)}, \lambda^{*}\right)=\frac{\pi\left(\lambda^{*} | \lambda_{-}^{(j-1)}\right) q_{\lambda}\left(\lambda^{*}, \lambda^{(j-1)}\right)}{\pi\left(\lambda^{(j-1)} | \lambda_{-}^{(j-1)}\right) q_{\lambda}\left(\lambda^{(j-1)}, \lambda^{*}\right)}
\]</span></p>
<p><img src="chp11/Figure2.png" width="564" /></p>
<blockquote>
<p>从figure2来看，看来这种基于fishing score method的方法收敛情况意外的好？
而且这让人感觉再推一步就到 Hamiltonian Monte Carlo 的感觉了，毕竟都用了一二阶导数的信息来着。。。需要再把这个proposal的形式推一下</p>
</blockquote>
<blockquote>
<p>总结一下，需要注意的有几点。1，Bayes下的B-spline，也就是P-spline那篇值得推一下。以及其中包括的B spline 参数的先验那个 first order random walk prior和最后得到的那个类似于Normal的形式。2，对于MH算法，那个基于Proposed by Gamerman 的基于Fisher scoring algorithm的构造proposal的方法。-3，文中写出注意的那个算法好像没啥新鲜的，就是一个一个抽，因为Gibbs。而因为之前几个参数有解析分布，可以直接抽，所以号称“block”抽，其实是一样的，还是顺着抽的思路，新<span class="math inline">\(\beta\)</span>，基于<span class="math inline">\(\beta\)</span>抽<span class="math inline">\(\psi_0\)</span>,基于<span class="math inline">\(\beta,\psi_0\)</span>抽<span class="math inline">\(\gamma\)</span>….. 2+,那个proposal的效果意外的好。1+,选knots那块咋搞定的还没懂。</p>
</blockquote>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Liu:2018wn">
<p>Liu, Meimei, Weiping Zhang, and Yu Chen. 2018. “Bayesian Joint Semiparametric MeanCovariance Modeling for Longitudinal Data.” <em>Communications in Mathematics and Statistics</em>, July.</p>
</div>
<div id="ref-Cepeda:2000vz">
<p>Cepeda, Edilberto, and Dani Gamerman. 2000. “Bayesian modeling of variance heterogeneity in normal regression models.” <em>Brazilian Journal of Probability and Statistics</em> 14 (2): 207–21.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
