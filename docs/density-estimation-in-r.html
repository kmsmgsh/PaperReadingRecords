<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 24 Density estimation in R | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 24 Density estimation in R | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 24 Density estimation in R | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2020-03-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html">
<link rel="next" href="integrated-nested-laplace-approximationsinla.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><i class="fa fa-check"></i><b>11</b> Bayesian Modeling of Joint Regressions for the Mean and Covariance Matrix</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#abstract-7"><i class="fa fa-check"></i><b>11.1</b> Abstract</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#introduction-9"><i class="fa fa-check"></i><b>11.2</b> Introduction</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#the-model"><i class="fa fa-check"></i><b>11.3</b> The model</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#bayesian-methodology"><i class="fa fa-check"></i><b>11.4</b> Bayesian Methodology</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#centering-and-order-methods"><i class="fa fa-check"></i><b>11.5</b> Centering and Order Methods</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><i class="fa fa-check"></i><b>12</b> MAXIMUM LIKELIHOOD ESTIMATION IN TRANSFORMED LINEAR REGRESSION WITH NONNORMAL ERRORS</a><ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#abstract-8"><i class="fa fa-check"></i><b>12.1</b> Abstract</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#introduction-10"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#parameter-estimation-and-inference-procedure"><i class="fa fa-check"></i><b>12.3</b> Parameter estimation and inference procedure</a><ul>
<li class="chapter" data-level="12.3.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#mle"><i class="fa fa-check"></i><b>12.3.1</b> MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><i class="fa fa-check"></i><b>13</b> Homogeneity tests of covariance matrices with high-dimensional longitudinal data</a><ul>
<li class="chapter" data-level="13.1" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#introduction-background"><i class="fa fa-check"></i><b>13.1</b> Introduction &amp; Background</a></li>
<li class="chapter" data-level="13.2" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#basic-setting"><i class="fa fa-check"></i><b>13.2</b> Basic setting</a></li>
<li class="chapter" data-level="13.3" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#homogeneity-tests-of-covariance-matrices"><i class="fa fa-check"></i><b>13.3</b> Homogeneity tests of covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html"><i class="fa fa-check"></i><b>14</b> Dirichlet-Laplace Priors for Optimal Shrinkage</a><ul>
<li class="chapter" data-level="14.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#abstract-9"><i class="fa fa-check"></i><b>14.1</b> Abstract</a></li>
<li class="chapter" data-level="14.2" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#introduction-11"><i class="fa fa-check"></i><b>14.2</b> Introduction</a></li>
<li class="chapter" data-level="14.3" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#a-new-class-of-shrinkage-priors"><i class="fa fa-check"></i><b>14.3</b> A NEW CLASS OF SHRINKAGE PRIORS:</a><ul>
<li class="chapter" data-level="14.3.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#bayesian-sparsity-priors-in-normal-means-problem"><i class="fa fa-check"></i><b>14.3.1</b> Bayesian Sparsity Priors in Normal Means Problem</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#global-local-shrinkage-rules"><i class="fa fa-check"></i><b>14.4</b> Global-Local shrinkage Rules</a></li>
<li class="chapter" data-level="14.5" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#dirichlet-kernel-priors"><i class="fa fa-check"></i><b>14.5</b> Dirichlet-Kernel Priors</a></li>
<li class="chapter" data-level="14.6" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#posterior-computation"><i class="fa fa-check"></i><b>14.6</b> Posterior Computation</a></li>
<li class="chapter" data-level="14.7" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#concentration-properties-of-dirichlet-laplace-priors"><i class="fa fa-check"></i><b>14.7</b> Concentration properties of dirichlet-laplace priors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><i class="fa fa-check"></i><b>15</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a><ul>
<li class="chapter" data-level="15.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#abstract-10"><i class="fa fa-check"></i><b>15.1</b> Abstract</a></li>
<li class="chapter" data-level="15.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#introduction-12"><i class="fa fa-check"></i><b>15.2</b> Introduction</a></li>
<li class="chapter" data-level="15.3" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#the-model-and-prior"><i class="fa fa-check"></i><b>15.3</b> The model and prior:</a><ul>
<li class="chapter" data-level="15.3.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#likelihood"><i class="fa fa-check"></i><b>15.3.1</b> Likelihood</a></li>
<li class="chapter" data-level="15.3.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#prior-specifiction"><i class="fa fa-check"></i><b>15.3.2</b> Prior specifiction</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#inference-and-simulation-method"><i class="fa fa-check"></i><b>15.4</b> Inference and Simulation method</a><ul>
<li class="chapter" data-level="15.4.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#bayesian-inference."><i class="fa fa-check"></i><b>15.4.1</b> Bayesian inference.</a></li>
<li class="chapter" data-level="15.4.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>15.4.2</b> Markov Chain Monte Carlo Sampling</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#simulation-analysis"><i class="fa fa-check"></i><b>15.5</b> Simulation Analysis</a></li>
<li class="chapter" data-level="15.6" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#real-data-analysis"><i class="fa fa-check"></i><b>15.6</b> Real data analysis</a></li>
<li class="chapter" data-level="15.7" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#conclusion"><i class="fa fa-check"></i><b>15.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><i class="fa fa-check"></i><b>16</b> Modeling local dependence in latent vector autoregressive models</a><ul>
<li class="chapter" data-level="16.1" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html#section-16.1"><i class="fa fa-check"></i><b>16.1</b> 英文摘要简介</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><i class="fa fa-check"></i><b>17</b> BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R</a><ul>
<li class="chapter" data-level="17.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#movel-averaged-estimations-and-predictions"><i class="fa fa-check"></i><b>17.1</b> 6 Movel averaged estimations and predictions</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#estimation"><i class="fa fa-check"></i><b>17.1.1</b> 6.1 Estimation</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#prediction"><i class="fa fa-check"></i><b>17.1.2</b> 6.2 Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><i class="fa fa-check"></i><b>18</b> Criteria for Bayesian Model Choice With Application to Variable Selection</a><ul>
<li class="chapter" data-level="18.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#abstract-11"><i class="fa fa-check"></i><b>18.1</b> Abstract:</a></li>
<li class="chapter" data-level="18.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-13"><i class="fa fa-check"></i><b>18.2</b> Introduction</a><ul>
<li class="chapter" data-level="18.2.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#background"><i class="fa fa-check"></i><b>18.2.1</b> Background</a></li>
<li class="chapter" data-level="18.2.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#notation."><i class="fa fa-check"></i><b>18.2.2</b> Notation.</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#criteria-for-objective-model-selection-priors."><i class="fa fa-check"></i><b>18.3</b> Criteria for objective model selection priors.</a><ul>
<li class="chapter" data-level="18.3.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#basic-criteria"><i class="fa fa-check"></i><b>18.3.1</b> Basic criteria:</a></li>
<li class="chapter" data-level="18.3.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#consistency-criteria"><i class="fa fa-check"></i><b>18.3.2</b> Consistency criteria</a></li>
<li class="chapter" data-level="18.3.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#predictive-matching-criteria."><i class="fa fa-check"></i><b>18.3.3</b> 2.4 Predictive matching criteria.</a></li>
<li class="chapter" data-level="18.3.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#invariance-criteria"><i class="fa fa-check"></i><b>18.3.4</b> Invariance criteria</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#objective-prior-distributions-for-variable-selection-in-normal-linear-models."><i class="fa fa-check"></i><b>18.4</b> Objective prior distributions for variable selection in normal linear models.</a><ul>
<li class="chapter" data-level="18.4.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-14"><i class="fa fa-check"></i><b>18.4.1</b> Introduction</a></li>
<li class="chapter" data-level="18.4.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#proposed-prior-the-robust-prior"><i class="fa fa-check"></i><b>18.4.2</b> Proposed prior (the “robust prior”)</a></li>
<li class="chapter" data-level="18.4.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#choosing-the-hyperparameters-for-p_irg"><i class="fa fa-check"></i><b>18.4.3</b> Choosing the hyperparameters for <span class="math inline">\(p_i^R(g)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#section-18.5"><i class="fa fa-check"></i><b>18.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><i class="fa fa-check"></i><b>19</b> Objective Bayesian Methods for Model Selection: Introduction and Comparison</a><ul>
<li class="chapter" data-level="19.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#bayes-factors"><i class="fa fa-check"></i><b>19.1</b> Bayes Factors</a><ul>
<li class="chapter" data-level="19.1.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#basic-framework"><i class="fa fa-check"></i><b>19.1.1</b> Basic Framework</a></li>
<li class="chapter" data-level="19.1.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-the-bayesian-approach-to-model-selection"><i class="fa fa-check"></i><b>19.1.2</b> Motivation for the Bayesian Approach to Model Selection</a></li>
<li class="chapter" data-level="19.1.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#utility-functions-and-prediction"><i class="fa fa-check"></i><b>19.1.3</b> Utility Functions and Prediction</a></li>
<li class="chapter" data-level="19.1.4" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.4</b> Motivation for Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.5" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#difficulties-in-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.5</b> Difficulties in Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.6" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#preview"><i class="fa fa-check"></i><b>19.1.6</b> Preview</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#objective-bayesian-model-selection-methods-with-illustrations-in-the-linear-model"><i class="fa fa-check"></i><b>19.2</b> Objective Bayesian Model Selection Methods, with Illustrations in the Linear Model</a><ul>
<li class="chapter" data-level="19.2.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#conventional-prior-approach"><i class="fa fa-check"></i><b>19.2.1</b> Conventional Prior Approach</a></li>
<li class="chapter" data-level="19.2.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#intrinsic-bayes-factor-ibf-approach"><i class="fa fa-check"></i><b>19.2.2</b> 2.2 Intrinsic Bayes Factor (IBF) approach</a></li>
<li class="chapter" data-level="19.2.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#the-fractional-bayes-factor-fbf-approach"><i class="fa fa-check"></i><b>19.2.3</b> 2.3 The Fractional Bayes Factor (FBF) Approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><i class="fa fa-check"></i><b>20</b> A Review of Bayesian Variable Selection Methods: What, How and Which</a><ul>
<li class="chapter" data-level="20.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#abstract-12"><i class="fa fa-check"></i><b>20.1</b> Abstract</a></li>
<li class="chapter" data-level="20.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#introduction-15"><i class="fa fa-check"></i><b>20.2</b> Introduction</a><ul>
<li class="chapter" data-level="20.2.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#notation"><i class="fa fa-check"></i><b>20.2.1</b> Notation:</a></li>
<li class="chapter" data-level="20.2.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#concepts-and-properties"><i class="fa fa-check"></i><b>20.2.2</b> Concepts and Properties</a></li>
<li class="chapter" data-level="20.2.3" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#variable-selection-methods"><i class="fa fa-check"></i><b>20.2.3</b> Variable Selection Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html"><i class="fa fa-check"></i><b>21</b> Marginal Likelihood From the Gibbs Output</a><ul>
<li class="chapter" data-level="21.1" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#abstract-13"><i class="fa fa-check"></i><b>21.1</b> Abstract</a></li>
<li class="chapter" data-level="21.2" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#introduction-16"><i class="fa fa-check"></i><b>21.2</b> Introduction</a></li>
<li class="chapter" data-level="21.3" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#notation-1"><i class="fa fa-check"></i><b>21.3</b> Notation:</a></li>
<li class="chapter" data-level="21.4" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#approach"><i class="fa fa-check"></i><b>21.4</b> Approach</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><i class="fa fa-check"></i><b>22</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a><ul>
<li class="chapter" data-level="22.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#abstract-14"><i class="fa fa-check"></i><b>22.1</b> Abstract</a></li>
<li class="chapter" data-level="22.2" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#introduction-17"><i class="fa fa-check"></i><b>22.2</b> Introduction</a></li>
<li class="chapter" data-level="22.3" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#the-method"><i class="fa fa-check"></i><b>22.3</b> The Method</a></li>
<li class="chapter" data-level="22.4" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#implementation-and-examples"><i class="fa fa-check"></i><b>22.4</b> 5 Implementation and Examples</a><ul>
<li class="chapter" data-level="22.4.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#iteratively-reweighted-least-squares"><i class="fa fa-check"></i><b>22.4.1</b> Iteratively Reweighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="22.5" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#using-samples-from-posterior-to-evaluate-the-marginal-likelihood"><i class="fa fa-check"></i><b>22.5</b> Using Samples From Posterior To Evaluate The Marginal Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html"><i class="fa fa-check"></i><b>23</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a></li>
<li class="chapter" data-level="24" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html"><i class="fa fa-check"></i><b>24</b> Density estimation in R</a><ul>
<li class="chapter" data-level="24.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#abstract-15"><i class="fa fa-check"></i><b>24.1</b> Abstract</a></li>
<li class="chapter" data-level="24.2" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#motivation"><i class="fa fa-check"></i><b>24.2</b> Motivation</a></li>
<li class="chapter" data-level="24.3" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#theoretical-approaches"><i class="fa fa-check"></i><b>24.3</b> Theoretical approaches</a><ul>
<li class="chapter" data-level="24.3.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#histogram"><i class="fa fa-check"></i><b>24.3.1</b> Histogram</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#kernel-density-estimation"><i class="fa fa-check"></i><b>24.4</b> Kernel density estimation</a><ul>
<li class="chapter" data-level="24.4.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#section-24.4.1"><i class="fa fa-check"></i><b>24.4.1</b> 罚似然函数方法</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#section-24.5"><i class="fa fa-check"></i><b>24.5</b> 密度估计的包</a><ul>
<li class="chapter" data-level="24.5.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#histogram"><i class="fa fa-check"></i><b>24.5.1</b> Histogram柱状图</a></li>
<li class="chapter" data-level="24.5.2" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#kernel-density-estimation-1"><i class="fa fa-check"></i><b>24.5.2</b> Kernel Density estimation</a></li>
<li class="chapter" data-level="24.5.3" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#penalized-approaches"><i class="fa fa-check"></i><b>24.5.3</b> Penalized approaches</a></li>
<li class="chapter" data-level="24.5.4" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#taut-strings-approach"><i class="fa fa-check"></i><b>24.5.4</b> Taut strings approach</a></li>
<li class="chapter" data-level="24.5.5" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#other-packages"><i class="fa fa-check"></i><b>24.5.5</b> Other packages</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#density-estimation-computation-speed"><i class="fa fa-check"></i><b>24.6</b> Density estimation computation speed</a></li>
<li class="chapter" data-level="24.7" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#accuracy-of-density-estimates"><i class="fa fa-check"></i><b>24.7</b> Accuracy of density estimates</a></li>
<li class="chapter" data-level="24.8" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#speed-vs.accuracy"><i class="fa fa-check"></i><b>24.8</b> Speed vs. accuracy</a></li>
<li class="chapter" data-level="24.9" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#conclusion-1"><i class="fa fa-check"></i><b>24.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="integrated-nested-laplace-approximationsinla.html"><a href="integrated-nested-laplace-approximationsinla.html"><i class="fa fa-check"></i><b>25</b> Integrated Nested Laplace Approximations(INLA)</a><ul>
<li class="chapter" data-level="25.1" data-path="integrated-nested-laplace-approximationsinla.html"><a href="integrated-nested-laplace-approximationsinla.html#abstract-16"><i class="fa fa-check"></i><b>25.1</b> Abstract</a></li>
<li class="chapter" data-level="25.2" data-path="integrated-nested-laplace-approximationsinla.html"><a href="integrated-nested-laplace-approximationsinla.html#the-inla-computing-scheme"><i class="fa fa-check"></i><b>25.2</b> The INLA computing scheme</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><i class="fa fa-check"></i><b>26</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="density-estimation-in-r" class="section level1">
<h1><span class="header-section-number">Paper 24</span> Density estimation in R</h1>
<p>Henry Deng and Hadley Wickham</p>
<p>冲着Hadley Wickham的名字也要看这篇文章。</p>
<p><a href="https://vita.had.co.nz/papers/density-estimation.pdf">论文本体</a></p>
<div id="abstract-15" class="section level2">
<h2><span class="header-section-number">24.1</span> Abstract</h2>
<p>Density estimation很重要,R有超过20个的包实现。一般很难使。This paper presents a brief outline of the theory underlying each package, as well as an overview of the code and comparison of speed and accuracy.</p>
<p>主要关心单变量的情况。但是也设计了一些其他特殊情况。</p>
<p>总的来说，ASH和KernSmooth最好，很快，精确，并且维护得很好。</p>
</div>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">24.2</span> Motivation</h2>
<p>太多包做density estimation。这些包用了不同的理论方法和计算性能。用户和开发者需要density estimation工具有不同的需求，所以不同的情况下有不同的合适的density estimation方法。这篇文章的目的是总结现有方法，这样能更容易的选取合适的包。</p>
<p>第二节简单的回顾了主流方法背后的理论基础，并且提供了相关文献。
第三节描述了实现这些方法的R包，高亮了如何使用这些density estimation函数，并列出了这些方法不同的特性（维度，区间，步长选择，等等…）
第四节比较了各个包的表现，包括计算速度（通过计算10~10e^7个观测点的density estimation）。Density estimation的准确度也很重要。第五节比较了三个不同分布（不同的难度等级）：均匀分布，正态分布，claw分布。第六节研究了计算时间和准确度的关系。第七节总结了我们的发现以及我们的推荐。</p>
</div>
<div id="theoretical-approaches" class="section level2">
<h2><span class="header-section-number">24.3</span> Theoretical approaches</h2>
<p>Density estimation建立了对于潜在的概率分布函数基于观测数据样本的估计。Density estimation可以是参数的形式，比如数据来源于已知的分布族。或者非参的形式，这样对未知分布的估计会更加灵活。我们开始简短的概述对应的理论，主要关注非参方法，因为非参方法更加普遍。常见方法包括：条形图 第2.1节，核方法 第2.2节，罚方法，第2.3节。我们希望能在不过多关注很细的细节的前提下尽量传递前沿方法的感觉。更为细节的介绍，我们推荐 Scott(1992a)和Silverman(1986).</p>
<div id="histogram" class="section level3">
<h3><span class="header-section-number">24.3.1</span> Histogram</h3>
<p>是最古老并且最不复杂的方法。主要好处是极端简单，并且计算很快。A histogram是分段常数（所以并不光滑），而且对于箱条起始点的选择非常敏感。</p>
<p>一个简单的加强版柱状图方法是average shifted histogram(ASH):这个方法更加光滑，而且避免了对于起点选择的敏感性，同时还保证了计算效率。这个方法的假设(Scott,1992b)是取m个柱状图,<span class="math inline">\(\hat{f}_{1}, \hat{f}_{2}, \ldots, \hat{f}_{m}\)</span>, 每个柱的宽度是h,并且起始点是<span class="math inline">\(t_{o}=0, \frac{h}{m}, \frac{2 h}{m}, \ldots, \frac{(m-1) h}{m}\)</span>. 正如其名字所介绍的，最na¨ıve 的ASH就是
<span class="math display">\[
\widehat{f}_{a s h}(x)=\frac{1}{m} \sum_{i=1}^{m} \hat{f}_{i}(x)
\]</span></p>
<p>有<span class="math inline">\(k=1,...,m\cdot n\)</span> 个柱遍布全部柱状图，每个跨越全部条状图，每个柱的中心是<span class="math inline">\((k+0.5) \frac{h}{m}\)</span>,占据的区间是<span class="math inline">\(\left[k \frac{h}{m},(k+1) \frac{h}{m}\right]\)</span>. ASH可以改造成更加一般的方法。估计每一点密度都使用所有柱,靠近数据点的柱权重更高。更一般形式的ASH是
<span class="math display">\[
\widehat{f}_{a s h}(x)=\frac{1}{m} \sum_{k=1}^{m \cdot n} w\left(l_{k}-x\right) \hat{c}_{k}(x)
\]</span>
其中，w是权重函数，<span class="math inline">\(l_k\)</span>是第k个柱的中心，<span class="math inline">\(\hat c_k\)</span>是这个柱中包含的点的数量。
因为单变量ASH是分段常数，可以通过计算<span class="math inline">\(m\cdot n\)</span>柱，并且计算rolling sum over m个临近的柱。这样能让ASH的计算速度非常快。</p>
</div>
</div>
<div id="kernel-density-estimation" class="section level2">
<h2><span class="header-section-number">24.4</span> Kernel density estimation</h2>
<p>Kernel density estimation方法克服了histogram方法的不连续问题。通过在每个点中心用kernel function然后总结得到一个密度的估计。最基本的kernel estimator 可以写成：
<span class="math display">\[
\widehat{f}_{k d e}(x)=\frac{1}{n} \sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right)
\]</span>
其中<span class="math inline">\(K\)</span>是kernel，<span class="math inline">\(h\)</span>是带宽(Scott,1992b).这个kernel是对称的，一般来说是正的并且积分到1。一般的kernel函数是uniform, triangle, Epanechnikov, quartic (biweight), tricube (triweight), Gaussian(normal), and cosine.</p>
<p>带宽，h，是一个光滑参数，大的带宽可以得到一个非常光滑的估计，小的值则能得到非常扭曲的估计。这个值的对结果的影响甚至超过了kernel的选择，所以选择一个好的带宽是得到好的估计的关键因素。</p>
<p>一般来说带宽通过极小化<span class="math inline">\(L^2\)</span>风险得到，也就是均值的平方积分误差：
<span class="math display">\[
\operatorname{MISE}(\widehat{f})=\mathrm{E}\left[\int(\widehat{f}(x)-f(x))^{2} d x\right]
\]</span>
这个优化问题一般并不容易解决,因为<span class="math inline">\(f(x)\)</span>不知道。由此引申出了很多其他的理论方法。一般来讲被广泛应用的方法是plug-in 选择法和cross validation
选择法。
对于kde方法主要的挑战是数据变化的密度，比如说对于很密集的数据，可以用小的带宽，而数据稀疏的部分则需要大的带宽。对于基本kde方法的扩展可以通过让带宽可变从而克服这个问题。</p>
<div id="section-24.4.1" class="section level3">
<h3><span class="header-section-number">24.4.1</span> 罚似然函数方法</h3>
<p>另外一个对密度函数估计的方法是混合m个“基”函数(密度函数)，调整拟合的好坏和模型的复杂程度。这样做的好处是能让密度函数在数据支持的地方变得很起伏摆动。一般来说，罚方法(Kauermann and Schellhase,2009)通过m个密度函数逼近数据x的密度函数
<span class="math display">\[
\widehat{f}_{p e n}(x)=\sum_{i=1}^{m} c_{i} \phi_{i}(x)
\]</span>
其中,<span class="math inline">\(\phi_k\)</span> 是密度函数，<span class="math inline">\(c_i\)</span>是被选择的权重使得<span class="math inline">\(\hat f_{\text{pen}}\)</span> 积分为1.一般来说基函数是等权重的，只在location参数,<span class="math inline">\(\mu_i\)</span>上有区别，所以我们能简化定义为一个和kernel方法类似的式子：
<span class="math display">\[
\widehat{f}_{p e n}(x)=\frac{1}{m} \sum_{i=1}^{m} K\left(\frac{x-\mu_{i}}{h}\right)
\]</span></p>
<p>和KDE方法相比较，基函数（密度函数）不在要求约束在以数据点为中心。</p>
<p><span class="math inline">\(\mu_i\)</span>一般叫做节点，罚密度估计的关键是找到节点最合适的数量和位置。一般来说会使用等距离的大量点散布在数据的范围内，然后极小化罚似然函数去移除对总体拟合质量贡献不大的节点。</p>
<p><span class="math inline">\(\lambda\)</span>的重要性是控制密度函数估计的光滑程度。一个很方便的方法是通过AIC准则对罚参数进行选取，也就是极小化：
<span class="math display">\[
\operatorname{AIC}(\lambda)=-l(\widehat{\beta})+d f(\lambda)
\]</span>
其中，<span class="math inline">\(d f(\lambda)=\operatorname{tr}\left(J_{p}^{-1}(\widehat{\beta}, \lambda) J_{p}(\widehat{\beta}, \lambda=0)\right)\)</span>.然而，来自于 Kauermann &amp; Schellhase的罚样条光滑方法通过Bayesian方法使用混合模型，</p>
</div>
</div>
<div id="section-24.5" class="section level2">
<h2><span class="header-section-number">24.5</span> 密度估计的包</h2>
<p>现在我们从介绍R包背后的理论转到介绍R包本身。表1总结了15个我们检查过的密度估计包。给出了基础信息包括，他们做了什么，理论基础，和使用这些包的最基本R代码。这一节剩下的部分介绍了每个包的更多细节。对于每个包，我们总结了输入，输出，和其特征。</p>
<div id="histogram" class="section level3">
<h3><span class="header-section-number">24.5.1</span> Histogram柱状图</h3>
<div id="graphics" class="section level4">
<h4><span class="header-section-number">24.5.1.1</span> graphics</h4>
<p><code>graphics::hist</code> 允许用户生成数据x的柱状图(Venables and Ripley,2002)。<code>break</code>参数确定了柱的数量，或者柱的边缘（对于不规则宽窄的柱），或者一个自动估计柱的数量的函数。默认设置中，这个函数会产生一个图，但是这个特性可以通过<code>plot=F</code>进行取消。这个函数返回一个列表，其内容是柱的边界，中点，计数，和密度(通过柱的宽度进行标准化)。</p>
</div>
<div id="ash" class="section level4">
<h4><span class="header-section-number">24.5.1.2</span> ash</h4>
<p><code>ash</code> 包(Scott,2009)估计了ASHs使用两个函数：<code>bin1</code> 和 <code>ash1</code>. <code>bin1</code>需要数据向量x，估计的区间，从<code>a</code>到<code>b</code>, 并且需要的柱的数量<code>nbin</code>, 并且计数了在每个柱中点的数量。<code>ash1</code>使用<code>bin1</code>的输出，然后计算一般ASH，然后权重通过<code>kopt</code>定义。
这样可以产生一个等空间预测的网格，在每个柱的中心。
<code>ash</code>也提供了2-d average shifted histograms 通过<code>bin2</code>和<code>ash2</code>函数。</p>
</div>
</div>
<div id="kernel-density-estimation-1" class="section level3">
<h3><span class="header-section-number">24.5.2</span> Kernel Density estimation</h3>
<p>CRAN包 <strong>GenKern</strong>, <strong>kerdiest</strong>, <strong>KernSmooth</strong>, <strong>ks</strong>, <strong>np</strong>, <strong>plugdensity</strong> 和 <strong>sm</strong> 都使用了kernel density 方法，还有<code>stats::density</code>. 这些方法主要区别是选择带宽的方法。</p>
<p><img src="https://cdn.mathpix.com/snip/images/vBYIv2pG_dkrkTRLkFdjkhQSisDeal7aY9hWvTbWRUQ.original.fullsize.png" /></p>
<div id="genkern" class="section level4">
<h4><span class="header-section-number">24.5.2.1</span> GenKern</h4>
<p>函数 <code>KernSec</code> 来自于 <strong>GenKen</strong> (Lucy and Aykroyd,2010) 产生了单变量密度估计使用高斯kernel对输入<span class="math inline">\(x\)</span>.其支持多个带宽，也就是：
<span class="math display">\[
\widehat{f}_{G K}(x) \propto \sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h_{i}}\right)
\]</span>
由参数 <strong>xbandwidth</strong> 定义。默认的带宽是一个数，用插入准则(Sheather and Jones,1991) 所定义。输出是一个网格，大小是 <strong>xgridsize</strong> 在 <strong>range.x</strong> 上的估计。
函数 <strong>KernSur</strong> 的工作方式类似，不过是2维的。</p>
</div>
<div id="kerdiest" class="section level4">
<h4><span class="header-section-number">24.5.2.2</span> kerdiest</h4>
</div>
<div id="kernsmooth" class="section level4">
<h4><span class="header-section-number">24.5.2.3</span> KernSmooth</h4>
</div>
<div id="ks" class="section level4">
<h4><span class="header-section-number">24.5.2.4</span> ks</h4>
<p>ks包可以计算最高6维的密度估计。 <strong>ks</strong> 提供了一系列带宽的选择方法：
exact MISE, asymptotic MISE 估计 <code>Hamise.mixt</code>和 <code>Hmise.mixt</code>,有偏交叉验证 <code>Hbcv</code>,最小二乘交叉验证 <code>Hlscv</code>, 插入估计 <code>Hpi</code>, 还有光滑交叉验证 <code>Hscv</code>. 全部带宽选择方法也有一个不同的估计，只估计对角元(假设维度是不相关的。)
&gt; 这句没懂，All bandwidth estimators also have a variant that estimates only the diagonal (i.e. it assumes the dimensions are uncorrelated).</p>
<p><strong>kde</strong> 估计密度 1-6维的x，使用带宽 H (或者h 如果是对角的) ，估计使用条状的数据，其中条的宽度是 <strong>bgridsize</strong> 如果参数 <strong>binned</strong> 是 <code>TRUE</code>, 也可以通过 <span class="math inline">\(w\)</span> 自定义权重。 输出是预测密度，在由gridsize，从xmin到xmax定义的网格，或者点的位置 <strong>eval.points</strong> .</p>
<p><strong>ks</strong> 也提供了pdf <strong>dkde</strong> ,cdf <strong>pkde</strong> , 逆cdf <strong>qkde</strong> ,和随机数生成器 <strong>prkde</strong> 函数通过预测的密度生成随机数。</p>
</div>
<div id="np" class="section level4">
<h4><span class="header-section-number">24.5.2.5</span> np</h4>
</div>
<div id="plugdensity" class="section level4">
<h4><span class="header-section-number">24.5.2.6</span> plugdensity</h4>
</div>
<div id="sm" class="section level4">
<h4><span class="header-section-number">24.5.2.7</span> sm</h4>
<p><strong>sm</strong> 包(Bowman and Azzalini,2010,1997) 可以计算kernel density estimation 从1维到3维。函数 <code>sm.density</code> 用输入x，带宽 h，和频率权重 <code>h.weights</code>. 如果没有指定带宽，<code>h.select</code>使用正态最优光滑估计。输出是预测密度，和标准误差，在 <code>eval.points</code>指定的位置。默认的图像输出可以通过 <code>display=&quot;none&quot;</code> 进行取消。</p>
<p><strong>sm</strong> 也提供了非参数 ANCOVA, 自回归分析，回归（包括logistic，Poisson和自相关），和范围上的密度估计。</p>
</div>
<div id="stats" class="section level4">
<h4><span class="header-section-number">24.5.2.8</span> stats</h4>
</div>
</div>
<div id="penalized-approaches" class="section level3">
<h3><span class="header-section-number">24.5.3</span> Penalized approaches</h3>
<p>以下包提供了密度估计基于罚方法并且趋向于有更少的tuning parameter。潜在的方法会比kernel density估计更加复杂.我们建议在使用这些方法之前先熟悉这些方法背后的文献。</p>
<div id="gss" class="section level4">
<h4><span class="header-section-number">24.5.3.1</span> gss</h4>
<p>gss (Gu,2011)使用罚似然方法进行非参数密度估计。
ssden有一个公式接口，用全局环境的变量的向量或者data frame作为输入数据。函数返回一个ssden对象，任意点的预测值可以通过 <code>dssden</code> 得到。 默认的节点数量和位置使用交叉验证进行选取，但是也可以通过<code>nbasis</code>和<code>id.basis</code>参数提供。
<code>gss</code>也提供了cdf <code>pssden</code> 和逆cdf <code>qssden</code> 函数由其生成的密度。对于1维的密度估计，也可以产生条件密度估计，光滑样条ANOVA，hazard模型，log-linear模型。</p>
</div>
<div id="locfit" class="section level4">
<h4><span class="header-section-number">24.5.3.2</span> locfit</h4>
</div>
<div id="pendensity" class="section level4">
<h4><span class="header-section-number">24.5.3.3</span> pendensity</h4>
</div>
<div id="logspline" class="section level4">
<h4><span class="header-section-number">24.5.3.4</span> logspline</h4>
</div>
</div>
<div id="taut-strings-approach" class="section level3">
<h3><span class="header-section-number">24.5.4</span> Taut strings approach</h3>
<div id="ftnonpar" class="section level4">
<h4><span class="header-section-number">24.5.4.1</span> ftnonpar</h4>
</div>
</div>
<div id="other-packages" class="section level3">
<h3><span class="header-section-number">24.5.5</span> Other packages</h3>
</div>
</div>
<div id="density-estimation-computation-speed" class="section level2">
<h2><span class="header-section-number">24.6</span> Density estimation computation speed</h2>
</div>
<div id="accuracy-of-density-estimates" class="section level2">
<h2><span class="header-section-number">24.7</span> Accuracy of density estimates</h2>
</div>
<div id="speed-vs.accuracy" class="section level2">
<h2><span class="header-section-number">24.8</span> Speed vs. accuracy</h2>
</div>
<div id="conclusion-1" class="section level2">
<h2><span class="header-section-number">24.9</span> Conclusion</h2>
<p>我们的发现建议最好的包确实表现了速度和精确度的权衡但是也很快，误差小，并且日常更新。我们建议从ASH或KernSmooth开始。这两个的表现分都很高，包的更新信息也显示这是两个最古老的密度估计包，并且有日常更新。</p>
<p>必须指出我们的推荐是基于一系列的我们能做的测试，基于用户的需要选择包可能会更合适。比如说我们的密度估计准确度的测试基于uniform,normal, 和claw分布。据我们观测 <strong>ftnonpar</strong> 在uniform的情况下表现的比其他包好，但是正态和claw分布则不如。简单来说，具体的理论方法可能在结果上在不同的潜在分布或者数据集上有更好的表现。
我们希望我们的对R的密度估计包的回顾能帮助他人更精确的把问题和答案匹配起来。</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="integrated-nested-laplace-approximationsinla.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
