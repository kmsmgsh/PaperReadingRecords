<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 5 Sparse inverse covariance estimation with the graphical lasso | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 5 Sparse inverse covariance estimation with the graphical lasso | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 5 Sparse inverse covariance estimation with the graphical lasso | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2019-04-13">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html">
<link rel="next" href="a-bayesian-lasso-via-reversible-jump-mcmc.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sparse-inverse-covariance-estimation-with-the-graphical-lasso" class="section level1">
<h1><span class="header-section-number">Paper 5</span> Sparse inverse covariance estimation with the graphical lasso</h1>
<p>by <span class="citation">(Friedman, Hastie, and Biostatistics <a href="#ref-Friedman:2008tb">2008</a>)</span></p>
<div id="summary" class="section level2">
<h2><span class="header-section-number">5.1</span> Summary</h2>
<p>Estimating sparse graph by lasso penalty to precision matrix.
Using coordinate descent procedure for the lasso-&gt; <em>graphical lasso</em>, which is remarkably fast.</p>
</div>
<div id="introduction-4" class="section level2">
<h2><span class="header-section-number">5.2</span> Introduction</h2>
<p>Estimation of sparse undirected graphical models through <span class="math inline">\(L_1\)</span> regularization. Basic model: observations follows <span class="math inline">\(MVN(\mu,\Sigma)\)</span>. An important result in graphical model is that if the <span class="math inline">\(ij\)</span> th component of <span class="math inline">\(\Sigma^{-1}\)</span> is 0, <span class="math inline">\(\sigma_{ij}=0\)</span>, then the variable <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are conditionally independent condition on other variables. This is the reason to impose penalty to precision matrix <span class="math inline">\(\Sigma^{-1}\)</span> to increase its sparsity.</p>
<p><span class="citation">(Meinshausen and Bühlmann <a href="#ref-Meinshausen:2006fu">2006</a>)</span> take a simple approach: estimate sparse graphical model by fitting lasso to each variable. The correspond <span class="math inline">\(\hat\Sigma_{ij}^{-1}\)</span> is nonzero if variable estiamted coefficient of variable <span class="math inline">\(i\)</span> on <span class="math inline">\(j\)</span> or <span class="math inline">\(j\)</span> on <span class="math inline">\(i\)</span> is nonzero.(They use AND rather than OR). Then they showed method is asymptotically consistenctly estimates of <span class="math inline">\(\Sigma^{-1}\)</span> for nonzero elements.</p>
<p>Others propose exact maximization of the <span class="math inline">\(L_1\)</span> penalized loglikelihood,
some adapt interior-point optimization methods, and simpler approach to <span class="citation">(Meinshausen and Bühlmann <a href="#ref-Meinshausen:2006fu">2006</a>)</span>.</p>
<p>This paper use blockwise coordinate descent approach in (Banerjee,et.al. 2007) at launch point, and propose a new algorithm for the exat problem. It also bridge the original idea and the exact problem.</p>
</div>
<div id="the-proposed-model" class="section level2">
<h2><span class="header-section-number">5.3</span> The Proposed Model</h2>
<p>Assume we have N <span class="math inline">\(MVN(\mu,\Sigma)\)</span> observations with dimension p. Let <span class="math inline">\(\Theta=\Sigma^{-1}\)</span>, and S be the empirical covariance matrix, the problem is maximize the penalized log-likelihood
<span class="math display">\[
\log \operatorname{det} \Theta-\operatorname{tr}(S \Theta)-\rho\|\Theta\|_{1}
\]</span>
over the nonnegative definite matrices <span class="math inline">\(\Theta\)</span>, <span class="citation">(Yuan and Lin <a href="#ref-Yuan:2007kk">2007</a>)</span> solves this problemusing the interior-point method for the “maxdet” problem. Banerjee and others (2007) use another framework for the optimization which impetus for this paper work.</p>
<p>They showed that the problem of penalized log-likelihood is convex and estimation of <span class="math inline">\(\Sigma\)</span> as
W. which is</p>
<p><span class="math display">\[
W=\left( \begin{array}{cc}{W_{11}} &amp; {w_{12}} \\ {w_{12}^{T}} &amp; {w_{22}}\end{array}\right), \quad S=\left( \begin{array}{cc}{S_{11}} &amp; {s_{12}} \\ {s_{12}^{T}} &amp; {s_{22}}\end{array}\right)
\]</span>
solving the problem by optimizing over each row and corresponding column of W in a block coordinate descent fashion. The solution for <span class="math inline">\(w_{12}\)</span> satisfies
<span class="math display">\[
w_{12}=\operatorname{argmin}_{y}\left\{y^{T} W_{11}^{-1} y :\left\|y-s_{12}\right\|_{\infty} \leqslant \rho\right\}
\]</span>
this is box-constrained quadratic program(QP) solve using interior-point procedure.</p>
<blockquote>
<p>If this procedure is initialized with a positive definite matrix, they show that the iterates from this procedure remains positive definite and invertible, even if p&gt;N</p>
</blockquote>
<p>Using convex duality, solve <span class="math inline">\(w_{12}\)</span> is equivalent to solving the dual problem</p>
<p><span class="math display" id="eq:2-4">\[
\begin{equation} 
\min _{\beta}\left\{\frac{1}{2}\left\|W_{11}^{1 / 2} \beta-b\right\|^{2}+\rho\|\beta\|_{1}\right\}
\tag{5.1}
\end{equation} 
\]</span>
where <span class="math inline">\(b=W_{11}^{-1 / 2} s_{12}\)</span>.
Expanding <span class="math inline">\(W \Theta=I\)</span> as</p>
<p><span class="math display">\[
\left( \begin{array}{cc}{W_{11}} &amp; {w_{12}} \\ {w_{12}^{T}} &amp; {w_{22}}\end{array}\right) \left( \begin{array}{ll}{\Theta_{11}} &amp; {\theta_{12}} \\ {\theta_{12}^{T}} &amp; {\theta_{22}}\end{array}\right)=\left( \begin{array}{cc}{I} &amp; {0} \\ {0^{T}} &amp; {1}\end{array}\right)
\]</span>
Now the subgradient equation for maximization the log-likelihood is
<span class="math display">\[
W-S-\rho \cdot \Gamma=0
\]</span>
by <em>convex optimization</em> Boyd 2004.</p>
<p>The computation omit…</p>
<p>…
Note that solution of <span class="math inline">\(\beta\)</span> to the lasso problem <a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#eq:2-4">(5.1)</a> ,gives (up to a negative constant), the corresponding part of <span class="math inline">\(\Theta:\theta_{12}=-\theta_{22} \beta\)</span>.</p>
<p>The main point of this paper:</p>
<p><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#eq:2-4">(5.1)</a> Lookslike a lasso LS problem, and the solutions <span class="math inline">\(\hat\beta\)</span> are easily seen to lasso estimates for the pth variable.</p>
<p>..</p>
<p>..</p>
<p>And this paragraph is extreme important</p>
<blockquote>
<p>Now to the main point of this paper. Problem (2.4) looks like a lasso (L1-regularized) least-squares problem. In fact if <span class="math inline">\(W_{11}\)</span> = <span class="math inline">\(S_{11}\)</span>, then the solutions βˆ are easily seen to equal the lasso estimates for the pth variable on the others and hence related to the Meinshausen and Bu ̈hlmann (2006) proposal. As pointed out by Banerjee and others (2007),<span class="math inline">\(W_{11} \neq S_{11}\)</span> in general, and hence, the Meinshausen and Bu ̈hlmann (2006) approach does not yield the maximum likelihood estimator. They point out that their blockwise interior point procedure is equivalent to recursively solving and updating the lasso problem (2.4), but do not pursue this approach. We do, to great advantage, because fast coordinate descent algorithms (Friedman and others, 2007) make solution of the lasso problem very attractive.</p>
</blockquote>
<p>…
然后时间不够看不懂了。</p>
<p>这篇的要点：</p>
<ul>
<li>Meinshausen and Bu ̈hlmann (2006) 的思路（在introduction中），</li>
<li>对于penalized loglikelihood的几个解决方案，interior-point optimization methods。</li>
<li>Block-wise coordinate descent approach</li>
<li>duality convex的那个式子</li>
<li>Boyd 2004 p641 提供的subgradient equation for MLE.</li>
</ul>
<p>没看懂的</p>
<ul>
<li>Graphical lasso是通过哪一点作为初始然后优化了结论？</li>
<li>优化过程的推导</li>
<li>几个等价性的问题。</li>
</ul>
<p>没看的</p>
<p>时间比较，实证分析，公式推演（基本上其实这篇文章就没看？</p>
<p>但是只花了2个多小时来着。</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Friedman:2008tb">
<p>Friedman, J, T Hastie, and R Tibshirani Biostatistics. 2008. “Sparse inverse covariance estimation with the graphical lasso.” <em>Biostatistics</em>.</p>
</div>
<div id="ref-Meinshausen:2006fu">
<p>Meinshausen, Nicolai, and Peter Bühlmann. 2006. “High-dimensional graphs and variable selection with the lasso.” <em>The Annals of Statistics</em> 34 (3): 1436–62.</p>
</div>
<div id="ref-Yuan:2007kk">
<p>Yuan, Ming, and Y Lin. 2007. “Model selection and estimation in the Gaussian graphical model.” <em>Biometrika</em> 94 (1): 19–35.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-bayesian-lasso-via-reversible-jump-mcmc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
