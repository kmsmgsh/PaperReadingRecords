<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 26 Parsimonious Covariance Matrix Estimation for Longitudinal Data | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 26 Parsimonious Covariance Matrix Estimation for Longitudinal Data | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 26 Parsimonious Covariance Matrix Estimation for Longitudinal Data | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2020-03-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="integrated-nested-laplace-approximationsinla.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><i class="fa fa-check"></i><b>11</b> Bayesian Modeling of Joint Regressions for the Mean and Covariance Matrix</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#abstract-7"><i class="fa fa-check"></i><b>11.1</b> Abstract</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#introduction-9"><i class="fa fa-check"></i><b>11.2</b> Introduction</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#the-model"><i class="fa fa-check"></i><b>11.3</b> The model</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#bayesian-methodology"><i class="fa fa-check"></i><b>11.4</b> Bayesian Methodology</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#centering-and-order-methods"><i class="fa fa-check"></i><b>11.5</b> Centering and Order Methods</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><i class="fa fa-check"></i><b>12</b> MAXIMUM LIKELIHOOD ESTIMATION IN TRANSFORMED LINEAR REGRESSION WITH NONNORMAL ERRORS</a><ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#abstract-8"><i class="fa fa-check"></i><b>12.1</b> Abstract</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#introduction-10"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#parameter-estimation-and-inference-procedure"><i class="fa fa-check"></i><b>12.3</b> Parameter estimation and inference procedure</a><ul>
<li class="chapter" data-level="12.3.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#mle"><i class="fa fa-check"></i><b>12.3.1</b> MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><i class="fa fa-check"></i><b>13</b> Homogeneity tests of covariance matrices with high-dimensional longitudinal data</a><ul>
<li class="chapter" data-level="13.1" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#introduction-background"><i class="fa fa-check"></i><b>13.1</b> Introduction &amp; Background</a></li>
<li class="chapter" data-level="13.2" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#basic-setting"><i class="fa fa-check"></i><b>13.2</b> Basic setting</a></li>
<li class="chapter" data-level="13.3" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#homogeneity-tests-of-covariance-matrices"><i class="fa fa-check"></i><b>13.3</b> Homogeneity tests of covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html"><i class="fa fa-check"></i><b>14</b> Dirichlet-Laplace Priors for Optimal Shrinkage</a><ul>
<li class="chapter" data-level="14.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#abstract-9"><i class="fa fa-check"></i><b>14.1</b> Abstract</a></li>
<li class="chapter" data-level="14.2" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#introduction-11"><i class="fa fa-check"></i><b>14.2</b> Introduction</a></li>
<li class="chapter" data-level="14.3" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#a-new-class-of-shrinkage-priors"><i class="fa fa-check"></i><b>14.3</b> A NEW CLASS OF SHRINKAGE PRIORS:</a><ul>
<li class="chapter" data-level="14.3.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#bayesian-sparsity-priors-in-normal-means-problem"><i class="fa fa-check"></i><b>14.3.1</b> Bayesian Sparsity Priors in Normal Means Problem</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#global-local-shrinkage-rules"><i class="fa fa-check"></i><b>14.4</b> Global-Local shrinkage Rules</a></li>
<li class="chapter" data-level="14.5" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#dirichlet-kernel-priors"><i class="fa fa-check"></i><b>14.5</b> Dirichlet-Kernel Priors</a></li>
<li class="chapter" data-level="14.6" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#posterior-computation"><i class="fa fa-check"></i><b>14.6</b> Posterior Computation</a></li>
<li class="chapter" data-level="14.7" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#concentration-properties-of-dirichlet-laplace-priors"><i class="fa fa-check"></i><b>14.7</b> Concentration properties of dirichlet-laplace priors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><i class="fa fa-check"></i><b>15</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a><ul>
<li class="chapter" data-level="15.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#abstract-10"><i class="fa fa-check"></i><b>15.1</b> Abstract</a></li>
<li class="chapter" data-level="15.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#introduction-12"><i class="fa fa-check"></i><b>15.2</b> Introduction</a></li>
<li class="chapter" data-level="15.3" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#the-model-and-prior"><i class="fa fa-check"></i><b>15.3</b> The model and prior:</a><ul>
<li class="chapter" data-level="15.3.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#likelihood"><i class="fa fa-check"></i><b>15.3.1</b> Likelihood</a></li>
<li class="chapter" data-level="15.3.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#prior-specifiction"><i class="fa fa-check"></i><b>15.3.2</b> Prior specifiction</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#inference-and-simulation-method"><i class="fa fa-check"></i><b>15.4</b> Inference and Simulation method</a><ul>
<li class="chapter" data-level="15.4.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#bayesian-inference."><i class="fa fa-check"></i><b>15.4.1</b> Bayesian inference.</a></li>
<li class="chapter" data-level="15.4.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>15.4.2</b> Markov Chain Monte Carlo Sampling</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#simulation-analysis"><i class="fa fa-check"></i><b>15.5</b> Simulation Analysis</a></li>
<li class="chapter" data-level="15.6" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#real-data-analysis"><i class="fa fa-check"></i><b>15.6</b> Real data analysis</a></li>
<li class="chapter" data-level="15.7" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#conclusion"><i class="fa fa-check"></i><b>15.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><i class="fa fa-check"></i><b>16</b> Modeling local dependence in latent vector autoregressive models</a><ul>
<li class="chapter" data-level="16.1" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html#section-16.1"><i class="fa fa-check"></i><b>16.1</b> 英文摘要简介</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><i class="fa fa-check"></i><b>17</b> BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R</a><ul>
<li class="chapter" data-level="17.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#movel-averaged-estimations-and-predictions"><i class="fa fa-check"></i><b>17.1</b> 6 Movel averaged estimations and predictions</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#estimation"><i class="fa fa-check"></i><b>17.1.1</b> 6.1 Estimation</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#prediction"><i class="fa fa-check"></i><b>17.1.2</b> 6.2 Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><i class="fa fa-check"></i><b>18</b> Criteria for Bayesian Model Choice With Application to Variable Selection</a><ul>
<li class="chapter" data-level="18.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#abstract-11"><i class="fa fa-check"></i><b>18.1</b> Abstract:</a></li>
<li class="chapter" data-level="18.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-13"><i class="fa fa-check"></i><b>18.2</b> Introduction</a><ul>
<li class="chapter" data-level="18.2.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#background"><i class="fa fa-check"></i><b>18.2.1</b> Background</a></li>
<li class="chapter" data-level="18.2.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#notation."><i class="fa fa-check"></i><b>18.2.2</b> Notation.</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#criteria-for-objective-model-selection-priors."><i class="fa fa-check"></i><b>18.3</b> Criteria for objective model selection priors.</a><ul>
<li class="chapter" data-level="18.3.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#basic-criteria"><i class="fa fa-check"></i><b>18.3.1</b> Basic criteria:</a></li>
<li class="chapter" data-level="18.3.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#consistency-criteria"><i class="fa fa-check"></i><b>18.3.2</b> Consistency criteria</a></li>
<li class="chapter" data-level="18.3.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#predictive-matching-criteria."><i class="fa fa-check"></i><b>18.3.3</b> 2.4 Predictive matching criteria.</a></li>
<li class="chapter" data-level="18.3.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#invariance-criteria"><i class="fa fa-check"></i><b>18.3.4</b> Invariance criteria</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#objective-prior-distributions-for-variable-selection-in-normal-linear-models."><i class="fa fa-check"></i><b>18.4</b> Objective prior distributions for variable selection in normal linear models.</a><ul>
<li class="chapter" data-level="18.4.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-14"><i class="fa fa-check"></i><b>18.4.1</b> Introduction</a></li>
<li class="chapter" data-level="18.4.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#proposed-prior-the-robust-prior"><i class="fa fa-check"></i><b>18.4.2</b> Proposed prior (the “robust prior”)</a></li>
<li class="chapter" data-level="18.4.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#choosing-the-hyperparameters-for-p_irg"><i class="fa fa-check"></i><b>18.4.3</b> Choosing the hyperparameters for <span class="math inline">\(p_i^R(g)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#section-18.5"><i class="fa fa-check"></i><b>18.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><i class="fa fa-check"></i><b>19</b> Objective Bayesian Methods for Model Selection: Introduction and Comparison</a><ul>
<li class="chapter" data-level="19.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#bayes-factors"><i class="fa fa-check"></i><b>19.1</b> Bayes Factors</a><ul>
<li class="chapter" data-level="19.1.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#basic-framework"><i class="fa fa-check"></i><b>19.1.1</b> Basic Framework</a></li>
<li class="chapter" data-level="19.1.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-the-bayesian-approach-to-model-selection"><i class="fa fa-check"></i><b>19.1.2</b> Motivation for the Bayesian Approach to Model Selection</a></li>
<li class="chapter" data-level="19.1.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#utility-functions-and-prediction"><i class="fa fa-check"></i><b>19.1.3</b> Utility Functions and Prediction</a></li>
<li class="chapter" data-level="19.1.4" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.4</b> Motivation for Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.5" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#difficulties-in-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.5</b> Difficulties in Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.6" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#preview"><i class="fa fa-check"></i><b>19.1.6</b> Preview</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#objective-bayesian-model-selection-methods-with-illustrations-in-the-linear-model"><i class="fa fa-check"></i><b>19.2</b> Objective Bayesian Model Selection Methods, with Illustrations in the Linear Model</a><ul>
<li class="chapter" data-level="19.2.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#conventional-prior-approach"><i class="fa fa-check"></i><b>19.2.1</b> Conventional Prior Approach</a></li>
<li class="chapter" data-level="19.2.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#intrinsic-bayes-factor-ibf-approach"><i class="fa fa-check"></i><b>19.2.2</b> 2.2 Intrinsic Bayes Factor (IBF) approach</a></li>
<li class="chapter" data-level="19.2.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#the-fractional-bayes-factor-fbf-approach"><i class="fa fa-check"></i><b>19.2.3</b> 2.3 The Fractional Bayes Factor (FBF) Approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><i class="fa fa-check"></i><b>20</b> A Review of Bayesian Variable Selection Methods: What, How and Which</a><ul>
<li class="chapter" data-level="20.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#abstract-12"><i class="fa fa-check"></i><b>20.1</b> Abstract</a></li>
<li class="chapter" data-level="20.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#introduction-15"><i class="fa fa-check"></i><b>20.2</b> Introduction</a><ul>
<li class="chapter" data-level="20.2.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#notation"><i class="fa fa-check"></i><b>20.2.1</b> Notation:</a></li>
<li class="chapter" data-level="20.2.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#concepts-and-properties"><i class="fa fa-check"></i><b>20.2.2</b> Concepts and Properties</a></li>
<li class="chapter" data-level="20.2.3" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#variable-selection-methods"><i class="fa fa-check"></i><b>20.2.3</b> Variable Selection Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html"><i class="fa fa-check"></i><b>21</b> Marginal Likelihood From the Gibbs Output</a><ul>
<li class="chapter" data-level="21.1" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#abstract-13"><i class="fa fa-check"></i><b>21.1</b> Abstract</a></li>
<li class="chapter" data-level="21.2" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#introduction-16"><i class="fa fa-check"></i><b>21.2</b> Introduction</a></li>
<li class="chapter" data-level="21.3" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#notation-1"><i class="fa fa-check"></i><b>21.3</b> Notation:</a></li>
<li class="chapter" data-level="21.4" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#approach"><i class="fa fa-check"></i><b>21.4</b> Approach</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><i class="fa fa-check"></i><b>22</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a><ul>
<li class="chapter" data-level="22.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#abstract-14"><i class="fa fa-check"></i><b>22.1</b> Abstract</a></li>
<li class="chapter" data-level="22.2" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#introduction-17"><i class="fa fa-check"></i><b>22.2</b> Introduction</a></li>
<li class="chapter" data-level="22.3" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#the-method"><i class="fa fa-check"></i><b>22.3</b> The Method</a></li>
<li class="chapter" data-level="22.4" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#implementation-and-examples"><i class="fa fa-check"></i><b>22.4</b> 5 Implementation and Examples</a><ul>
<li class="chapter" data-level="22.4.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#iteratively-reweighted-least-squares"><i class="fa fa-check"></i><b>22.4.1</b> Iteratively Reweighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="22.5" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#using-samples-from-posterior-to-evaluate-the-marginal-likelihood"><i class="fa fa-check"></i><b>22.5</b> Using Samples From Posterior To Evaluate The Marginal Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap-1.html"><i class="fa fa-check"></i><b>23</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a></li>
<li class="chapter" data-level="24" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html"><i class="fa fa-check"></i><b>24</b> Density estimation in R</a><ul>
<li class="chapter" data-level="24.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#abstract-15"><i class="fa fa-check"></i><b>24.1</b> Abstract</a></li>
<li class="chapter" data-level="24.2" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#motivation"><i class="fa fa-check"></i><b>24.2</b> Motivation</a></li>
<li class="chapter" data-level="24.3" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#theoretical-approaches"><i class="fa fa-check"></i><b>24.3</b> Theoretical approaches</a><ul>
<li class="chapter" data-level="24.3.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#histogram"><i class="fa fa-check"></i><b>24.3.1</b> Histogram</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#kernel-density-estimation"><i class="fa fa-check"></i><b>24.4</b> Kernel density estimation</a><ul>
<li class="chapter" data-level="24.4.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#section-24.4.1"><i class="fa fa-check"></i><b>24.4.1</b> 罚似然函数方法</a></li>
</ul></li>
<li class="chapter" data-level="24.5" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#section-24.5"><i class="fa fa-check"></i><b>24.5</b> 密度估计的包</a><ul>
<li class="chapter" data-level="24.5.1" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#histogram"><i class="fa fa-check"></i><b>24.5.1</b> Histogram柱状图</a></li>
<li class="chapter" data-level="24.5.2" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#kernel-density-estimation-1"><i class="fa fa-check"></i><b>24.5.2</b> Kernel Density estimation</a></li>
<li class="chapter" data-level="24.5.3" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#penalized-approaches"><i class="fa fa-check"></i><b>24.5.3</b> Penalized approaches</a></li>
<li class="chapter" data-level="24.5.4" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#taut-strings-approach"><i class="fa fa-check"></i><b>24.5.4</b> Taut strings approach</a></li>
<li class="chapter" data-level="24.5.5" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#other-packages"><i class="fa fa-check"></i><b>24.5.5</b> Other packages</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#density-estimation-computation-speed"><i class="fa fa-check"></i><b>24.6</b> Density estimation computation speed</a></li>
<li class="chapter" data-level="24.7" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#accuracy-of-density-estimates"><i class="fa fa-check"></i><b>24.7</b> Accuracy of density estimates</a></li>
<li class="chapter" data-level="24.8" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#speed-vs.accuracy"><i class="fa fa-check"></i><b>24.8</b> Speed vs. accuracy</a></li>
<li class="chapter" data-level="24.9" data-path="density-estimation-in-r.html"><a href="density-estimation-in-r.html#conclusion-1"><i class="fa fa-check"></i><b>24.9</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="integrated-nested-laplace-approximationsinla.html"><a href="integrated-nested-laplace-approximationsinla.html"><i class="fa fa-check"></i><b>25</b> Integrated Nested Laplace Approximations(INLA)</a><ul>
<li class="chapter" data-level="25.1" data-path="integrated-nested-laplace-approximationsinla.html"><a href="integrated-nested-laplace-approximationsinla.html#abstract-16"><i class="fa fa-check"></i><b>25.1</b> Abstract</a></li>
<li class="chapter" data-level="25.2" data-path="integrated-nested-laplace-approximationsinla.html"><a href="integrated-nested-laplace-approximationsinla.html#the-inla-computing-scheme"><i class="fa fa-check"></i><b>25.2</b> The INLA computing scheme</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><i class="fa fa-check"></i><b>26</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a><ul>
<li class="chapter" data-level="26.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#notation-2"><i class="fa fa-check"></i><b>26.1</b> Notation:</a></li>
<li class="chapter" data-level="26.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#likelihood-1"><i class="fa fa-check"></i><b>26.2</b> Likelihood</a></li>
<li class="chapter" data-level="26.3" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#prior-specification-1"><i class="fa fa-check"></i><b>26.3</b> Prior Specification</a></li>
<li class="chapter" data-level="26.4" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#inference-and-simulation-method."><i class="fa fa-check"></i><b>26.4</b> Inference And Simulation Method.</a><ul>
<li class="chapter" data-level="26.4.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#markov-chain-monte-carlo-sampling-1"><i class="fa fa-check"></i><b>26.4.1</b> Markov Chain Monte Carlo Sampling</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#simulation-study"><i class="fa fa-check"></i><b>26.5</b> Simulation Study</a></li>
<li class="chapter" data-level="26.6" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#application"><i class="fa fa-check"></i><b>26.6</b> Application</a><ul>
<li class="chapter" data-level="26.6.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#longitudinal-case"><i class="fa fa-check"></i><b>26.6.1</b> Longitudinal case</a></li>
<li class="chapter" data-level="26.6.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#intraday-elextricity-demand."><i class="fa fa-check"></i><b>26.6.2</b> Intraday Elextricity Demand.</a></li>
</ul></li>
<li class="chapter" data-level="26.7" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1.html#conclusion-2"><i class="fa fa-check"></i><b>26.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parsimonious-covariance-matrix-estimation-for-longitudinal-data-1" class="section level1">
<h1><span class="header-section-number">Paper 26</span> Parsimonious Covariance Matrix Estimation for Longitudinal Data</h1>
<p>Can be used in longitudinal data with autoregressive/antedependence model.</p>
<p>Structure:</p>
<ul>
<li>2: Bayesian hierachical model</li>
<li>3: Bayesian inference, MCMC Sampling method</li>
<li>4: Simulation Study</li>
<li>5: Real data analysis</li>
<li>6: Summarize</li>
</ul>
<p>Notation:</p>
<p><span class="math display">\[
e_i \sim N(0,\Sigma)
\]</span>
MCD:
<span class="math display">\[
\Sigma^{-1}=B D B^{\prime}
\]</span></p>
<p>This work is related to Gaussian covariance selection model, also called Gausian graphical models.</p>
<p>Alternative factorization of the covariance matrix is spectral decomposition :
<span class="math display">\[
\Sigma=O^{\prime} \Lambda O
\]</span>
O can be further decomposed into product of Givens rotation matrices, then the covariance becomes eigenvalues and Givens angles. Yang and Berger (1994) placed reference prior and carry out Bayesian inference on <span class="math inline">\(\Sigma\)</span>. This method are used to be compare the estimation efficiency.</p>
<div id="notation-2" class="section level2">
<h2><span class="header-section-number">26.1</span> Notation:</h2>
<p><span class="math display">\[
\Sigma^{-1}=\left\{\sigma^{i, j}\right\}
\]</span>
<span class="math inline">\(B=\left\{b_{i, j}\right\}\)</span>, <span class="math inline">\(D=\operatorname{diag}\left(d_{1}, \ldots, d_{m}\right)\)</span>. To allow parsimony in representation,set indicator
<span class="math display">\[
\begin{aligned}
&amp;b_{i, j}=0 \quad \text { iff } \quad \gamma_{i, j}=0\\
&amp;b_{i, j} \neq 0 \quad \text { iff } \quad \gamma_{i, j}=1
\end{aligned}
\]</span>
model parameter
<span class="math inline">\(\gamma=\left\{\gamma_{i, j} | j=1, \ldots, m-1 ; i&gt;j\right\}\)</span>.</p>
</div>
<div id="likelihood-1" class="section level2">
<h2><span class="header-section-number">26.2</span> Likelihood</h2>
<p>The likelihood of the parameters <span class="math inline">\((B, D, \gamma)\)</span> is the density of <span class="math inline">\(e=\left(e_{1}^{\prime}, \ldots, e_{n}^{\prime}\right)^{\prime}\)</span> given <span class="math inline">\((B, D, \gamma)\)</span>, that is,
<span class="math display">\[
\begin{array}{l}
p(e | B, D, \gamma) \\
\quad=(2 \pi)^{-n m / 2}|D|^{n / 2} \exp \left\{-\frac{1}{2} \sum_{i=1}^{n} e_{i}^{\prime} B D B^{\prime} e_{i}\right\} \\
\quad=(2 \pi)^{-n m / 2} \prod_{i=1}^{m}\left(d_{i}\right)^{n / 2} \exp \left\{-\frac{1}{2} \sum_{k=1}^{m} d_{k} b_{k}^{\prime} A b_{k}\right\}
\end{array}
\]</span>
where <span class="math inline">\(b_{k}=\left(b_{1, k}, \dots, b_{m, k}\right)^{\prime}\)</span> is the kth column of <span class="math inline">\(B\)</span> and <span class="math inline">\(A=\sum_{i=1}^{n} e_{i} e_{i}^{\prime}\)</span>. A is PD almost surely if <span class="math inline">\(m\leq n\)</span>.</p>
<p>Denote the free parameters in <span class="math inline">\(\beta_k=\{b_{i,k}|i&gt;k, \gamma_{i, k}=1\}\)</span> the quadratic form in likelihood,</p>
<blockquote>
<p>The likelihood from step 2 to 3 is obtained by the matrix multiple by column expansion, which is use column vector to present the matrix multiplication.</p>
</blockquote>
<p>can be written in terms of a quadratic function of <span class="math inline">\(\beta_k\)</span>, so that
<span class="math display">\[
b_{k}^{\prime} A b_{k}=\left\{\begin{array}{ll}
a_{k, k}+2 \beta_{k}^{\prime} a_{k}+\beta_{k}^{\prime} A_{k} \beta_{k} &amp; \text { for } k=1, \ldots, m-1 \\
a_{m, m} &amp; \text { for } k=m
\end{array}\right.
\]</span>
with
<span class="math display">\[
a_{k}=\left\{a_{i, k} | i&gt;k, \gamma_{i, k}=1\right\}
\]</span>
and
<span class="math display">\[
A_{k}=\left\{a_{i, j} | i&gt;k, j&gt;k, \gamma_{i, j}=1\right\}
\]</span>
The unconstrained elements in kth column: the dimension of <span class="math inline">\(\beta_k\)</span> is <span class="math inline">\(q_k\)</span>, then the total number of unconstrained elements in B in model <span class="math inline">\(\gamma\)</span> is <span class="math inline">\(q_{\gamma}=\sum_{k=1}^{m-1} q_{k}\)</span>.
Then the likelihood can be expressed as</p>
<p><span class="math display">\[\begin{align*}
p\left(e | B_{\gamma}, D, \gamma\right)=(2 \pi)^{-n m / 2} \prod_{k=1}^{m}\left(d_{k}\right)^{n / 2}\\

\times \exp \left(-\frac{d_{k}}{2}\left\{S_{k}(\gamma)+\left(\beta_{k}-m_{k}\right)^{\prime} A_{k}\left(\beta_{k}-m_{k}\right)\right\}\right)
\end{align*}\]</span>
where <span class="math inline">\(m_{k}=-A_{k}^{-1} a_{k}\)</span> and <span class="math inline">\(S_{k}(\gamma)=a_{k, k}-a_{k}^{\prime} A_{k}^{-1} a_{k}\)</span></p>
</div>
<div id="prior-specification-1" class="section level2">
<h2><span class="header-section-number">26.3</span> Prior Specification</h2>
<p>Use conditional prior <span class="math inline">\(p(B | \gamma, D)\)</span> similar to prior used to variable subset uncertainty in linear regression.</p>
<p>When given <span class="math inline">\(\gamma\)</span>, some entries of <span class="math inline">\(B\)</span> are fixed to be zero, then the remain unconstrained elements denoted as <span class="math inline">\(B_\gamma\)</span>. The fractional conditional prior for <span class="math inline">\(B_\gamma\)</span> is by setting</p>
<p><span class="math display">\[
p\left(B_{\gamma} | \gamma, D\right) \propto p(e | B, D, \gamma)^{1 / n}
\]</span>
Then
<span class="math display">\[
\begin{array}{c}
p\left(\beta_{1}, \ldots, \beta_{m} | D, \gamma\right)=\prod_{k=1}^{m-1} p\left(\beta_{k} | D, \gamma\right) \quad \text { with } \\
\beta_{k} | D, \gamma \sim N\left(m_{k}, \Omega_{k}\right)
\end{array}
\]</span>
where <span class="math inline">\(m_k\)</span> is as defined earlier and <span class="math inline">\(\Omega_{k}=\frac{n}{d_{k}} A_{k}^{-1}\)</span>.
<span class="math inline">\(\gamma_{i,j}\)</span>, with <span class="math inline">\(j=1,...,m-1\)</span> and <span class="math inline">\(i&gt;j\)</span>, prior independent with <span class="math inline">\(p\left(\gamma_{i, j}=1 | \omega\right)=\omega\)</span>. Then the parsimonious measure of model is <span class="math inline">\(m(m-1) \omega / 2\)</span>.</p>
<p>The parameter <span class="math inline">\(\omega\)</span> is integrated out of the analsis, with
<span class="math display">\[
\begin{aligned}
p(\gamma) &amp;=\int p(\gamma | \omega) p(\omega) d \omega \\
&amp;=\int_{0}^{1} \omega^{q_{\gamma}}(1-\omega)^{\left(r-q_{\gamma}\right)} d \omega=\operatorname{beta}\left(q_{\gamma}+1, r-q_{\gamma}+1\right)
\end{aligned}
\]</span>
with <span class="math inline">\(r=m(m-1)/2\)</span>.</p>
<p>prior for D is as usual Gamma distribution
<span class="math display">\[
\begin{array}{c}
p\left(d_{1}, \ldots, d_{m} | \xi, \kappa\right)=\prod_{k=1}^{m} p\left(d_{k} | \xi, \kappa\right), \quad \text { where } \\
p\left(d_{k} | \xi, \kappa\right)=\frac{d_{k}^{\xi / \kappa-1} \exp \left(-d_{k} / \kappa\right)}{\Gamma(\xi / \kappa) \kappa^{\xi / \kappa}}
\end{array}
\]</span></p>
<p>the <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\kappa\)</span> is hyper-parameter, which is insensitive, so set <span class="math inline">\(\xi=100\)</span> and <span class="math inline">\(\kappa=1000\)</span>.</p>
<p>Also shrinkage prior of <span class="math inline">\(d_i\)</span> are considered,
<span class="math inline">\(log(d_i)\)</span> are independent and <span class="math inline">\(N(\mu_d,\tau_d)\)</span> and the prior for <span class="math inline">\(\mu_d\)</span> and <span class="math inline">\(\tau_d\)</span> is <span class="math inline">\(p\left(\mu_{d}, \tau_{d}\right) \propto 1 / \tau_{d}\)</span>.</p>
<p>log-normal prior requires MH step for generation of <span class="math inline">\(d_i\)</span>.</p>
<p>These two priors perform similar.</p>
</div>
<div id="inference-and-simulation-method." class="section level2">
<h2><span class="header-section-number">26.4</span> Inference And Simulation Method.</h2>
<p>Model-wise average:
Posterior Mean estimatiors:
<span class="math display">\[
E(B | e)=\sum_{\gamma} E(B | \gamma, e) p(\gamma | e)
\]</span>
by
<span class="math display">\[
\widehat{B}=J^{-1} \sum_{j=1}^{J} E\left(B | \gamma^{[j]}, D^{[j]}, e\right)
\]</span>
Similarly for D is
<span class="math display">\[
\widehat{D}=J^{-1} \sum_{j=1}^{J} E\left(D | \gamma^{[j]}, B^{[j]}, e\right)
\]</span>
Then the estimate of <span class="math inline">\(\Sigma\)</span> is
<span class="math display">\[
\widehat{\Sigma^{-1}}=J^{-1} \sum_{j=1}^{J} B^{[j]} D^{[j]}\left(B^{\prime}\right)^{[j]}
\]</span>
The estimate of number of non-zero elements is
<span class="math display">\[
\hat{\omega}=\frac{1}{J} \sum_{j=1}^{J} q_{\gamma^{U 1}} / r
\]</span></p>
<p>The probability of <span class="math inline">\(p\left(\gamma_{i, k}=1 | e\right)\)</span> can be obtained by
<span class="math display">\[
\hat{p}\left(\gamma_{i, k}=1 | e\right)=\frac{1}{J} \sum_{j=1}^{J} \gamma_{i, k}^{[j]}
\]</span>
that is, whether i-k entry are 1 in different models.</p>
<blockquote>
<p>Not very related, estimators for other loss function:</p>
</blockquote>
<p>Based on Yang and Berger(1994)</p>
<p><span class="math display">\[
L_{1}(\widehat{\Sigma}, \Sigma)=\operatorname{tr}\left(\widehat{\Sigma} \Sigma^{-1}\right)-\log \left|\widehat{\Sigma} \Sigma^{-1}\right|-m
\]</span>
and
<span class="math display">\[
L_{2}(\widehat{\Sigma}, \Sigma)=\operatorname{tr}\left(\widehat{\Sigma} \Sigma^{-1}-I\right)^{2}
\]</span></p>
<div id="markov-chain-monte-carlo-sampling-1" class="section level3">
<h3><span class="header-section-number">26.4.1</span> Markov Chain Monte Carlo Sampling</h3>
<ol style="list-style-type: decimal">
<li>Generate from <span class="math inline">\(B | \gamma, D, e\)</span></li>
<li>Generate from <span class="math inline">\(D | B, \gamma, e\)</span></li>
<li>Generate from <span class="math inline">\(\gamma_{i, j} | \gamma_{| i, j}, D, e, \quad\)</span> for <span class="math inline">\(j=1, \ldots, m-1\)</span>
<span class="math inline">\(i&gt;j\)</span></li>
</ol>
<p>For B, the parameters <span class="math inline">\(\beta_k\)</span> is independent MVN
For D, the posterior distribution is independent Gamma distribution.</p>
<p>Prior is not primarily for B and D to generate likelihood. Prior also can be correct using MH algorithm.</p>
<blockquote>
<p>也就是说，主要部分都是一个Gaussian加一个Gamma，所以能直接抽，最多后面加一个MH对于prior做修正。 具体算法细节得从另外的部分开始，但是可以参考对于indicator的转移核的抽法。或者直接说就是，用likelihood部分的MVN和Gamma做proposal，这样新的和旧的就差一个prior这样。大概是？没推，但是直观上是这样。</p>
</blockquote>
<p>3， generate <span class="math inline">\(\gamma_{i,j}\)</span></p>
<p><span class="math display">\[
\underbrace{p\left(\gamma_{i, j} | \gamma_{| i, j}, D, e\right)}_{\pi^{*}\left(\gamma_{i, j}\right)} \propto \underbrace{p(e | \gamma, D)}_{l\left(\gamma_{i, j}\right)} \underbrace{p\left(\gamma_{i, j} | \gamma_{\backslash i, j}\right)}_{\pi\left(\gamma_{i, j}\right)}
\]</span></p>
<blockquote>
<p>This transition kernel idea is important and useful for my project.</p>
</blockquote>
<p>The transition kernel to generate <span class="math inline">\(\gamma_{i,j}\)</span> is
<span class="math display">\[
Q\left(\gamma_{i, j}^{c}=1 \rightarrow \gamma_{i, j}^{g}=0\right)=\pi\left(\gamma_{i, j}=0\right) \frac{l\left(\gamma_{i, j}=0\right)}{l\left(\gamma_{i, j}=1\right)+l\left(\gamma_{i, j}=0\right)}
\]</span>
and
<span class="math display">\[
Q\left(\gamma_{i, j}^{c}=0 \rightarrow \gamma_{i, j}^{g}=1\right)=\pi\left(\gamma_{i, j}=1\right) \frac{l\left(\gamma_{i, j}=1\right)}{l\left(\gamma_{i, j}=1\right)+l\left(\gamma_{i, j}=0\right)}
\]</span>
<span class="math inline">\(\gamma^{c}_{i,j}\)</span> is current value of <span class="math inline">\(\gamma_{i,j}\)</span> and <span class="math inline">\(\gamma^{g}_{i,j}\)</span> is the generated <span class="math inline">\(\gamma_{i,j}\)</span> is
<span class="math display">\[Q\left(\gamma_{i, j}^{c}=1 \rightarrow \gamma_{i, j}^{g}=0\right)=\pi\left(\gamma_{i, j}=0\right) \frac{l\left(\gamma_{i, j}=0\right)}{l\left(\gamma_{i, j}=1\right)+l\left(\gamma_{i, j}=0\right)}\]</span>
and
<span class="math display">\[Q\left(\gamma_{i, j}^{c}=0 \rightarrow \gamma_{i, j}^{g}=1\right)=\pi\left(\gamma_{i, j}=1\right) \frac{l\left(\gamma_{i, j}=1\right)}{l\left(\gamma_{i, j}=1\right)+l\left(\gamma_{i, j}=0\right)}\]</span></p>
<p>Kohn et al(2001) showed that this transition kenerl satisfied detailed balance criteria.</p>
<p>Then the algorithm can be obtained by</p>
<ol style="list-style-type: decimal">
<li>Generate <span class="math inline">\(u\)</span> from a uniform distribution on <span class="math inline">\((0,1)\)</span></li>
<li><ol style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(\gamma_{i, j}^{c}=1\)</span> and <span class="math inline">\(u&gt;\pi\left(\gamma_{i, j}=0\right),\)</span> then set <span class="math inline">\(\gamma_{i, j}^{g}=1\)</span></li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li>If <span class="math inline">\(\gamma_{i, j}^{c^{\prime}}=1\)</span> and <span class="math inline">\(u&lt;\pi\left(\gamma_{i, j}=0\right),\)</span> then generate <span class="math inline">\(\gamma_{i, j}^{g}\)</span>
from the density
<span class="math display">\[
\operatorname{Pr}\left(\gamma_{i, j}=0\right)=\frac{l\left(\gamma_{i, j}=0\right)}{l\left(\gamma_{i, j}=1\right)+l\left(\gamma_{i, j}=0\right)}
\]</span></li>
<li>If <span class="math inline">\(\gamma_{i, j}^{c}=0\)</span> and <span class="math inline">\(u&gt;\pi\left(\gamma_{i, j}=1\right),\)</span> then set <span class="math inline">\(\gamma_{i, j}^{g}=0\)</span></li>
<li>If <span class="math inline">\(\gamma_{i, j}^{c}=0\)</span> and <span class="math inline">\(u&lt;\pi\left(\gamma_{i, j}=1\right),\)</span> then generate <span class="math inline">\(\gamma_{i, j}^{g}\)</span> from the density
<span class="math display">\[
\operatorname{Pr}\left(\gamma_{i, j}=1\right)=\frac{l\left(\gamma_{i, j}=1\right)}{l\left(\gamma_{i, j}=1\right)+l\left(\gamma_{i, j}=0\right)}
\]</span></li>
</ol>
<p>in case (a)(c), only need evaluate prior <span class="math inline">\(\pi(\gamma_{i,j}=1)\)</span> rather than full posterior.</p>
<p>In examples, most of the lower-triangular elements are identified as non-zero. (a) most frequently.</p>
</div>
</div>
<div id="simulation-study" class="section level2">
<h2><span class="header-section-number">26.5</span> Simulation Study</h2>
<p>Cholesky Decomposition: SK estimator</p>
<p>compared with</p>
<p>Yang and Berger: YB84 estimator</p>
<p>Example 1: Homoscedastic System: <span class="math inline">\(B=I, D=I\)</span>,so in this case <span class="math inline">\(\Sigma=I\)</span>.</p>
<p>Example 2: Heteroscedastic System. <span class="math inline">\(B=I\)</span> and <span class="math inline">\(D=diag(\frac{1}{m}, \frac{1}{m-1}, \dots, 1)\)</span>, so <span class="math inline">\(\Sigma=\operatorname{diag}(m, m-1, \dots, 1)\)</span>.</p>
<p>Example 3, AR: <span class="math inline">\(\theta=0.8\)</span>, <span class="math inline">\(b_{i+1,i}=-\theta\)</span> for <span class="math inline">\(i=1,...,m-1;b_{i,j}=0\)</span> for <span class="math inline">\(j=1,...,m-2\)</span> and <span class="math inline">\(i&gt;j+1\)</span>; <span class="math inline">\(d_m=100/(1-\theta^2)\)</span> and <span class="math inline">\(d_i=100\)</span> for <span class="math inline">\(i=2,...,m\)</span>.</p>
<p>Example 4: MA: MA(1), in this case, the lower triangular factor of <span class="math inline">\(B\)</span> is full, <span class="math inline">\(b_{i,j}\neq 0\)</span> for all <span class="math inline">\(i&gt;j\)</span>. band of <span class="math inline">\(B\)</span> are of <span class="math inline">\(O(-\theta^i)\)</span>.</p>
<p>cases: <span class="math inline">\((m,n)\)</span>, (5,40),(15,40),(30,100)</p>
<p><span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(\Sigma^{-1}\)</span> were estimated for the simulated data, using the Bayes estimator for squared error loss (posterior means) and <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> loss function.</p>
<p>MLE is</p>
<p><span class="math inline">\(\widehat{\Sigma}=\frac{1}{n} \sum_{i=1}^{n} e_{i} e_{i}^{\prime}\)</span>
and <span class="math inline">\(\quad \widehat{\Sigma^{-1}}=(\widehat{\Sigma})^{-1}\)</span></p>
<p>The result of Bayesian posterior mean estimators and MLE were assessed by calculating the squared error loss function</p>
<p><span class="math display">\[
D(\widehat{\Sigma}, \Sigma)=\frac{1}{m^{2}}\left(\sum_{i} \sum_{j}\left(\hat{\sigma}_{i j}-\sigma_{i j}\right)^{2}\right)^{1 / 2}
\]</span></p>
<p><span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> was assessed relative to these loss functions.</p>
<blockquote>
<p>所以表格内容是在三种评价标准，matrix distance measure，<span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> 下，YB94， SK 和 MLE的表现。</p>
</blockquote>
<blockquote>
<p>用parsomonia的情况下，SK的效率稍微高一点，但是如果是full的YB94效率会更高一点。</p>
</blockquote>
<p>MH rate of YB94 is around 5%.</p>
</div>
<div id="application" class="section level2">
<h2><span class="header-section-number">26.6</span> Application</h2>
<p>###Non-zero mean case:</p>
<p>Modified as:
<span class="math display">\[
y=X \alpha+e
\]</span>
Then <span class="math inline">\(e \sim N\left(0, I_{n} \otimes \Sigma\right)\)</span>, y is <span class="math inline">\(mn\times 1\)</span> observation vector, <span class="math inline">\(\alpha\)</span> is vector of regression coefficients. prior of <span class="math inline">\(\alpha\)</span> is proportion to constant.</p>
<p>Then conditional posterior density of <span class="math inline">\(\alpha\)</span> is
<span class="math display">\[
\begin{aligned}
p\left(\boldsymbol{\alpha} | y, B_{\gamma}, D, \gamma\right) &amp; \propto p\left(y | \alpha, B_{\gamma}, D, \gamma\right) p\left(B_{\gamma} | \gamma, \alpha, D\right) \\
&amp; \propto p\left(y | \alpha, B_{\gamma}, D, \gamma\right)^{1+1 / n}
\end{aligned}
\]</span></p>
<p>Conditional posterior density of normal with mean <span class="math inline">\(\left(X^{\prime}\left(I \otimes \Sigma^{-1}\right) X\right)^{-1} X^{\prime}\left(I \otimes \Sigma^{-1}\right) y\)</span> and covariance matrix <span class="math inline">\(n(1+n)^{-1}\left(X^{\prime}\left(I \otimes \Sigma^{-1}\right) X\right)^{-1}\)</span>. mixture estimate of <span class="math inline">\(\alpha\)</span> is
<span class="math display">\[
\begin{aligned}
\hat{\alpha} &amp;=\frac{1}{J} \sum_{k=1}^{J} E\left(\alpha | y, \Sigma^{[k]}\right) \\
&amp;=\frac{1}{J} \sum_{k=1}^{J}\left(X^{\prime}\left(I \otimes\left(\Sigma^{-1}\right)^{[k]}\right) X\right)^{-1} X^{\prime}\left(I \otimes\left(\Sigma^{-1}\right)^{[k]}\right) y
\end{aligned}
\]</span></p>
<div id="longitudinal-case" class="section level3">
<h3><span class="header-section-number">26.6.1</span> Longitudinal case</h3>
<p><span class="math display">\[
\begin{aligned}
\log \left(y_{t, i}\right)=\left(a_{k}+b_{k} T_{t}+c_{k} T_{t}^{2}\right)+e_{t, i} &amp; \\
\text { for } i=1, \ldots, n, t &amp;=1, \ldots, m
\end{aligned}
\]</span></p>
<p>error structure is independent across cows, but not across time.</p>
<p><span class="math inline">\(e_{i}=\left(e_{1, i}, \dots, e_{m, i}\right)&#39;\)</span> are independent distributed <span class="math inline">\(N(0,\Sigma)\)</span>.</p>
</div>
<div id="intraday-elextricity-demand." class="section level3">
<h3><span class="header-section-number">26.6.2</span> Intraday Elextricity Demand.</h3>
<p>— tbc</p>
</div>
</div>
<div id="conclusion-2" class="section level2">
<h2><span class="header-section-number">26.7</span> Conclusion</h2>
<p>This method may also be useful for some cross-sectional covariance matrices. Can be applied to high-dimensional covariance matrices. YB94 is impractical to apply because high rejection rate Metropolis-Hastings step becomes computationally prohibitive as the size of the matrix increases.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="integrated-nested-laplace-approximationsinla.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
