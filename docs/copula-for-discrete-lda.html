<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 3 Copula for discrete LDA | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 3 Copula for discrete LDA | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 3 Copula for discrete LDA | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2019-06-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html">
<link rel="next" href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><i class="fa fa-check"></i><b>11</b> Bayesian Modeling of Joint Regressions for the Mean and Covariance Matrix</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#abstract-7"><i class="fa fa-check"></i><b>11.1</b> Abstract</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#introduction-9"><i class="fa fa-check"></i><b>11.2</b> Introduction</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#the-model"><i class="fa fa-check"></i><b>11.3</b> The model</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#bayesian-methodology"><i class="fa fa-check"></i><b>11.4</b> Bayesian Methodology</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#centering-and-order-methods"><i class="fa fa-check"></i><b>11.5</b> Centering and Order Methods</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><i class="fa fa-check"></i><b>12</b> MAXIMUM LIKELIHOOD ESTIMATION IN TRANSFORMED LINEAR REGRESSION WITH NONNORMAL ERRORS</a><ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#abstract-8"><i class="fa fa-check"></i><b>12.1</b> Abstract</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#introduction-10"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#parameter-estimation-and-inference-procedure"><i class="fa fa-check"></i><b>12.3</b> Parameter estimation and inference procedure</a><ul>
<li class="chapter" data-level="12.3.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#mle"><i class="fa fa-check"></i><b>12.3.1</b> MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><i class="fa fa-check"></i><b>13</b> Homogeneity tests of covariance matrices with high-dimensional longitudinal data</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="copula-for-discrete-lda" class="section level1">
<h1><span class="header-section-number">Paper 3</span> Copula for discrete LDA</h1>
<p>Lack of suitable mutivariate joint distributions for discrete variables that incorporate with the correlation.</p>
<p>Maximizing the full likelihooe function constructed from copula representation can be computational infeasible–&gt; Composite pairwise likelihood approach.</p>
<p>Copula usage: Gaussian copula:Formally:</p>
<p>A set of random variables <span class="math inline">\(\boldsymbol{U}=\left(U _ {1}, \dots, U _ {d}\right)^{\mathrm{T}}\)</span> follows a Gaussian copula model if their joint distribution is specified by
<span class="math display">\[
F\left(u  _  {1}, \ldots, u  _  {d}\right)=P\left(U  _  {1} \leq u  _  {1}, \ldots, U  _  {d} \leq u  _  {d}\right)=\Phi  _  {d}\left(v  _  {1}, \ldots, v  _  {d} ; \mathbf{R}\right)
\]</span>
<span class="math inline">\(\Phi _ d\)</span> is the porbability distribution function of the d-dimensional standardised normal distribution with zero mean, <span class="math inline">\(\mathbf R\)</span> is the correlation matrix.</p>
<p><span class="math inline">\(v _ i=\Phi^{-1} (w _ i)\)</span> Where <span class="math inline">\(w _ i=P(U _ i\leq u _ i)\)</span> is the marginal distribution of <span class="math inline">\(U _ i\leq i\leq d\)</span>.</p>
<p>Other copulas, like t-copula can also be applied.</p>
<p>Notation: standard Longitudinal data format: <span class="math inline">\(\mathbf{y} _ {i}=\left(y _ {i 1}, \ldots, y _ {i m _ {i}}\right)^{\mathrm{T}}\)</span> , <span class="math inline">\(m _ i\)</span> longitudinal measurements fr i-th subject, discrete response <span class="math inline">\(y _ {ij}\)</span> is observed at time <span class="math inline">\(t _ {ij}\)</span> .Consider <span class="math inline">\(y _ {i j} \in\{0,1,2, \ldots\}\)</span> . and <span class="math inline">\(\mathbf{t} _ {i}=\left(t _ {i 1}, \ldots, t _ {i m _ {i}}\right)^{\mathrm{T}}\)</span></p>
<p><span class="math inline">\(\{ y _ {ij},x _ {ij},t _ {ij} \}(i=1,...,n;j=1,...,m _ i).\)</span> For categorical responses, we assume that <span class="math inline">\(y _ {ij}\)</span> follows exponential family distribution (so that GLMs can be used modelling marginally discrete responses.)
<span class="math display">\[
f(y)=c(y ; \varphi) \exp [\{y \theta-\psi(\theta)\} / \varphi]
\]</span>
Since <span class="math inline">\(\psi&#39;(\theta)=E(Y):=\mu\)</span>.</p>
<p>canonical link function:
<span class="math display">\[
(\psi&#39;)^{-1} (\mu)=g(\mu)
\]</span>
Then the GLM marginally for each <span class="math inline">\(y _ {ij}\)</span> as
<span class="math display">\[
g(E(y _ {ij}))=g(\mu _ {ij})=x^T _ {ij}\beta
\]</span>
<span class="math inline">\(\operatorname{var}(y)=\varphi \psi^{\prime \prime}(\theta)\)</span></p>
<p>Joint distribution of <span class="math inline">\(y _ i\)</span> following the Gaussian copula representation
<span class="math display">\[
F _ {m _ i}(y _ i)=P(Y _ {i1}\leq y _ {i1},...,Y _ {im _ i}\leq y _ {im _ i})=\Phi _ {m _ i}(z _ {i1},...,z _ {im _ i};\mathbf R _ i)
\]</span>
where <span class="math inline">\(z _ {ij}=\Phi^{-1} _ 1 \{F(y _ {ij})\}\)</span></p>
<p>This is, <span class="math inline">\(F(y _ {ij})\)</span> is the probability of <span class="math inline">\(Y _ {i _ 1} \leq y _ {i1}\)</span> , it’s in the probability space [0,1], <span class="math inline">\(\Phi _ 1^{-1} \{F(y _ {ij})\}\)</span> make the value from probability space back to sample space. Then this is in continuous sample space, then use multivariate Normal distribution as the joint distribution.</p>
<p>In a special case when the responses are binary, the correlation between two observations is a monotone function of the corresponding element in <span class="math inline">\(R _ i\)</span> .</p>
<p>The HPC insed in decomposite correlation coefficients <span class="math inline">\(R _ i\)</span> .</p>
<p>…</p>
<p>The description of HPC</p>
<p>…</p>
<p>modelling angles by
<span class="math display">\[
\omega _ {i j k}=\pi / 2-\operatorname{atan}\left(\mathbf{w} _ {i j k}^{\mathrm{T}} \boldsymbol{\gamma}\right)
\]</span>
<span class="math inline">\(\mathbf{w} _ {ijk}\in \mathbb R^{q}\)</span> is a covariate and <span class="math inline">\(\mathbb \gamma\)</span> is a <span class="math inline">\(q\times 1\)</span> unknown parameters.</p>
<p>Let <span class="math inline">\(\boldsymbol{\theta}=\left(\boldsymbol{\beta}^{\mathrm{T}}, \boldsymbol{\gamma}^{\mathrm{T}}, \varphi^{\mathrm{T}}\right)\)</span>. Then follow the framework upon, we can construct likelihood and estimate <span class="math inline">\(\mathbb \theta\)</span> by MLE.</p>
<p>Gaussian copula has continuous support on <span class="math inline">\(\mathbb R^d\)</span> while discrete response variable are defined only on discrete grid points.</p>
<p>Full likelihood:</p>
<p><span class="math display">\[
\begin{aligned} L(\boldsymbol{\theta}) &amp;=\prod _ {i=1}^{n} P\left(Y _ {i 1}=y _ {i 1}, \ldots, Y _ {i m _ {i}}=y _ {i m _ {i}}\right) \\ &amp;=\prod _ {i=1}^{n} P\left(y _ {i 1}-1&lt;Y _ {i 1} \leq y _ {i 1} \leq y _ {i 1}, \ldots, y _ {i m _ {i}}-1&lt;Y _ {i m _ {i}} \leq y _ {i m _ {i}}\right) \\ &amp;=\prod _ {i=1}^{n} \int \cdots \int _ {\mathbf{Z} _ {i}^{-}&lt;\mathbf{u} \leq \mathbf{Z} _ {i}} \phi _ {m _ {i}}\left(\mathbf{u} ; \mathbf{R} _ {i}\right) d \mathbf{u} \end{aligned}
\]</span>
积分形式应该来源于Gaussian copula的定义。</p>
<blockquote>
<p>Laplace 展开对u，精度可能不够，特别是binary</p>
<p>包括pairwise likelihood，需要满足一定条件才能逼近的好</p>
</blockquote>
<p>This is hard to deal with, then the author use the composite likelihood by using pairwise likelihood.</p>
<p>Construct all pairwise likelihoods via bivariate copula as
<span class="math display">\[
p L(\boldsymbol{\theta})=\prod _ {i=1}^{n} \prod _ {1 \leq j&lt;k \leq m _ {i}} \int _ {z _ {i j}^{-}}^{z _ {i j}} \int _ {z _ {i k}^{-}}^{z _ {i k}} \phi _ {2}\left(\mathbf{u} ; \rho _ {i j k}\right) d \mathbf{u}
\]</span>
That is, using two-integral instead of <span class="math inline">\(m _ i\)</span> integrals.</p>
<p><span class="math inline">\(\rho _ {ijk}\)</span> is specified by the HPC.</p>
<p>log pairwise likelihood function is
<span class="math display">\[
p l(\boldsymbol{\theta})=\sum _ {i=1}^{n} \sum _ {1 \leq j&lt;k \leq m _ {i}} \log \int _ {z _ {i j}^{-}}^{z _ {i j}} \int _ {z _ {i k}^{-}}^{z _ {i k}} \phi _ {2}\left(\mathbf{u} ; \rho _ {i j k}\right) d \mathbf{u} :=\sum _ {i=1}^{n} \sum _ {1 \leq j&lt;k \leq m _ {i}} l _ {i j k}(\theta)
\]</span>
and the score function is
<span class="math display">\[
\mathbf{S} _ {n}(\boldsymbol{\theta})=\frac{\partial p l}{\partial \boldsymbol{\theta}}=\sum _ {i=1}^{n} \sum _ {1 \leq j&lt;k \leq m _ {i}} \frac{\partial l _ {i j k}}{\partial \boldsymbol{\theta}} :=\sum _ {i=1}^{n} \mathbf{S} _ {n i}(\boldsymbol{\theta})
\]</span>
Employ modified Fisher scoring algorithm to maximize the pairwise likelihood function.</p>
<p>Song, et. al. 2009</p>
<div id="joint-regression-analysis-of-correlated-data-using-gaussian-copulas" class="section level2">
<h2><span class="header-section-number">3.1</span> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</h2>
<p>Estimating equations (EE)-based approach. Join marginal models for correlated outcomes. Shortcomings associated with the EE method due to lack of fuly parametric model: 1. loss of estimation efficiency 2. the lack of procedures for model assessment and selection, 3. Difficulty of incorporating vector outcomes of mixed types.</p>
<p>Burn injury data: <span class="math inline">\(y _ 1=log(\textit{burn area}+1)\)</span> <span class="math inline">\(y _ 2\)</span> 1 for death from burn injury and 0 for survival.</p>
<p>Normally people do two seperate regression model but the problem is <span class="math inline">\(y _ 1\)</span> and <span class="math inline">\(y _ 2\)</span> are not independent, it’s joint. How to consider the correlation in these two marginal models?</p>
<p>Develop a unified and flexible likelihood framework to join various marginal models.</p>
<p>Use GLMs as marginal models. To join marginal GLMs, invoke Gaussian copulas as the link model, and the resulting joint regression model is refrred to as the vector GLM(VGLM).</p>
<p>VGLM for correlated discrete outcomes and correlated mixed outcomes. Comparisons to the moment-based EE approach.</p>
<p>Jointly analyze vector data by the GLM approach multidimensional GLMs, or VGLMs, specify the conditional distribution of a vector response y given x as follows :
<span class="math display">\[
f(\mathbf{y} | \mathbf{x} ; \boldsymbol{\beta}, \boldsymbol{\varphi}, \Gamma)=\delta\left(\mathbf{y}, \eta _ {1}, \ldots, \eta _ {m} ; \boldsymbol{\varphi}, \Gamma\right)
\]</span>
Parametric link model <span class="math inline">\(\delta(\cdot ;\varphi,\Gamma)\)</span> is parameterized by the vector of dispersion parameters <span class="math inline">\(\boldsymbol \varphi=(\varphi _ 1,...,\varphi _ m)^T\)</span></p>
<p>The <span class="math inline">\(\Gamma\)</span> characterizes the association among the components of <span class="math inline">\(\mathbf y\)</span>.</p>
<p>This article consider a new class of parametric link models <span class="math inline">\(\delta(\cdot)\)</span> via multivariate distributions generated by Gaussian copulas.</p>
<p>Multivariate ED Family distribution</p>
<p>Cumulative distribution function of <span class="math inline">\(ED(\mu _ j,\varphi _ j)\)</span> denoted by <span class="math inline">\(G _ j(y _ j;\mu _ j,\varphi _ j)\)</span>
<span class="math display">\[
F(\mathbf{y} ; \boldsymbol{\mu}, \varphi, \Gamma)=C\left\{G _ {1}\left(y _ {1} ; \mu _ {1}, \varphi _ {1}\right), \ldots, G _ {m}\left(y _ {m} ; \mu _ {m}, \varphi _ {m}\right) | \Gamma\right\}
\]</span>
<span class="math inline">\(\boldsymbol \mu=(\mu _ 1,\cdots,\mu _ m)^T\)</span> is the vector of m means,<span class="math inline">\(\boldsymbol\varphi=(\varphi _ 1,...,\varphi _ m)^T\)</span> is dispersion parameters, <span class="math inline">\(C(\cdot)\)</span> is m-variate Gaussian copula with
<span class="math display">\[
\begin{aligned} C(\mathbf{u} | \Gamma) &amp;=\Phi _ {m}\left\{\Phi^{-1}\left(u _ {1}\right), \ldots, \Phi^{-1}\left(u _ {m}\right) | \Gamma\right\} \\ \mathbf{u} &amp;=\left(u _ {1}, \ldots, u _ {m}\right)^{T} \in(0,1)^{m} \end{aligned}
\]</span>
<span class="math inline">\(\Gamma\)</span> is Pearson correlation matrix. For non-Gaussian margins, <span class="math inline">\(\Gamma\)</span> becomes a pairwise non-linear association, van der Waerden coefficient, which is defined by
<span class="math display">\[
\gamma _ {i j}=\operatorname{corr}\left[\Phi^{-1}\left\{G _ {i}\left(y _ {i}\right)\right\}, \Phi^{-1}\left\{G _ {j}\left(y _ {j}\right)\right\}\right]
\]</span>
For continuous marginal CDF, <span class="math inline">\(\gamma _ {ij}\)</span> represents the linear correlation of two normal scores <span class="math inline">\(\Phi^{-1}\left\{G _ {t}\left(y _ {t}\right)\right\}, t=i, j\)</span> . For discrete cases, the equation still holds but interpretation would be different. For binary case with same joint probability mass function, then the <span class="math inline">\(\gamma _ {ij}\)</span> can be interpreted as the tetrachoric correlation.</p>
<p>The density functions of MEDs with different marginal distribution, in continuous case, the MED can be expressed by
<span class="math display">\[
f(\mathbf{y} ; \boldsymbol{\mu}, \varphi, \Gamma)=c\left\{G _ {1}\left(y _ {1}\right), \ldots, G _ {m}\left(y _ {m}\right) | \Gamma\right\} \prod _ {i=1}^{m} g _ {i}\left(y _ {i} ; \mu _ {i}, \varphi _ {i}\right)
\]</span>
where <span class="math inline">\(c(\cdot)\)</span> is density of the copula <span class="math inline">\(C( \cdot)\)</span>
<span class="math display">\[
c(\mathbf{u} | \Gamma)=|\Gamma|^{-1 / 2} \exp \left\{\frac{1}{2} \mathbf{q}^{T}\left(I _ {m}-\Gamma^{-1}\right) \mathbf{q}\right\}
\]</span>
When all margins are discrete, the joint probability function of a discrete MED distribution with the form:
<span class="math display">\[
\begin{aligned} f(\mathbf{y}) &amp;=\mathrm{P}\left(Y _ {1}=y _ {1}, \ldots, Y _ {m}=y _ {m}\right) \\ &amp;=\sum _ {j _ {1}=1}^{2} \cdots \sum _ {j _ {m}=1}^{2}(-1)^{j _ {1}+\cdots+j _ {m}} C\left(u _ {1, j _ {1}}, \ldots, u _ {m, j _ {m}} | \Gamma\right) \end{aligned}
\]</span>
Another case is the mixed outcome.</p>
<ol start="3" style="list-style-type: decimal">
<li>Simultaneous Maximum Likelihood Inference</li>
</ol>
<p>Responses <span class="math inline">\(\left(\mathbf{y} _ {1}, \ldots, \mathbf{y} _ {n}\right)\)</span> with covariates <span class="math inline">\(\left(X _ {1}, \ldots, X _ {n}\right)\)</span> follows m-variate MED distribution
<span class="math display">\[
\mathbf{y} _ {i}\left|X _ {i}=\left(\mathbf{x} _ {i 1}, \ldots, \mathbf{x} _ {i m}\right) \sim \operatorname{MED} _ {m}\left(\boldsymbol{\mu} _ {i}, \boldsymbol{\varphi} _ {i}, \Gamma\right), \quad i=1, \ldots, n\right.
\]</span>
Responses <span class="math inline">\(\boldsymbol {y_i}\)</span> has mean <span class="math inline">\(\boldsymbol{\mu_i}=\left(\mu _ {i 1}\left(\mathbf{x} _ {i 1}\right), \ldots, \mu _ {i m}\left(\mathbf{x} _ {i m}\right)\right)^{T}\)</span> and dispersion <span class="math inline">\(\varphi _ {i}=\left(\varphi _ {i 1}, \ldots, \varphi _ {i m}\right)^{T}\)</span> .</p>
<p>Mean <span class="math inline">\(\mu _ i\)</span> follows marginal GLM $h _ j(_ {ij})=_ j(x _ {ij}) $ with <span class="math inline">\(\eta _ {i j}=\mathbf{x} _ {i j}^{T} \boldsymbol{\beta} _ {j}\)</span></p>
<p>Let <span class="math inline">\(\theta=(\beta,\varphi,\Gamma)\)</span> . For normal longitudinal or clustered data analysis, VGLM with <span class="math inline">\(\Gamma\)</span> has some specific form like AR(1), that is <span class="math inline">\(\Gamma(\alpha)\)</span></p>
<p>Loglikelihood： <span class="math inline">\(\ell(\boldsymbol{\theta} ; Y, X)=\sum _ {i=1}^{n} \ell _ {i}\left(\boldsymbol{\theta} ; \mathbf{y} _ {i}, X _ {i}\right)\)</span>，then MLE <span class="math inline">\(\hat{\boldsymbol{\theta}}=\underset{\boldsymbol{\theta}}{\operatorname{argmax}} \ell(\boldsymbol{\theta} ; Y, X)\)</span> .</p>
<p>Search for result we use Gauss-Newton type algorithm.</p>
<p>The observed fisher Information using the sandwich form: <span class="math inline">\(\hat{\mathcal{I}}=\mathbf{A} _ {n}^{-1}(\hat{\boldsymbol{\theta}}) \mathbf{B} _ {n}(\hat{\boldsymbol{\theta}}) \mathbf{A} _ {n}^{-1}(\hat{\boldsymbol{\theta}})\)</span> where <span class="math inline">\(\mathbf{A} _ {n}(\theta)\)</span> is the numerical Hessian matrix and <span class="math inline">\(B _ n(\theta)=\frac{1}{n} \sum _ {i=1}^{n} \dot{\ell} _ {i}\left(\boldsymbol{\theta} ; \mathbf{y} _ {i}, X _ {i}\right) \dot{\ell} _ {i}\left(\boldsymbol{\theta} ; \mathbf{y} _ {i}, X _ {i}\right)^{T}\)</span> . The iteration updates of parameter <span class="math inline">\(\theta\)</span> is by
<span class="math display">\[
\theta^{k+1}=\theta^{k}+\epsilon\left\{\mathbf{B} _ {n}\left(\theta^{k}\right)\right\}^{-1} \dot{\ell}\left(\theta^{k}\right)
\]</span>
<span class="math inline">\(\epsilon\)</span> is step-halving term. Among Newton-Raphson, downhill simplex, quasi-Newton with numerical derivatives, the Gauss-Newton appears to privde the best trade-off.</p>
<ol start="4" style="list-style-type: decimal">
<li>VGLMs for Trivariate Discrete Data</li>
</ol>
<p>Moment-based EE method .etc</p>
<p>Trivariate VGLMs</p>
<p>probability mass function:
<span class="math display">\[
\begin{aligned} f\left(\mathbf{y} _ {i} ; \boldsymbol{\theta}\right) &amp;=P\left(Y _ {i 1}=y _ {i 1}, Y _ {i 2}=y _ {i 2}, Y _ {i 3}=y _ {i 3}\right) \\ &amp;=\sum _ {j _ {1}=1}^{2} \sum _ {j _ {2}=1}^{2} \sum _ {j _ {3}=1}^{2}(-1)^{j _ {1}+j _ {2}+j _ {3}} C\left(u _ {i, 1, j _ {1}}, u _ {i, 2, j _ {2}}, u _ {i, 3, j _ {3}} | \alpha\right) \end{aligned}
\]</span>
where <span class="math inline">\(C\left(u _ {i, 1, j _ {1}}, u _ {i, 2, j _ {2}}, u _ {i, 3, j _ {3}} | \alpha\right)=\Phi _ {3}\left\{\Phi^{-1}\left(u _ {i, 1, j _ {1}}\right), \Phi^{-1}\left(u _ {i, 2, j _ {2}}\right)\right. ,\Phi^{-1}\left(u _ {i, 3, j _ {3}}\right) | \alpha \}\)</span>with <span class="math inline">\(\alpha\)</span> is exchangeable correlation coefficient <span class="math inline">\(\alpha\)</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
