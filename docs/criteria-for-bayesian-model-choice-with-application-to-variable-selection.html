<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Paper 18 Criteria for Bayesian Model Choice With Application to Variable Selection | A paper reading record</title>
  <meta name="description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Paper 18 Criteria for Bayesian Model Choice With Application to Variable Selection | A paper reading record" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Paper 18 Criteria for Bayesian Model Choice With Application to Variable Selection | A paper reading record" />
  
  <meta name="twitter:description" content="This is a reading record and notes for paper reading, in order to make me concentrate and remind of the paper I have read." />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2019-10-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html">
<link rel="next" href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Mini paper reading record</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="reading-review.html"><a href="reading-review.html"><i class="fa fa-check"></i>reading review</a><ul>
<li class="chapter" data-level="0.1" data-path="reading-review.html"><a href="reading-review.html#reviews-1-in-april2019"><i class="fa fa-check"></i><b>0.1</b> reviews 1 in April,2019</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><i class="fa fa-check"></i><b>1</b> Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties</a><ul>
<li class="chapter" data-level="1.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract:</a></li>
<li class="chapter" data-level="1.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#introduction-1"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-variable-selection"><i class="fa fa-check"></i><b>1.3</b> Penalized Least Squares And Variable Selection</a><ul>
<li class="chapter" data-level="1.3.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#smoothly-clipped-absolute-deviation-penalty"><i class="fa fa-check"></i><b>1.3.1</b> Smoothly Clipped Absolute Deviation Penalty</a></li>
<li class="chapter" data-level="1.3.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#performance-of-thresholding-rules"><i class="fa fa-check"></i><b>1.3.2</b> Performance of Thresholding Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#variable-selection-via-penalized-likelihood"><i class="fa fa-check"></i><b>1.4</b> Variable selection via penalized likelihood</a><ul>
<li class="chapter" data-level="1.4.1" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#penalized-least-squares-and-likelihood"><i class="fa fa-check"></i><b>1.4.1</b> Penalized Least Squares and Likelihood</a></li>
<li class="chapter" data-level="1.4.2" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#sampling-properties-and-oracle-properties."><i class="fa fa-check"></i><b>1.4.2</b> Sampling properties and oracle properties.</a></li>
<li class="chapter" data-level="1.4.3" data-path="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html"><a href="variable-selection-via-nonconcave-penalized-likelihood-and-its-oracle-properties.html#a-new-unified-algorithm"><i class="fa fa-check"></i><b>1.4.3</b> A new Unified Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><i class="fa fa-check"></i><b>2</b> Random effects selection in generalized linear mixed models via shrinkage penalty function</a><ul>
<li class="chapter" data-level="2.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#abstract-1"><i class="fa fa-check"></i><b>2.1</b> Abstract:</a></li>
<li class="chapter" data-level="2.2" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#introduction-2"><i class="fa fa-check"></i><b>2.2</b> Introduction</a></li>
<li class="chapter" data-level="2.3" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#methodology-and-algorithm"><i class="fa fa-check"></i><b>2.3</b> Methodology and Algorithm</a><ul>
<li class="chapter" data-level="2.3.1" data-path="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html"><a href="random-effects-selection-in-generalized-linear-mixed-models-via-shrinkage-penalty-function.html#algorithms"><i class="fa fa-check"></i><b>2.3.1</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html"><i class="fa fa-check"></i><b>3</b> Copula for discrete LDA</a><ul>
<li class="chapter" data-level="3.1" data-path="copula-for-discrete-lda.html"><a href="copula-for-discrete-lda.html#joint-regression-analysis-of-correlated-data-using-gaussian-copulas"><i class="fa fa-check"></i><b>3.1</b> Joint Regression Analysis of Correlated Data Using Gaussian Copulas</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><i class="fa fa-check"></i><b>4</b> Estimation of random-effects model for longitudinal data with nonignorable missingness using Gibbs sampling</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#abstract-2"><i class="fa fa-check"></i><b>4.1</b> Abstract:</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#introduction-3"><i class="fa fa-check"></i><b>4.2</b> Introduction:</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#proposed-model"><i class="fa fa-check"></i><b>4.3</b> Proposed model</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#modelling-time-varying-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Modelling time-varying coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html"><a href="estimation-of-random-effects-model-for-longitudinal-data-with-nonignorable-missingness-using-gibbs-sampling.html#bayesian-estimation-using-gibbs-sampler"><i class="fa fa-check"></i><b>4.3.2</b> Bayesian estimation using Gibbs sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><i class="fa fa-check"></i><b>5</b> Sparse inverse covariance estimation with the graphical lasso</a><ul>
<li class="chapter" data-level="5.1" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#summary"><i class="fa fa-check"></i><b>5.1</b> Summary</a></li>
<li class="chapter" data-level="5.2" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#introduction-4"><i class="fa fa-check"></i><b>5.2</b> Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html"><a href="sparse-inverse-covariance-estimation-with-the-graphical-lasso.html#the-proposed-model"><i class="fa fa-check"></i><b>5.3</b> The Proposed Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Lasso via reversible-jump MCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 先验：标题党：</a></li>
<li class="chapter" data-level="6.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#abstract-3"><i class="fa fa-check"></i><b>6.2</b> Abstract:</a></li>
<li class="chapter" data-level="6.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#introduction-5"><i class="fa fa-check"></i><b>6.3</b> Introduction</a><ul>
<li class="chapter" data-level="6.3.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#sparse-linear-models"><i class="fa fa-check"></i><b>6.3.1</b> Sparse linear models:</a></li>
<li class="chapter" data-level="6.3.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#related-work-and-our-contribution"><i class="fa fa-check"></i><b>6.3.2</b> Related work and our contribution</a></li>
<li class="chapter" data-level="6.3.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#notations"><i class="fa fa-check"></i><b>6.3.3</b> Notations</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-fully-bayesian-lasso-model"><i class="fa fa-check"></i><b>6.4</b> A fully Bayesian Lasso model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#prior-specification"><i class="fa fa-check"></i><b>6.4.1</b> 2.1. Prior specification</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#bayesian-computation"><i class="fa fa-check"></i><b>6.5</b> Bayesian computation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#design-of-model-transition."><i class="fa fa-check"></i><b>6.5.1</b> Design of model transition.</a></li>
<li class="chapter" data-level="6.5.2" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-usual-metropolis-hastings-update-for-unchanged-model-dimension"><i class="fa fa-check"></i><b>6.5.2</b> A usual Metropolis-Hastings update for unchanged model dimension</a></li>
<li class="chapter" data-level="6.5.3" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#a-birth-and-death-strategy-for-changed-model-dimension"><i class="fa fa-check"></i><b>6.5.3</b> A birth-and-death strategy for changed model dimension</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="a-bayesian-lasso-via-reversible-jump-mcmc.html"><a href="a-bayesian-lasso-via-reversible-jump-mcmc.html#simulation"><i class="fa fa-check"></i><b>6.6</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html"><i class="fa fa-check"></i><b>7</b> The Bayesian Lasso</a><ul>
<li class="chapter" data-level="7.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#abstract-4"><i class="fa fa-check"></i><b>7.1</b> Abstract</a></li>
<li class="chapter" data-level="7.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hierarchical-model-and-gibbs-sampler"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Model and Gibbs Sampler</a></li>
<li class="chapter" data-level="7.3" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#choosing-the-bayesian-lasso-parameter"><i class="fa fa-check"></i><b>7.3</b> Choosing the Bayesian Lasso parameter</a><ul>
<li class="chapter" data-level="7.3.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>7.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
<li class="chapter" data-level="7.3.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#hyperpriors-for-the-lasso-parameter"><i class="fa fa-check"></i><b>7.3.2</b> Hyperpriors for the Lasso Parameter</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#extensions"><i class="fa fa-check"></i><b>7.4</b> Extensions</a><ul>
<li class="chapter" data-level="7.4.1" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#bridge-regression"><i class="fa fa-check"></i><b>7.4.1</b> Bridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="the-bayesian-lasso.html"><a href="the-bayesian-lasso.html#thehuberized-lasso"><i class="fa fa-check"></i><b>7.4.2</b> The“Huberized Lasso”</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html"><i class="fa fa-check"></i><b>8</b> Bayesian lasso regression</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#summary-1"><i class="fa fa-check"></i><b>8.1</b> Summary</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#introduction-6"><i class="fa fa-check"></i><b>8.2</b> Introduction</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-lasso-posterior-distribution"><i class="fa fa-check"></i><b>8.3</b> The Lasso posterior distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#direct-characterization"><i class="fa fa-check"></i><b>8.3.1</b> Direct characterization</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-based-estimation-and-prediction"><i class="fa fa-check"></i><b>8.3.2</b> Posterior-based estimation and prediction</a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-univariate-case."><i class="fa fa-check"></i><b>8.3.3</b> The univariate case.</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#posterior-inference-via-gibbs-sampling"><i class="fa fa-check"></i><b>8.4</b> Posterior Inference via Gibbs sampling</a><ul>
<li class="chapter" data-level="8.4.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-standard-gibbs-sampler."><i class="fa fa-check"></i><b>8.4.1</b> The standard Gibbs sampler.</a></li>
<li class="chapter" data-level="8.4.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#the-orthogonalized-gibbs-sampler"><i class="fa fa-check"></i><b>8.4.2</b> The orthogonalized Gibbs sampler</a></li>
<li class="chapter" data-level="8.4.3" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#comparing-samplers"><i class="fa fa-check"></i><b>8.4.3</b> Comparing samplers</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#examples"><i class="fa fa-check"></i><b>8.5</b> Examples</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example1prediction-along-the-solution-path"><i class="fa fa-check"></i><b>8.5.1</b> Example1:Prediction along the solution path</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#example2-prediction-when-modelling-lambda"><i class="fa fa-check"></i><b>8.5.2</b> Example2: Prediction when modelling <span class="math inline">\(\lambda\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-lasso-regression.html"><a href="bayesian-lasso-regression.html#discussion"><i class="fa fa-check"></i><b>8.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><i class="fa fa-check"></i><b>9</b> Bayesian analysis of joint mean and covariance models for longitudinal data</a><ul>
<li class="chapter" data-level="9.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#abstract-5"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#introduction-7"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#joint-mean-and-covariance-models"><i class="fa fa-check"></i><b>9.3</b> Joint mean and covariance models</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#bayesian-analysis-of-jmvmsjmcm"><i class="fa fa-check"></i><b>9.4</b> Bayesian analysis of JMVMs(jmcm)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#prior"><i class="fa fa-check"></i><b>9.4.1</b> Prior</a></li>
<li class="chapter" data-level="9.4.2" data-path="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html"><a href="bayesian-analysis-of-joint-mean-and-covariance-models-for-longitudinal-data.html#gibbs-sampling-and-conditional-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Gibbs sampling and conditional distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><i class="fa fa-check"></i><b>10</b> Bayesian Joint Semiparametric Mean-Covariance Modelling for Longitudinal Data</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#abstract-6"><i class="fa fa-check"></i><b>10.1</b> Abstract</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#introduction-8"><i class="fa fa-check"></i><b>10.2</b> Introduction</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models-and-bayesian-estimation-methods"><i class="fa fa-check"></i><b>10.3</b> Models and Bayesian Estimation Methods</a><ul>
<li class="chapter" data-level="10.3.1" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#models"><i class="fa fa-check"></i><b>10.3.1</b> Models</a></li>
<li class="chapter" data-level="10.3.2" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#smoothing-splines-and-priors"><i class="fa fa-check"></i><b>10.3.2</b> Smoothing Splines and Priors</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html"><a href="bayesian-joint-semiparametric-mean-covariance-modelling-for-longitudinal-data.html#mcmc-sampling"><i class="fa fa-check"></i><b>10.4</b> MCMC Sampling</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><i class="fa fa-check"></i><b>11</b> Bayesian Modeling of Joint Regressions for the Mean and Covariance Matrix</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#abstract-7"><i class="fa fa-check"></i><b>11.1</b> Abstract</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#introduction-9"><i class="fa fa-check"></i><b>11.2</b> Introduction</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#the-model"><i class="fa fa-check"></i><b>11.3</b> The model</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#bayesian-methodology"><i class="fa fa-check"></i><b>11.4</b> Bayesian Methodology</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html"><a href="bayesian-modeling-of-joint-regressions-for-the-mean-and-covariance-matrix.html#centering-and-order-methods"><i class="fa fa-check"></i><b>11.5</b> Centering and Order Methods</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><i class="fa fa-check"></i><b>12</b> MAXIMUM LIKELIHOOD ESTIMATION IN TRANSFORMED LINEAR REGRESSION WITH NONNORMAL ERRORS</a><ul>
<li class="chapter" data-level="12.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#abstract-8"><i class="fa fa-check"></i><b>12.1</b> Abstract</a></li>
<li class="chapter" data-level="12.2" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#introduction-10"><i class="fa fa-check"></i><b>12.2</b> Introduction</a></li>
<li class="chapter" data-level="12.3" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#parameter-estimation-and-inference-procedure"><i class="fa fa-check"></i><b>12.3</b> Parameter estimation and inference procedure</a><ul>
<li class="chapter" data-level="12.3.1" data-path="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html"><a href="maximum-likelihood-estimation-in-transformed-linear-regression-with-nonnormal-errors.html#mle"><i class="fa fa-check"></i><b>12.3.1</b> MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><i class="fa fa-check"></i><b>13</b> Homogeneity tests of covariance matrices with high-dimensional longitudinal data</a><ul>
<li class="chapter" data-level="13.1" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#introduction-background"><i class="fa fa-check"></i><b>13.1</b> Introduction &amp; Background</a></li>
<li class="chapter" data-level="13.2" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#basic-setting"><i class="fa fa-check"></i><b>13.2</b> Basic setting</a></li>
<li class="chapter" data-level="13.3" data-path="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html"><a href="homogeneity-tests-of-covariance-matrices-with-high-dimensional-longitudinal-data.html#homogeneity-tests-of-covariance-matrices"><i class="fa fa-check"></i><b>13.3</b> Homogeneity tests of covariance matrices</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html"><i class="fa fa-check"></i><b>14</b> Dirichlet-Laplace Priors for Optimal Shrinkage</a><ul>
<li class="chapter" data-level="14.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#abstract-9"><i class="fa fa-check"></i><b>14.1</b> Abstract</a></li>
<li class="chapter" data-level="14.2" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#introduction-11"><i class="fa fa-check"></i><b>14.2</b> Introduction</a></li>
<li class="chapter" data-level="14.3" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#a-new-class-of-shrinkage-priors"><i class="fa fa-check"></i><b>14.3</b> A NEW CLASS OF SHRINKAGE PRIORS:</a><ul>
<li class="chapter" data-level="14.3.1" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#bayesian-sparsity-priors-in-normal-means-problem"><i class="fa fa-check"></i><b>14.3.1</b> Bayesian Sparsity Priors in Normal Means Problem</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#global-local-shrinkage-rules"><i class="fa fa-check"></i><b>14.4</b> Global-Local shrinkage Rules</a></li>
<li class="chapter" data-level="14.5" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#dirichlet-kernel-priors"><i class="fa fa-check"></i><b>14.5</b> Dirichlet-Kernel Priors</a></li>
<li class="chapter" data-level="14.6" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#posterior-computation"><i class="fa fa-check"></i><b>14.6</b> Posterior Computation</a></li>
<li class="chapter" data-level="14.7" data-path="dirichlet-laplace-priors-for-optimal-shrinkage.html"><a href="dirichlet-laplace-priors-for-optimal-shrinkage.html#concentration-properties-of-dirichlet-laplace-priors"><i class="fa fa-check"></i><b>14.7</b> Concentration properties of dirichlet-laplace priors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><i class="fa fa-check"></i><b>15</b> Parsimonious Covariance Matrix Estimation for Longitudinal Data</a><ul>
<li class="chapter" data-level="15.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#abstract-10"><i class="fa fa-check"></i><b>15.1</b> Abstract</a></li>
<li class="chapter" data-level="15.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#introduction-12"><i class="fa fa-check"></i><b>15.2</b> Introduction</a></li>
<li class="chapter" data-level="15.3" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#the-model-and-prior"><i class="fa fa-check"></i><b>15.3</b> The model and prior:</a><ul>
<li class="chapter" data-level="15.3.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#likelihood"><i class="fa fa-check"></i><b>15.3.1</b> Likelihood</a></li>
<li class="chapter" data-level="15.3.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#prior-specifiction"><i class="fa fa-check"></i><b>15.3.2</b> Prior specifiction</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#inference-and-simulation-method"><i class="fa fa-check"></i><b>15.4</b> Inference and Simulation method</a><ul>
<li class="chapter" data-level="15.4.1" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#bayesian-inference."><i class="fa fa-check"></i><b>15.4.1</b> Bayesian inference.</a></li>
<li class="chapter" data-level="15.4.2" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#markov-chain-monte-carlo-sampling"><i class="fa fa-check"></i><b>15.4.2</b> Markov Chain Monte Carlo Sampling</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#simulation-analysis"><i class="fa fa-check"></i><b>15.5</b> Simulation Analysis</a></li>
<li class="chapter" data-level="15.6" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#real-data-analysis"><i class="fa fa-check"></i><b>15.6</b> Real data analysis</a></li>
<li class="chapter" data-level="15.7" data-path="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html"><a href="parsimonious-covariance-matrix-estimation-for-longitudinal-data.html#conclusion"><i class="fa fa-check"></i><b>15.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><i class="fa fa-check"></i><b>16</b> Modeling local dependence in latent vector autoregressive models</a><ul>
<li class="chapter" data-level="16.1" data-path="modeling-local-dependence-in-latent-vector-autoregressive-models.html"><a href="modeling-local-dependence-in-latent-vector-autoregressive-models.html#section-16.1"><i class="fa fa-check"></i><b>16.1</b> 英文摘要简介</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><i class="fa fa-check"></i><b>17</b> BayesVarSel: Bayesian Testing, Variable Selection and model averaging in Linear Models using R</a><ul>
<li class="chapter" data-level="17.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#movel-averaged-estimations-and-predictions"><i class="fa fa-check"></i><b>17.1</b> 6 Movel averaged estimations and predictions</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#estimation"><i class="fa fa-check"></i><b>17.1.1</b> 6.1 Estimation</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html"><a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html#prediction"><i class="fa fa-check"></i><b>17.1.2</b> 6.2 Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><i class="fa fa-check"></i><b>18</b> Criteria for Bayesian Model Choice With Application to Variable Selection</a><ul>
<li class="chapter" data-level="18.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#abstract-11"><i class="fa fa-check"></i><b>18.1</b> Abstract:</a></li>
<li class="chapter" data-level="18.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-13"><i class="fa fa-check"></i><b>18.2</b> Introduction</a><ul>
<li class="chapter" data-level="18.2.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#background"><i class="fa fa-check"></i><b>18.2.1</b> Background</a></li>
<li class="chapter" data-level="18.2.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#notation."><i class="fa fa-check"></i><b>18.2.2</b> Notation.</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#criteria-for-objective-model-selection-priors."><i class="fa fa-check"></i><b>18.3</b> Criteria for objective model selection priors.</a><ul>
<li class="chapter" data-level="18.3.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#basic-criteria"><i class="fa fa-check"></i><b>18.3.1</b> Basic criteria:</a></li>
<li class="chapter" data-level="18.3.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#consistency-criteria"><i class="fa fa-check"></i><b>18.3.2</b> Consistency criteria</a></li>
<li class="chapter" data-level="18.3.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#predictive-matching-criteria."><i class="fa fa-check"></i><b>18.3.3</b> 2.4 Predictive matching criteria.</a></li>
<li class="chapter" data-level="18.3.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#invariance-criteria"><i class="fa fa-check"></i><b>18.3.4</b> Invariance criteria</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#objective-prior-distributions-for-variable-selection-in-normal-linear-models."><i class="fa fa-check"></i><b>18.4</b> Objective prior distributions for variable selection in normal linear models.</a><ul>
<li class="chapter" data-level="18.4.1" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#introduction-14"><i class="fa fa-check"></i><b>18.4.1</b> Introduction</a></li>
<li class="chapter" data-level="18.4.2" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#proposed-prior-the-robust-prior"><i class="fa fa-check"></i><b>18.4.2</b> Proposed prior (the “robust prior”)</a></li>
<li class="chapter" data-level="18.4.3" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#choosing-the-hyperparameters-for-p_irg"><i class="fa fa-check"></i><b>18.4.3</b> Choosing the hyperparameters for <span class="math inline">\(p_i^R(g)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html"><a href="criteria-for-bayesian-model-choice-with-application-to-variable-selection.html#section-18.5"><i class="fa fa-check"></i><b>18.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><i class="fa fa-check"></i><b>19</b> Objective Bayesian Methods for Model Selection: Introduction and Comparison</a><ul>
<li class="chapter" data-level="19.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#bayes-factors"><i class="fa fa-check"></i><b>19.1</b> Bayes Factors</a><ul>
<li class="chapter" data-level="19.1.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#basic-framework"><i class="fa fa-check"></i><b>19.1.1</b> Basic Framework</a></li>
<li class="chapter" data-level="19.1.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-the-bayesian-approach-to-model-selection"><i class="fa fa-check"></i><b>19.1.2</b> Motivation for the Bayesian Approach to Model Selection</a></li>
<li class="chapter" data-level="19.1.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#utility-functions-and-prediction"><i class="fa fa-check"></i><b>19.1.3</b> Utility Functions and Prediction</a></li>
<li class="chapter" data-level="19.1.4" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#motivation-for-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.4</b> Motivation for Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.5" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#difficulties-in-objective-bayesian-model-selection"><i class="fa fa-check"></i><b>19.1.5</b> Difficulties in Objective Bayesian Model Selection</a></li>
<li class="chapter" data-level="19.1.6" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#preview"><i class="fa fa-check"></i><b>19.1.6</b> Preview</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#objective-bayesian-model-selection-methods-with-illustrations-in-the-linear-model"><i class="fa fa-check"></i><b>19.2</b> Objective Bayesian Model Selection Methods, with Illustrations in the Linear Model</a><ul>
<li class="chapter" data-level="19.2.1" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#conventional-prior-approach"><i class="fa fa-check"></i><b>19.2.1</b> Conventional Prior Approach</a></li>
<li class="chapter" data-level="19.2.2" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#intrinsic-bayes-factor-ibf-approach"><i class="fa fa-check"></i><b>19.2.2</b> 2.2 Intrinsic Bayes Factor (IBF) approach</a></li>
<li class="chapter" data-level="19.2.3" data-path="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html"><a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html#the-fractional-bayes-factor-fbf-approach"><i class="fa fa-check"></i><b>19.2.3</b> 2.3 The Fractional Bayes Factor (FBF) Approach</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><i class="fa fa-check"></i><b>20</b> A Review of Bayesian Variable Selection Methods: What, How and Which</a><ul>
<li class="chapter" data-level="20.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#abstract-12"><i class="fa fa-check"></i><b>20.1</b> Abstract</a></li>
<li class="chapter" data-level="20.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#introduction-15"><i class="fa fa-check"></i><b>20.2</b> Introduction</a><ul>
<li class="chapter" data-level="20.2.1" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#notation"><i class="fa fa-check"></i><b>20.2.1</b> Notation:</a></li>
<li class="chapter" data-level="20.2.2" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#concepts-and-properties"><i class="fa fa-check"></i><b>20.2.2</b> Concepts and Properties</a></li>
<li class="chapter" data-level="20.2.3" data-path="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html"><a href="a-review-of-bayesian-variable-selection-methods-what-how-and-which.html#variable-selection-methods"><i class="fa fa-check"></i><b>20.2.3</b> Variable Selection Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html"><i class="fa fa-check"></i><b>21</b> Marginal Likelihood From the Gibbs Output</a><ul>
<li class="chapter" data-level="21.1" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#abstract-13"><i class="fa fa-check"></i><b>21.1</b> Abstract</a></li>
<li class="chapter" data-level="21.2" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#introduction-16"><i class="fa fa-check"></i><b>21.2</b> Introduction</a></li>
<li class="chapter" data-level="21.3" data-path="marginal-likelihood-from-the-gibbs-output.html"><a href="marginal-likelihood-from-the-gibbs-output.html#notation-1"><i class="fa fa-check"></i><b>21.3</b> Notation:</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><i class="fa fa-check"></i><b>22</b> Approximate Bayesian Inference with the Weighted Likelihood Bootstrap</a><ul>
<li class="chapter" data-level="22.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#abstract-14"><i class="fa fa-check"></i><b>22.1</b> Abstract</a></li>
<li class="chapter" data-level="22.2" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#introduction-17"><i class="fa fa-check"></i><b>22.2</b> Introduction</a></li>
<li class="chapter" data-level="22.3" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#the-method"><i class="fa fa-check"></i><b>22.3</b> The Method</a></li>
<li class="chapter" data-level="22.4" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#implementation-and-examples"><i class="fa fa-check"></i><b>22.4</b> 5 Implementation and Examples</a><ul>
<li class="chapter" data-level="22.4.1" data-path="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html"><a href="approximate-bayesian-inference-with-the-weighted-likelihood-bootstrap.html#iteratively-reweighted-least-squares"><i class="fa fa-check"></i><b>22.4.1</b> Iteratively Reweighted Least Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A paper reading record</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="criteria-for-bayesian-model-choice-with-application-to-variable-selection" class="section level1">
<h1><span class="header-section-number">Paper 18</span> Criteria for Bayesian Model Choice With Application to Variable Selection</h1>
<div id="abstract-11" class="section level2">
<h2><span class="header-section-number">18.1</span> Abstract:</h2>
<p>在主观Bayesian model selection中，没有任何一个方法在决定先验分布中占主导地位。</p>
</div>
<div id="introduction-13" class="section level2">
<h2><span class="header-section-number">18.2</span> Introduction</h2>
<div id="background" class="section level3">
<h3><span class="header-section-number">18.2.1</span> Background</h3>
<p>“objective model selection priors”: not subjective prior.</p>
<p>There has been no agreement as to which are most appealing or most successful.</p>
</div>
<div id="notation." class="section level3">
<h3><span class="header-section-number">18.2.2</span> Notation.</h3>
<p>y be a data vector of size n.
<span class="math display">\[
M_{0} : f_{0}(\mathbf{y} | \boldsymbol{\alpha}), \quad M_{i} : f_{i}\left(\mathbf{y} | \boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right), \quad i=1,2, \ldots, N-1
\]</span></p>
<p><span class="math inline">\(\alpha\)</span>是intercept,<span class="math inline">\(\beta_i\)</span>是<span class="math inline">\(k_i\)</span>维的参数。
null model, prior is <span class="math inline">\(\pi_0(\alpha)\)</span>.</p>
<p>Model selection prior:
<span class="math display">\[
\pi_{i}\left(\boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right)=\pi_{i}(\boldsymbol{\alpha}) \pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}\right)
\]</span>
然后<span class="math inline">\(M_i\)</span>的后验概率可以写成:
<span class="math display">\[
\operatorname{Pr}\left(M_{i} | \mathbf{y}\right)=\frac{B_{i 0}}{1+\left(\sum_{j=1}^{N-1} B_{j 0} P_{j 0}\right)}
\]</span>
式中的<span class="math inline">\(P_{j0}\)</span>是先验odds (prior ratio) <span class="math inline">\(P_{j 0}=\operatorname{Pr}\left(M_{j}\right) / \operatorname{Pr}\left(M_{0}\right)\)</span>也就是<span class="math inline">\(M_j\)</span>的先验除以null model的先验。
<span class="math inline">\(B_{j0}\)</span>是模型<span class="math inline">\(M_j\)</span>关于<span class="math inline">\(M_0\)</span>的Bayes factor，定义是
<span class="math display">\[
B_{j 0}=\frac{m_{j}(\mathbf{y})}{m_{0}(\mathbf{y})} \quad \text { with } m_{j}(\mathbf{y})=\int f_{j}\left(\mathbf{y} | \boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right) \pi_{j}\left(\boldsymbol{\alpha}, \boldsymbol{\beta}_{j}\right) d \boldsymbol{\alpha} d \boldsymbol{\beta}_{j}
\]</span></p>
<p><span class="math inline">\(m_{0}(\mathbf{y})=\int f_{0}(\mathbf{y} | \boldsymbol{\alpha}) \pi_{0}(\boldsymbol{\alpha}) d \boldsymbol{\alpha}\)</span>是<span class="math inline">\(M_j\)</span>模型的边际似然函数，模型<span class="math inline">\(M_j\)</span>和<span class="math inline">\(M_0\)</span>对应的模型先验是<span class="math inline">\(\pi_{j}\left(\boldsymbol{\alpha}, \boldsymbol{\beta}_{j}\right)\)</span>和<span class="math inline">\(\pi_{0}(\boldsymbol{\alpha})\)</span>.
而这篇文章主要关注于讨论<span class="math inline">\(\pi_{j}\left(\boldsymbol{\alpha}, \boldsymbol{\beta}_{j}\right)\)</span>和<span class="math inline">\(\pi_{0}(\boldsymbol{\alpha})\)</span>的选取。</p>
</div>
</div>
<div id="criteria-for-objective-model-selection-priors." class="section level2">
<h2><span class="header-section-number">18.3</span> Criteria for objective model selection priors.</h2>
<p>Jeffreys’s <em>desiderate</em>: precursors to the criteria developed herein.</p>
<p>这篇文章把prior的criteria分成四类：basic,consistency criteria, predictive matching criteria and invariance criteria.</p>
<div id="basic-criteria" class="section level3">
<h3><span class="header-section-number">18.3.1</span> Basic criteria:</h3>
<p>一般来说prior for <span class="math inline">\(\beta_i\)</span>应该是proper的，因为这些只出现在Bayes factors <span class="math inline">\(B_{i0}\)</span> 的分子中，如果使用了improper prior，improper prior对应的任意常数就没法消去了，使得<span class="math inline">\(B_{i0}\)</span>不是良定的。</p>
<p>基于同样的原因，模糊的proper prior不能在<span class="math inline">\(B_{i0}\)</span>中使用。因为这个任意的模糊量会在Bayes factor中以乘的形式体现，使得Bayes factor也变成一个任意的量。
所以有以下准则：
Each conditional prior <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}\right)\)</span> must be proper(integrating to one) and cannot be arbitrarily vague in the sense of almost all of its mass being outside any believable compact set.</p>
</div>
<div id="consistency-criteria" class="section level3">
<h3><span class="header-section-number">18.3.2</span> Consistency criteria</h3>
<p>Following Liang et al(2008)考虑两个主要的consistency criteria: model selection consistency and information consistency:</p>
<p>Criterion 2(Model selection consistency) If data y have been generated by <span class="math inline">\(M_i\)</span>, then the posterior probability of <span class="math inline">\(M_i\)</span> should converge to 1 as the sample size <span class="math inline">\(n\rightarrow\infty\)</span>.</p>
<p>模型选择一致性不是特别有争议性，虽然有些时候真实模型并不是所有备选的模型中的一个。
然后有一系列文献关心这个准则：</p>
<pre><code>Fern´andez, Ley and Steel (2001),
Berger, Ghosh and Mukhopadhyay (2003), Liang et al. (2008), Casella et al.
(2009), Guo and Speckman (2009).</code></pre>
<p>Criterion 3(Information consistency): For any model <span class="math inline">\(M_i\)</span>, if <span class="math inline">\(\{y_m,m=1,...\}\)</span> is a sequence of data vectors of fixed size such that, as <span class="math inline">\(m\rightarrow\infty\)</span>,
<span class="math display">\[
\Lambda_{i 0}\left(\mathbf{y}_{m}\right)=\frac{\sup _{\boldsymbol{\alpha}, \boldsymbol{\beta}_{i}} f_{i}\left(\mathbf{y}_{m} | \boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right)}{\sup _{\boldsymbol{\alpha}} f_{0}\left(\mathbf{y}_{m} | \boldsymbol{\alpha}\right)} \rightarrow \infty \quad \text { then } B_{i 0}\left(\mathbf{y}_{m}\right) \rightarrow \infty
\]</span>
在正态误差的线性模型中，这个criteria等价于，如果考虑一系列的数据向量，对应的F(或t)统计量趋于无穷，那么Bayes factor也应该是一样的。</p>
<p>Jeffreys(1961)使用了这个思路去验证Cauchy prior测试正态的均值是0, also been highlighted in</p>
<pre><code>Berger and
Pericchi (2001), Bayarri and Garc´ıa-Donato (2008), Liang et al. (2008).</code></pre>
<p>可以构造一个Bayesian answer violates information consistency的例子，但是这个例子是非常小的sample sizes，然后先验有极端平的尾部。更多的，违反information consistency 会让频率和bayesian有非常大的冲突，可以被看成unattractive.</p>
<p>第三类consistency用来表述objective model selection priors typically depend on specific featues of the model. 比如sample size或者考虑特殊的covariates。</p>
<p>Criterion 4(Intrinsic prior consistency) Let <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}, n\right)\)</span> denote the prior for the model specific parameters of model <span class="math inline">\(M_i\)</span> with sample size n. Then as <span class="math inline">\(n\rightarrow \infty\)</span> and under suitable conditions on the evolution of the model with n, <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}, n\right)\)</span> should converge to a proper prior <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}\right)\)</span></p>
<p>思想是模型的特征和样本大小frequently affect model selection priors, 比如一些特征应该在n很大时消失。如果存在一个这样的极限先验，就叫intrinsic prior, Berger and Pericchi(2001).</p>
</div>
<div id="predictive-matching-criteria." class="section level3">
<h3><span class="header-section-number">18.3.3</span> 2.4 Predictive matching criteria.</h3>
<p>如果一个先验的量级差了2，在一维的情况不会有很大区别，但是在50维的情况下，就会导致Bayes factor上差<span class="math inline">\(2^{50}\)</span>。</p>
<p>Jeffreys 定义了 “minimal sample size” logically be unable to discriminate between two hypotheses, and argued that the prior distributions should be chosen to the yield equal marginal likelihoods for the two hypotheses.</p>
<p>如果无法区分的两个hypotheses，那么选取的先验分布在这两个hypotheses下也应该得到相等的边际似然。</p>
<p>例子：</p>
<p><span class="math display">\[
M_{1} : y \sim \frac{1}{\sigma} p_{1}\left(\frac{y-\mu}{\sigma}\right) \quad \text { and } \quad M_{2} : y \sim \frac{1}{\sigma} p_{2}\left(\frac{y-\mu}{\sigma}\right)
\]</span></p>
<p>Definition 1. The model/prior pairs <span class="math inline">\(\{M_i,\pi_i\}\)</span> and <span class="math inline">\(\{M_j,\pi_j\}\)</span> are <em>predictive matching</em> at sample size <span class="math inline">\(n^*\)</span> if the predictive distributions <span class="math inline">\(m_{i}\left(\mathbf{y}^{*}\right)\)</span> and <span class="math inline">\(m_{j}\left(\mathbf{y}^{*}\right)\)</span> are close in terms of some distance measure for data of that sample size. The model/prior pairs <span class="math inline">\(\{M_i,\pi_i\}\)</span> and <span class="math inline">\(\{M_i,\pi_i\}\)</span> and <span class="math inline">\(\{M_j,\pi_j\}\)</span> are exact predictive matching at sample size <span class="math inline">\(n^*\)</span> if <span class="math inline">\(m_{i}\left(\mathbf{y}^{*}\right)=m_{j}\left(\mathbf{y}^{*}\right)\)</span> for all <span class="math inline">\(\mathbf{y}^*\)</span> of sample size <span class="math inline">\(n^*\)</span>.</p>
<p><em>Predictive matching at sample size <span class="math inline">\(n^*\)</span>.</em>
一组模型和先验被称为 predictive matching at sample size <span class="math inline">\(n^*\)</span>,如果预测分布<span class="math inline">\(m_i(\boldsymbol y^*)\)</span> 和 <span class="math inline">\(m_j(\boldsymbol y^*)\)</span>基于此样本量，距离测度非常靠近。如果是 exact predictive matching at sample size <span class="math inline">\(n^*\)</span> 如果 <span class="math inline">\(m_{i}\left(\mathbf{y}^{*}\right)=m_{j}\left(\mathbf{y}^{*}\right)\)</span>. 也就是之前是靠的很近，exact是完全相等。</p>
<p>Definition 2 (Minimal training sample). A minimal training sample <span class="math inline">\(\boldsymbol y_i^*\)</span> for <span class="math inline">\(\{M_i,\pi_i\}\)</span> is a sample of minimal size <span class="math inline">\(n^*_i\geq 1\)</span> with a finite nonzero marginal density <span class="math inline">\(m_i(\boldsymbol y_i^*)\)</span>.</p>
<p>Definition 3 (Null predictive matching). The model selection priors are <em>null predictive matching</em> if each of the model/prior pairs <span class="math inline">\(\{M_i,\pi_i\}\)</span> and <span class="math inline">\(\{M_0,\pi_0\}\)</span> are exact predictive matching for all minimal training samples <span class="math inline">\(\boldsymbol y^*_i\)</span> for <span class="math inline">\(\{M_i,\pi_i\}\)</span></p>
<p>就是把j改成对null model成立。</p>
<p>Definition 4 (Dimensional predictive matching). The model selection priors are dimensional predictive matching if each of the model/prior pairs <span class="math inline">\(\{M_i.\pi_i\}\)</span> and <span class="math inline">\(\{M_j,\pi_j\}\)</span> of the same complexity/dimension (i.e., <span class="math inline">\(k_i=k_j\)</span>) are exact predictive matching for all minimal training samples <span class="math inline">\(\boldsymbol y_i^*\)</span> for models of that dimension.</p>
<p>只要是同样维度的模型，就match，是dimensional predictive matching所描述的问题。</p>
</div>
<div id="invariance-criteria" class="section level3">
<h3><span class="header-section-number">18.3.4</span> Invariance criteria</h3>
<p>model selection is invariance to the units of measurement being used:</p>
<p>Criterion 6(Measurement invariance). The units of measurement used for the observations or model parameters should not affect Bayesian answers.</p>
<p>More generally, model structures are invariant to group fransformations.</p>
<p>Definition 5. The family of densities for <span class="math inline">\(\mathbf{y} \in \mathbb{R}^{n}, \mathfrak{F} :=\{f(\mathbf{y} | \boldsymbol{\theta}) : \boldsymbol{\theta} \in \Theta\}\)</span> is said to be <em>invariant under the group of transformations</em> <span class="math inline">\(G :=\left\{g : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\right\}\)</span> if, for every <span class="math inline">\(g \in \mathfrak{G}\)</span> and <span class="math inline">\(\theta \in \Theta\)</span>, there exists a unique <span class="math inline">\(\theta^*\in \Theta\)</span> such that <span class="math inline">\(X=g(Y)\)</span> has density <span class="math inline">\(f\left(\mathbf{x} | \boldsymbol{\theta}^{*}\right) \in \mathfrak{F}\)</span>. In such a situation, <span class="math inline">\(\theta^*\)</span> will be denoted <span class="math inline">\(\overline g(\theta)\)</span>.</p>
<p>Criterion 7(Group invariance). If all models are invariant under a group of transformations <span class="math inline">\(G_0\)</span>, then the conditional distributions, <span class="math inline">\(\pi(\beta_i|\alpha)\)</span>, should be chosen in such a way that the conditional marginal distributions
<span class="math display">\[
f_{i}(\mathbf{y} | \boldsymbol{\alpha})=\int f_{i}\left(\mathbf{y} | \boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right) \pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}\right) d \boldsymbol{\beta}_{i}
\]</span></p>
</div>
</div>
<div id="objective-prior-distributions-for-variable-selection-in-normal-linear-models." class="section level2">
<h2><span class="header-section-number">18.4</span> Objective prior distributions for variable selection in normal linear models.</h2>
<div id="introduction-14" class="section level3">
<h3><span class="header-section-number">18.4.1</span> Introduction</h3>
<p>Variable selection in normal linear models.</p>
<p><span class="math display">\[
\begin{aligned} M_{0} : f_{0}\left(\mathbf{y} | \boldsymbol{\beta}_{0}, \sigma\right) &amp;=\mathcal{N}_{n}\left(\mathbf{y} | \mathbf{X}_{0} \boldsymbol{\beta}_{0}, \sigma^{2} \mathbf{I}\right) \\ M_{i} : f_{i}\left(\mathbf{y} | \boldsymbol{\beta}_{i}, \boldsymbol{\beta}_{0}, \sigma\right) &amp;=\mathcal{N}_{n}\left(\mathbf{y} | \mathbf{X}_{0} \boldsymbol{\beta}_{0}+\mathbf{X}_{i} \boldsymbol{\beta}_{i}, \sigma^{2} \mathbf{I}\right), \quad i=1, \ldots, 2^{p}-1 \end{aligned}
\]</span></p>
<p>Zellner and Siow (1980) common objective estimation priors for <span class="math inline">\(\alpha\)</span> and multivariate Cauchy priors for <span class="math inline">\(\pi_i(\beta_i|\alpha)\)</span>, centered at zero and with prior scale matrix <span class="math inline">\(\sigma^{2} n\left(\mathbf{X}_{i}^{\prime} \mathbf{X}_{i}\right)^{-1}\)</span>; Zellner (1986) use similar scale matrix called “g-prior”.</p>
</div>
<div id="proposed-prior-the-robust-prior" class="section level3">
<h3><span class="header-section-number">18.4.2</span> Proposed prior (the “robust prior”)</h3>
<p>Under model <span class="math inline">\(M_i\)</span>, the prior is of the form
<span class="math display">\[
\begin{aligned} \pi_{i}^{R}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right) &amp;=\pi\left(\boldsymbol{\beta}_{0}, \sigma\right) \times \pi_{i}^{R}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\beta}_{0}, \sigma\right) \\ &amp;=\sigma^{-1} \times \int_{0}^{\infty} \mathcal{N}_{k_{i}}\left(\boldsymbol{\beta}_{i} | \mathbf{0}, g \boldsymbol{\Sigma}_{i}\right) p_{i}^{R}(g) d g \end{aligned}
\]</span>
where <span class="math inline">\(\mathbf{\Sigma}_{i}=\operatorname{Cov}\left(\hat{\boldsymbol{\beta}}_{i}\right)=\sigma^{2}\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\)</span> is the covariance of the maximum likelihood estimator of <span class="math inline">\(\beta_i\)</span>, with
<span class="math display">\[
\mathbf{V}_{i}=\left(\mathbf{I}_{n}-\mathbf{X}_{0}\left(\mathbf{X}_{0}^{t} \mathbf{X}_{0}\right)^{-1} \mathbf{X}_{0}^{t}\right) \mathbf{X}_{i}
\]</span>
and
<span class="math display">\[
p_{i}^{R}(g)=a\left[\rho_{i}(b+n)\right]^{a}(g+b)^{-(a+1)} 1_{\left\{g&gt;\rho_{i}(b+n)-b\right\}}
\]</span>
<span class="math display">\[
\text { with } a&gt;0, b&gt;0 \quad \text { and } \quad \rho_{i} \geq \frac{b}{b+n}
\]</span></p>
<p>Under these condition, the <span class="math inline">\(p_i^R(g)\)</span> is a proper density, and <span class="math inline">\(g\)</span> is positive, so that <span class="math inline">\(\pi_i^R(\beta_i|\beta_0,\sigma)\)</span> is proper, satisfying the first part of the Basic criterion.</p>
<p><span class="math inline">\(p_i^R(g)\)</span>是proper prior</p>
<p><span class="math display">\[
a[\rho_i(b+n)]^a(g+b)^{-(a+1)}1_{\{g&gt;\rho_i(b+n)-b\}}
\]</span>
with <span class="math inline">\(a&gt;0,b&gt;0\)</span> and <span class="math inline">\(\rho_i\geq\frac{b}{b+n}\)</span>.
所以
<span class="math inline">\(\rho_i(b+n)-b\geq 0\)</span>
然后对g的prior进行积分的话，积分下限就从0，变成<span class="math inline">\(\rho_i(b+n)-b\geq 0\)</span>.</p>
<p><span class="math display">\[
\begin{align}
&amp;\int _{\rho_i(b+n)-b}^\infty a[\rho_i(b+n)]^a (g+b)^{-(a+1)}dg\\
=&amp; a[\rho_i(b+n)]^a \left (-\frac{1}{a}(g+b)^{-a}\right)|^{\infty}_{\rho_i(b+n)-b}\\
=&amp;a[\rho_i(b+n)]^a\left(\frac{1}{a}(\rho_i(b+n))^{-a}\right)\\
=&amp;1
\end{align}
\]</span>
所以是proper prior.
但是<span class="math inline">\(\beta_i\)</span>的积分为1应该怎么写。。。
<span class="math display">\[
\begin{align}
&amp;\int ^{\infty}_{-\infty} \sigma^{-1} \left( \int _{0}^{\infty} \mathcal{N}_{k_{i}}\left(\boldsymbol{\beta}_{i} | \mathbf{0}, g \mathbf{\Sigma}_{i}\right) p_i^R(g)dg \right)d\boldsymbol\beta_i\\
=&amp; \int ^{\infty}_{-\infty} \sigma^{-1} \left( \int _{0}^{\infty}(2\pi)^{-\frac{k_i}{2}}|g\Sigma|^{-\frac{1}{2}}exp(-\frac{1}{2}(\beta_i)^T(g\Sigma_i)^{-1}(\beta_i))\\
\cdot a\left[\rho_{i}(b+n)\right]^{a}(g+b)^{-(a+1)} 1_{\left\{g&gt;\rho_{i}(b+n)-b\right\}} dg\right)d\beta_i
\end{align}
\]</span></p>
<p>Result 1. The <em>conditional marginals</em>
<span class="math display">\[
f_{i}\left(\mathbf{y} | \boldsymbol{\beta}_{0}, \sigma\right)=\int \mathcal{N}_{n}\left(\mathbf{y} | \mathbf{X}_{0} \boldsymbol{\beta}_{0}+\mathbf{X}_{i} \boldsymbol{\beta}_{i}, \sigma^{2} \mathbf{I}\right) \pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\beta}_{0}, \sigma\right) d \boldsymbol{\beta}_{i}
\]</span>
are invariant under <span class="math inline">\(G_0\)</span> if and only if <span class="math inline">\(\pi(\beta_i|\beta_0,\sigma)\)</span> has the form:<span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\beta}_{0}, \sigma\right)=\sigma^{-k_{i}} h_{i}\left(\frac{\boldsymbol{\beta}_{i}}{\sigma}\right)\)</span>,<span class="math inline">\(h_i\)</span> is any proper density with support <span class="math inline">\(\mathcal R^k\)</span>. For example, the robust prior is a particular case with
<span class="math display">\[
h_{i}^{R}(\mathbf{u})=\int \mathcal{N}_{k_{i}}\left(\mathbf{u} | \mathbf{0}, g\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\right) p_{i}^{R}(g) d g.
\]</span>
基于Group invariance criterion, 以上结论导出了，条件on common parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\beta_i\)</span> must be scaled by <span class="math inline">\(\sigma\)</span>, centered at zero and not depend on <span class="math inline">\(\beta_0\)</span>. Robust prior 满足Group invariance criterion.</p>
<p>right-Haar density for the common parameters <span class="math inline">\((\beta_0,\sigma)\)</span>, namely
<span class="math display">\[
\pi_{i}\left(\boldsymbol{\beta}_{0}, \sigma\right)=\pi^{H}\left(\boldsymbol{\beta}_{0}, \sigma\right)=\sigma^{-1}
\]</span></p>
<p>The overall model prior would be of the form
<span class="math display">\[
\pi_{i}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right)=\sigma^{-1-k_{i}} h_{i}\left(\frac{\boldsymbol{\beta}_{i}}{\sigma}\right)
\]</span></p>
<p>Result 2. For <span class="math inline">\(M_i\)</span>, let the prior <span class="math inline">\(\pi_i(\beta_0,\beta_i,\sigma)\)</span> be of the form like previous part, where <span class="math inline">\(h_i\)</span> is symmetric about zero. Then all model/prior pairs <span class="math inline">\(\{M_i,\pi_i\}\)</span> are exact predictive matching for <span class="math inline">\(n^*=k_0+1\)</span>.</p>
<p>Group invariance criterion and Predictive matching criterion imply that model selection priors should be of the form <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right)=\sigma^{-1-k_{i}} h_{i}\left(\frac{\boldsymbol{\beta}_{i}}{\sigma}\right)\)</span>, with <span class="math inline">\(h_i\)</span> symmetric about zero. Robust prior satisfies these criteria as robust prior is clearly symmetric about zero.
任何scale of Normal mixture 也满足这些criteria, 因为 <span class="math inline">\(h(\cdot)\)</span> 是关于0对称的。</p>
<div id="justification-of-the-prior-for-the-model-specific-parameters" class="section level4">
<h4><span class="header-section-number">18.4.2.1</span> Justification of the prior for the model specific parameters</h4>
<p>Result 3. For <span class="math inline">\(M_i\)</span>, let the prior be as in <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right)=\sigma^{-1-k_{i}} h_{i}\left(\frac{\boldsymbol{\beta}_{i}}{\sigma}\right)\)</span> form, where <span class="math inline">\(h_i\)</span> is the scale mixture of normals in <span class="math inline">\(h_{i}^{R}(\mathbf{u})=\int \mathcal{N}_{k_{i}}\left(\mathbf{u} | \mathbf{0}, g\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\right) p_{i}^{R}(g) d g\)</span>. The priors are then null predictive matching and dimensional predictive matching for samples of size <span class="math inline">\(k_0+k_i\)</span>, and no choice of the conditional scale matrix other than <span class="math inline">\(\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\)</span> (or a multiple) can achieve this predictive matching.</p>
<p>这里是surprising 的结论，predictive matching result for larger sample sizes <span class="math inline">\((k_0+k_i)\)</span> than are encountered in typical predictive matching results. 这个结论只在scale matrices proportional to <span class="math inline">\(\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\)</span>时成立。</p>
</div>
</div>
<div id="choosing-the-hyperparameters-for-p_irg" class="section level3">
<h3><span class="header-section-number">18.4.3</span> Choosing the hyperparameters for <span class="math inline">\(p_i^R(g)\)</span></h3>
<div id="introduction." class="section level4">
<h4><span class="header-section-number">18.4.3.1</span> Introduction.</h4>
<p>Bayes factor of <span class="math inline">\(M_i\)</span> to <span class="math inline">\(M_0\)</span> can be expressed by
<span class="math display">\[
B_{i 0}=Q_{i 0}^{-\left(n-k_{0}\right) / 2} \frac{2 a}{k_{i}+2 a}\left[\rho_{i}(n+b)\right]^{-k_{i} / 2} \mathrm{AP}_{i}
\]</span>
in this case, <span class="math inline">\(AP_i\)</span> is the hypergeometric function of two variables, or Apell hypergeometric function
<span class="math display">\[
\mathrm{AP}_{i}=\mathrm{F}_{1}\left[a+\frac{k_{i}}{2} ; \frac{k_{i}+k_{0}-n}{2}, \frac{n-k_{0}}{2} ; a+1+\frac{k_{i}}{2} ; \frac{(b-1)}{\rho_{i}(b+n)} ; \frac{b-Q_{i 0}^{-1}}{\rho_{i}(b+n)}\right]
\]</span>
and <span class="math inline">\(Q_{i 0}=\mathrm{SSE}_{i} / \mathrm{SSE}_{0}\)</span> is the ratio of the sum of squared errors of models <span class="math inline">\(M_i\)</span> and <span class="math inline">\(M_0\)</span>.</p>
<p>有一个closed form of Bayes factor不是必须的，但是当处理<span class="math inline">\(2^p\)</span>个模型的问题时，一个closed form of Bayes factor会非常有用。</p>
<p>建议的超参数是<span class="math inline">\(a=1/2,b=1\)</span> and <span class="math inline">\(\rho_{i}=\left(k_{i}+k_{0}\right)^{-1}\)</span>.</p>
</div>
<div id="implications-of-the-consistency-criteria." class="section level4">
<h4><span class="header-section-number">18.4.3.2</span> Implications of the consistency criteria.</h4>
<p>一致性准则提供了一个值得考虑的知道关于如何选取 a,b 和 <span class="math inline">\(\rho_i\)</span>. 特别的，可以导出如下结论</p>
<p>Result 4. The three consistency criterion are satisfied by the robust prior if a and <span class="math inline">\(\rho_i\)</span> do not depend on n, <span class="math inline">\(\lim _{n \rightarrow \infty} \frac{b}{n}=c \geq 0\)</span>,<span class="math inline">\(\lim _{n \rightarrow \infty} \rho_{i}(b+n)=\infty \text { and } n \geq k_{i}+k_{0}+2 a\)</span>.</p>
<p><em>Use of model selection consistency</em></p>
<p>假设<span class="math inline">\(M_i\)</span> 是真实模型，考虑任意其他模型<span class="math inline">\(M_j\)</span>.一个关键性的假设关于模型选择一致性是，asymptotically, the design matrices are such that the models are differentiated, in the sense that
<span class="math display">\[
\lim _{n \rightarrow \infty} \frac{\boldsymbol{\beta}_{i}^{t} \mathbf{V}_{i}^{t}\left(\mathbf{I}-\mathbf{P}_{j}\right) \mathbf{V}_{i} \boldsymbol{\beta}_{i}}{n}=b_{j} \in(0, \infty)
\]</span>
where <span class="math inline">\(\mathbf{P}_{j}=\mathbf{V}_{j}\left(\mathbf{V}_{j}^{t} \mathbf{V}_{j}\right)^{-1} \mathbf{V}_{j}^{t}\)</span>.</p>
<p>Result 5. Suppose the formula upon is satisfied and that the priors <span class="math inline">\(\pi_i(\beta_0,\beta_i,\sigma)\)</span> are of the form (15), with <span class="math inline">\(h_{i}(\mathbf{u})=\int \mathcal{N}_{k_{i}}\left(\mathbf{u} | \mathbf{0}, g\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\right) p_{i}(g) d g\)</span>. If the <span class="math inline">\(p_i(g)\)</span> are proper densities such that
<span class="math display">\[
\lim _{n \rightarrow \infty} \int_{0}^{\infty}(1+g)^{-k_{i} / 2} p_{i}(g) d g=0
\]</span>
model selection consistency will result.</p>
<p>Corollary 1. The prior distributions in <span class="math inline">\(\begin{aligned} \pi_{i}^{R}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right) &amp;=\pi\left(\boldsymbol{\beta}_{0}, \sigma\right) \times \pi_{i}^{R}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\beta}_{0}, \sigma\right) \\ &amp;=\sigma^{-1} \times \int_{0}^{\infty} \mathcal{N}_{k_{i}}\left(\boldsymbol{\beta}_{i} | \mathbf{0}, g \boldsymbol{\Sigma}_{i}\right) p_{i}^{R}(g) d g \end{aligned}\)</span> are model selection consistent if
<span class="math display">\[
\lim _{n \rightarrow \infty} \rho_{i}(b+n)=\infty
\]</span></p>
</div>
<div id="choice-of-hyper-parameters" class="section level4">
<h4><span class="header-section-number">18.4.3.3</span> choice of hyper-parameters</h4>
<p>b=1 has a notable computational advantage.</p>
<p>看不下去了。以后再看吧</p>
</div>
</div>
</div>
<div id="section-18.5" class="section level2">
<h2><span class="header-section-number">18.5</span> 总结</h2>
<p>重新理解摘要：在客观贝叶斯模型选择问题中，没有哪一个基准在决定客观先验时占主导地位。事实上，很多准则分别被提出来，并且应用在寻找合适的先验上。这篇文章formalize大部分常见的准则,并且放在一起形成了新的准则。结果导出新的客观模型选择先验拥有很多良好的性质。</p>
<p>而且正常情况improper prior不能直接应用，导致无法简单的找到一个“vague proper priors”.(由于normalising constant的随意性)</p>
<p>这篇文章使用的model selection 方法基于Bayes factor，使用后验概率来衡量模型的好坏：
<span class="math display">\[
\operatorname{Pr}\left(M_{i} | \mathbf{y}\right)=\frac{B_{i 0}}{1+\left(\sum_{j=1}^{N-1} B_{j 0} P_{j 0}\right)}
\]</span></p>
<p>然后给出了几个基本准则</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>基础准则： conditional prior <span class="math inline">\(\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}\right)\)</span> 必须是proper。</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>模型选择的一致性：当sample size <span class="math inline">\(n\rightarrow \infty\)</span>时，如果数据来自模型<span class="math inline">\(M_i\)</span>,那么<span class="math inline">\(M_i\)</span>的后验概率应该收敛到1.</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>信息一致性：一个数据向量的序列<span class="math inline">\(\{\boldsymbol y_m,m=1,...\}\)</span>，维度固定，当<span class="math inline">\(m\rightarrow \infty\)</span>,有
<span class="math display">\[
\Lambda_{i 0}\left(\mathbf{y}_{m}\right)=\frac{\sup _{\boldsymbol{\alpha}, \boldsymbol{\beta}_{i}} f_{i}\left(\mathbf{y}_{m} | \boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right)}{\sup _{\boldsymbol{\alpha}} f_{0}\left(\mathbf{y}_{m} | \boldsymbol{\alpha}\right)} \rightarrow \infty \quad \text { then } B_{i 0}\left(\mathbf{y}_{m}\right) \rightarrow \infty
\]</span>
等价的来说，一系列数据对应的F或者t统计量趋向于无穷，那么Bayes factor也一样。这个准则的目的是避免Bayes方法和frequency方法有冲突。一般而言我们往往不会关心和frequency方法有冲突的先验。</li>
</ol></li>
<li><ol start="4" style="list-style-type: decimal">
<li>intrinsic prior consistency，如果一个先验和样本量n有关，那么当<span class="math inline">\(n\rightarrow\infty\)</span> 时，这个先验应该收敛到一个proper prior。</li>
</ol></li>
<li><ol start="5" style="list-style-type: decimal">
<li>Predictive matching criteria: 很复杂，简单来说我的理解就是，如果数据量较小<span class="math inline">\(&lt;n^*\)</span>,没法把两个模型分开的话，就称这两个模型在样本量<span class="math inline">\(n^*\)</span>上predictive matching at sample size <span class="math inline">\(n^*\)</span>.</li>
</ol></li>
</ul>
<p>模型选择先验应该对 appropriately defined “minimal sample size” in comparing <span class="math inline">\(M_i\)</span> with <span class="math inline">\(M_j\)</span>满足predictive matching.</p>
<ul>
<li><ol start="6" style="list-style-type: decimal">
<li>Measurement invariance: 观测的单位和模型参数的单位不应该影响Bayes选择的结论。</li>
</ol></li>
<li><ol start="7" style="list-style-type: decimal">
<li>Group invariance，如果全部模型关于一个group of transformations <span class="math inline">\(G_0\)</span> invariant,那么条件分布 <span class="math inline">\(\pi_i(\beta_i|\alpha)\)</span>用某种方式选择使得条件边际分布
<span class="math display">\[
f_{i}(\mathbf{y} | \boldsymbol{\alpha})=\int f_{i}\left(\mathbf{y} | \boldsymbol{\alpha}, \boldsymbol{\beta}_{i}\right) \pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\alpha}\right) d \boldsymbol{\beta}_{i}
\]</span>
也关于<span class="math inline">\(G_0\)</span>不变。</li>
</ol></li>
</ul>
<p>推荐的先验：或者叫“robust prior”:
有这个形式:
<span class="math display">\[
\begin{aligned} \pi_{i}^{R}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right) &amp;=\pi\left(\boldsymbol{\beta}_{0}, \sigma\right) \times \pi_{i}^{R}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\beta}_{0}, \sigma\right) \\ &amp;=\sigma^{-1} \times \int_{0}^{\infty} \mathcal{N}_{k_{i}}\left(\boldsymbol{\beta}_{i} | \mathbf{0}, g \boldsymbol{\Sigma}_{i}\right) p_{i}^{R}(g) d g \end{aligned}
\]</span>
同时也是g-prior,就是正态先验然后在方差那多个g，然后把g积掉。
<span class="math inline">\(\boldsymbol{\Sigma}_{i}=\operatorname{Cov}\left(\hat{\boldsymbol{\beta}}_{i}\right)=\sigma^{2}\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\)</span> 是covariance of the MLE for <span class="math inline">\(\beta_i\)</span> with
<span class="math display">\[
\mathbf{V}_{i}=\left(\mathbf{I}_{n}-\mathbf{X}_{0}\left(\mathbf{X}_{0}^{t} \mathbf{X}_{0}\right)^{-1} \mathbf{X}_{0}^{t}\right) \mathbf{X}_{i}
\]</span>
and
<span class="math display">\[
p_{i}^{R}(g)=a\left[\rho_{i}(b+n)\right]^{a}(g+b)^{-(a+1)} 1_{\left\{g&gt;\rho_{i}(b+n)-b\right\}}
\]</span>
with
<span class="math display">\[
a&gt;0, b&gt;0 \quad \text { and } \quad \rho_{i} \geq \frac{b}{b+n}.
\]</span></p>
<p>这个先验有很好的性质，比如其尾部表现和多元student分布一致。而先验尾部的厚度又和information consistency criteria息息相关。</p>
<p>这里提出了一个更一般的形式：
<span class="math display">\[
\pi_{i}\left(\boldsymbol{\beta}_{i} | \boldsymbol{\beta}_{0}, \sigma\right)=\sigma^{-k_{i}} h_{i}\left(\frac{\boldsymbol{\beta}_{i}}{\sigma}\right)
\]</span>
robust prior是这个形式下的一种特殊情况
<span class="math display">\[
h_{i}^{R}(\mathbf{u})=\int \mathcal{N}_{k_{i}}\left(\mathbf{u} | \mathbf{0}, g\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}\right) p_{i}^{R}(g) d g
\]</span>
而对于高斯模型的条件边际分布，是<span class="math inline">\(G_0\)</span>不变当且仅当先验有上文的那个general形式。</p>
<p>如果先验有形式：
<span class="math display">\[
\pi_{i}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right)=\sigma^{-1-k_{i}} h_{i}\left(\frac{\boldsymbol{\beta}_{i}}{\sigma}\right)
\]</span>
那么当先验有如上形式时，任意模型和先验对<span class="math inline">\(\{M_i,\pi_i\}\)</span>是exacet predictive matching for <span class="math inline">\(n^*=k_0+1\)</span>。</p>
<p>既然有那么好的形式，那另外一个问题也自然而然的产生了，对于robust prior, 超参数该如何选取？</p>
<p>对于这个问题，consistency criteria可以指导超参数的选取。</p>
<p>然后根据这个可以得出a,b,<span class="math inline">\(\rho_i\)</span>该满足的一系列条件。</p>
<p>然而有了条件还不够，到底具体的该如何选取？</p>
<p>满足一定的条件下，
<span class="math display">\[
a=1/2
\]</span>
.</p>
<p>b应该取的让<span class="math inline">\(b/n-&gt;c=0\)</span>，这样会比较flat tailed. 总的来说
<span class="math display">\[
b=1
\]</span></p>
<p><span class="math inline">\(\rho_i\)</span>的选取应该是最复杂的，</p>
<p><span class="math display">\[
\left.\rho_{i} \text { must be a constant (independent of } n\right) \text { and } \rho_{i} \geq 1 /\left(1+k_{0}+k_{i}\right)
\]</span></p>
<p>….
The choice <span class="math inline">\(\rho_{i}=1 /\left(k_{0}+k_{i}+1\right)\)</span> is the minimum value of <span class="math inline">\(\rho_i\)</span> and is, hence, certainly a candidate.</p>
<p><span class="math inline">\(\rho_i=1/(k_0+k_i)\)</span>.</p>
<p>总结，变量选择的方法：</p>
<p>先验：
<span class="math display">\[
\pi_{i}^{R}\left(\boldsymbol{\beta}_{0}, \boldsymbol{\beta}_{i}, \sigma\right)=\sigma^{-1} \times \int_{0}^{\infty} \mathcal{N}_{k_{i}}\left(\boldsymbol{\beta}_{i} | \mathbf{0}, g \mathbf{\Sigma}_{i}\right) p_{i}^{R}(g) d g
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}{\text { where } \boldsymbol{\Sigma}_{i}=\sigma^{2}\left(\mathbf{V}_{i}^{t} \mathbf{V}_{i}\right)^{-1}, \mathbf{V}_{i}=\left(\mathbf{I}_{n}-\mathbf{X}_{0}\left(\mathbf{X}_{0}^{t} \mathbf{X}_{0}\right)^{-1} \mathbf{X}_{0}^{t}\right) \mathbf{X}_{i}, \text { and }} \\ {\qquad p_{i}^{R}(g)=\frac{1}{2}\left[\frac{(1+n)}{\left(k_{i}+k_{0}\right)}\right]^{1 / 2}(g+1)^{-3 / 2} 1_{\left\{g&gt;\left(k_{i}+k_{0}\right)^{-1}(1+n)-1\right\}}}\end{array}
\]</span>
那么Bayes factor就有closed form：
<span class="math display">\[
\begin{aligned} B_{i 0}=&amp;\left[\frac{n+1}{k_{i}+k_{0}}\right]^{-k_{i} / 2} \\ &amp; \times \frac{Q_{i 0}^{-\left(n-k_{0}\right) / 2}}{k_{i}+1} _2 F_{1}\left[\frac{k_{i}+1}{2} ; \frac{n-k_{0}}{2} ; \frac{k_{i}+3}{2} ; \frac{\left(1-Q_{i 0}^{-1}\right)\left(k_{i}+k_{0}\right)}{(1+n)}\right] \end{aligned}
\]</span>
其中
<span class="math inline">\(_{2} F_{1}\)</span>是standard hypergeometric function. and <span class="math inline">\(Q_{i 0}=\mathrm{SSE}_{i} / \mathrm{SSE}_{0}\)</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesvarsel-bayesian-testing-variable-selection-and-model-averaging-in-linear-models-using-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="objective-bayesian-methods-for-model-selection-introduction-and-comparison.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["paper_reanding_record.pdf", "paper_reanding_record.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
