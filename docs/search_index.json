[
["bayesian-lasso-regression.html", "Paper 9 Bayesian lasso regression 9.1 Summary 9.2 Introduction 9.3 The Lasso posterior distribution", " Paper 9 Bayesian lasso regression (Hans 2009) 9.1 Summary 又是independent Laplace(double-exponential) prior on regression coefficient. &gt; This paper introduces new aspects of the braoder Bayesian treatment of lasso regression. It is shown that the standard lasso prediction method does not neccessarily agree with model-based, Bayesian predictions. 然后再加上new Gibbs sampler. 9.2 Introduction Lasso introduction \\[ \\hat{\\beta}_{\\mathrm{L}}=\\operatorname{argmin}_{\\beta}(y-X \\beta)^{\\mathrm{T}}(y-X \\beta)+\\lambda\\|\\beta\\|_{1} \\] Laplace prior \\[ p(\\beta | \\tau)=(\\tau / 2)^{p} \\exp \\left(-\\tau\\|\\beta\\|_{1}\\right) \\] The posterior mode of \\(\\beta\\) is the lasso estimate with penalty \\(\\lambda=2\\tau\\sigma^2\\). A new, direct characterization of the posterior distribution \\(p(\\beta|y,\\sigma^2,\\tau)\\) is introduced, along with a discussion about estimation and prediction under the lasso from a model-based perspective. The Bayesian connection to the lasso is examined, with particular attention paid to the problem of predicting future observation via the posterior prediction distribution. The direct characterization of the posterior is shown to facilitate sampling from the postrior distribution via two new Gibbs samplers that do not require the use of latent variables. 9.3 The Lasso posterior distribution 9.3.1 Direct characterization Model: \\[ \\begin{aligned} p\\left(y | \\beta, \\sigma^{2}, \\tau\\right) &amp;=\\mathrm{N}\\left(y | X \\beta, \\sigma^{2} I_{n}\\right) \\\\ p\\left(\\beta | \\tau, \\sigma^{2}\\right) &amp;=\\left(\\frac{\\tau}{2 \\sigma}\\right)^{p} \\exp \\left(\\tau \\sigma^{-1}\\|\\beta\\|_{1}\\right) \\end{aligned} \\] This prior retains the property that, for fixed \\(\\tau\\) and \\(\\sigma^2\\), the mode of \\(p\\left(\\beta | y, \\sigma^{2}, \\tau\\right)\\) is the lasso estimate with penalty parameter \\(\\lambda=2 \\tau \\sigma\\). A direct characterization of the posterior distribution \\(p\\left(\\beta | y, \\sigma^{2}, \\tau\\right)\\) does not require the inclusion of latent variables is constructed as follows. Let \\(\\mathcal{Z}=\\{-1,1\\}^{p}\\) represent the set of all \\(2^p\\) possible p-vectors whose elements are \\(\\pm 1\\). For any vector \\(z \\in \\mathcal{Z}\\), let \\(\\mathcal{O}_{z} \\subset R^{p}\\) represent the corresponding orthant: if \\(\\beta \\in \\mathcal{O}_{z}\\), then \\(\\beta_{j} \\geqslant 0\\) if \\(z_{j}=1\\) and \\(\\beta_{j}&lt;0\\) if \\(z_{j}=-1\\). 所以说\\(z_j\\)是\\(\\beta\\)正负的indicator？这么做有什么意义？orthant是什么意思？象限？这里指什么？j-维空间的元素-》对应\\(\\beta_j\\)和\\(z_j\\)吗？ Write the density function for the orthant-truncated normal distribution and its associated orthant integrals as \\[ \\mathrm{N}^{[z]}(\\beta | m, S) \\equiv \\frac{\\mathrm{N}(\\beta | m, S)}{\\mathrm{P}(z, m, S)} 1\\left(\\beta \\in \\mathcal{O}_{z}\\right), \\quad \\mathrm{P}(z, m, S)=\\int_{\\mathcal{O}_{z}} \\mathrm{N}(t | m, S) d t \\] Applying Bayes’ theorem to the lasso regression model, the posterior distribution is orthant-wise Gaussian, \\[ p\\left(\\beta | y, \\sigma^{2}, \\tau\\right)=\\sum_{z \\in \\mathcal{Z}} \\omega_{z} \\mathrm{N}^{[z]}\\left(\\beta | \\mu_{z}, \\Sigma\\right) \\] i.e. a collection of \\(2^p\\) different normal distributions that are each restricted to a different orthant. 所以这个象限确实是象限的意思看来。。。因为每个维度（数轴）由正负分成两个象限，然后这几个象限两两组合，所以\\(2^p\\) 个不同的象限 The covariance structure in each of the \\(2^p\\) orthants is the same, \\(\\Sigma=\\sigma^{2}\\left(X^{\\mathrm{T}} X\\right)^{-1}\\), whereas the location parameters depend on the orthants: \\(\\mu_{z}=\\hat{\\beta}_{\\mathrm{OLS}}-\\tau \\sigma^{-1} \\Sigma z\\), where \\(\\hat{\\beta}_{\\mathrm{O} \\mathrm{L} \\mathrm{S}}=\\left(X^{\\mathrm{T}} X\\right)^{-1} X^{\\mathrm{T}} y\\). The normalized weight for each orthant is \\[ \\omega_{z}=\\left\\{\\frac{\\mathrm{P}\\left(z, \\mu_{z}, \\Sigma\\right)}{\\mathrm{N}\\left(0 | \\mu_{z}, \\Sigma\\right)}\\right\\} /\\left\\{\\sum_{z \\in \\mathcal{Z}} \\frac{\\mathrm{P}\\left(z, \\mu_{z}, \\Sigma\\right)}{\\mathrm{N}\\left(0 | \\mu_{z}, \\Sigma\\right)}\\right\\} \\] Classic lasso prior has the similar posterior, the only change is that the location parameters become \\(\\mu_{z}=\\hat{\\beta}_{\\mathrm{ous}}-\\tau \\Sigma z\\). 感觉这个想法很牛逼但是没搞懂为啥要这么做。。。。基于orthant的分布, orthant-truncated normal distribution 如果可以计算multivariate normal orthant integrals, 后验分布可以被积出来或者sample出来当p很小的时候，那么久可以直接得到后验的inference，然后估计\\(\\sigma^2,\\tau\\)的后验均值。对于很大的p，后验估计可以通过MCMC搞定。 \\(\\sigma^{-2} \\sim \\operatorname{Ga}(a, b)\\),\\(\\tau \\sim \\operatorname{Ga}(r, s)\\). 9.3.2 Posterior-based estimation and prediction Strictly speaking, when \\(\\beta \\in R^{p}\\), this interpretation is not correct: under this loss function, the expected posterior loss is equal to one for all \\(b \\in R^{p}\\) and so any point \\(b \\in R^{p}\\) is a suitable \\(\\hat b\\). The posterior mode can, though, be interpreted as the limit of a sequence of Bayes rules. The posterior mode can, though, be interpreted as the limit of a sequence of Bayes rules. Consider the sequence of loss functions \\(l_{\\varepsilon}(b, \\beta)=1-1\\left\\{\\beta \\in B_{\\varepsilon(b)}\\right\\}\\), where the indicator function equals one if an \\(\\epsilon-\\)ball centred at \\(b\\) contains \\(\\beta\\) and is zero otherwise. In the limit as \\(\\epsilon \\rightarrow 0\\), the sequence of Bayes-optimal estimators \\(\\hat{b}_{\\varepsilon}\\) converges to the posterior mode. 这一段没懂，怎么就和0-1损失函数等价了，而且对于\\(\\beta\\in R^p\\) 为啥不成立，而且这个sequence的loss function是什么意思？要去看Bernardo&amp;Smith,2000,pp. 257-258吗。。 给定\\(\\beta\\)的点估计\\(\\hat\\beta\\),接下来要考虑的问题就是预测问题\\(\\tilde y\\) with given new value \\(\\tilde X\\),是\\(\\tilde X\\hat \\beta\\).而对于贝尔意思问题，就是考虑y的边际分布\\(p\\left(\\tilde{y} | y, \\sigma^{2}, \\tau\\right)=\\int p\\left(\\tilde{y} | y, \\beta, \\sigma^{2}, \\tau\\right) p\\left(\\beta | y, \\sigma^{2}, \\tau\\right) d \\beta\\). 对于回归问题，posterior predictive mean is \\(E\\left(\\tilde{y} | y, \\sigma^{2}, \\tau\\right)=\\tilde{X} E\\left(\\beta | y, \\sigma^{2}, \\tau\\right)\\). 所以posterior mean同时可以促进点估计和预测。（Bayes 后验这套对参数的点估计和预测都有用？） 而对于0-1损失函数这套不成立，后验估计分布的最高点不一定是\\(\\tilde{X} \\hat{\\beta}_{\\mathrm{L}}\\). 最基础的lasso预测，如果预测是在一列0-1损失函数上得出的，需要使用数值优化方法极大化\\(p\\left(\\tilde{y} | y, \\sigma^{2}, \\tau\\right)\\). 9.3.3 The univariate case. Pericchi &amp; Smith (1992)引进了beta的一种后验估计方法，单变量，intercept only， normal mean. For fixed \\(\\sigma^2\\) and \\(\\tau\\), the univariate posterior is \\[ p\\left(\\beta | y, \\sigma^{2}, \\tau\\right)=w \\mathrm{N}^{-}\\left(\\beta | \\mu_{-}, v^{2}\\right)+(1-w) \\mathrm{N}^{+}\\left(\\beta | \\mu_{+}, v^{2}\\right) \\] where \\(N^{-}\\) and \\(N^+\\) correspond to \\(N^{[z]}\\) for \\(z=-1\\) and \\(z=1\\),respectively. The common scale term is \\(v^{2}=\\sigma^{2}\\left(x^{\\mathrm{T}} x\\right)^{-1}\\), and the two location parameters are \\(\\mu_{+}=\\hat{\\beta}_{\\mathrm{ols}}-\\tau \\sigma^{-1} v^{2}\\) and \\(\\mu_{-}=\\hat{\\beta}_{\\mathrm{ols}}+\\tau \\sigma^{-1} v^{2}\\). The weight is \\[ w=\\frac{\\Phi\\left(\\frac{-\\mu_{-}}{v}\\right) / \\mathrm{N}\\left(0 | \\mu_{-}, v^{2}\\right)}{\\Phi\\left(\\frac{-\\mu_{-}}{v}\\right) / \\mathrm{N}\\left(0 | \\mu_{-}, v^{2}\\right)+\\Phi\\left(\\frac{\\mu_{+}}{v}\\right) / \\mathrm{N}\\left(0 | \\mu_{+}, v^{2}\\right)} \\] where \\(\\Phi(\\cdot)\\) is the standard normal cumulative distribution function. \\[ E\\left(\\beta | y, \\sigma^{2}, \\tau\\right)=\\hat{\\beta}_{\\mathrm{OLS}}+\\tau \\sigma^{-1} v^{2}\\{w-(1-w) \\} \\] denote as \\(\\hat\\beta_B\\).Because \\(-1 \\leqslant w-(1-w) \\leqslant 1\\), the effect of the prior on the posterior mean relative to the least-squares estimate is bounded, \\(\\left|\\hat{\\beta}_{\\mathrm{B}}-\\hat{\\beta}_{\\mathrm{O} \\mathrm{LS}}\\right| \\leqslant \\tau \\sigma\\left(x^{\\mathrm{T}} x\\right)^{-1}\\), and the bound is controlled by the amount of penalization. \\[ p\\left(\\tilde{y} | y, \\sigma^{2}, \\tau\\right)=w\\left\\{\\frac{\\Phi\\left(\\frac{-\\tilde{\\mu}_{-}}{\\tilde{v}}\\right)}{\\Phi\\left(\\frac{-\\mu_{-}}{v}\\right)} \\mathrm{N}\\left(\\tilde{y} | \\tilde{x} \\mu_{-}, \\tilde{\\sigma}^{2}\\right)\\right\\}+(1-w)\\left\\{\\frac{\\Phi\\left(\\frac{\\tilde{\\mu}_{+}}{\\tilde{v}}\\right)}{\\Phi\\left(\\frac{\\mu_{+}}{v}\\right)} \\mathrm{N}\\left(\\tilde{y} | \\tilde{x} \\mu_{+}, \\tilde{\\sigma}^{2}\\right)\\right\\} \\] where \\(\\tilde{v}^{2}=v^{2} /\\left\\{1+\\tilde{x}^{2} /\\left(x^{\\mathrm{T}} x\\right)\\right\\}, \\tilde{\\sigma}^{2}=\\sigma^{2}\\left\\{1+\\tilde{x}^{2} /\\left(x^{\\mathrm{T}} x\\right)\\right\\}\\). and \\[ \\tilde{\\mu}_{+}=\\left(\\frac{\\tilde{x} \\tilde{y}+x^{\\mathrm{T}} y-\\sigma \\tau}{\\tilde{x}^{2}+x^{\\mathrm{T}} x}\\right), \\quad \\tilde{\\mu}_{-}=\\left(\\frac{\\tilde{x} \\tilde{y}+x^{\\mathrm{T}} y+\\sigma \\tau}{\\tilde{x}^{2}+x^{\\mathrm{T}} x}\\right) \\] References "]
]
