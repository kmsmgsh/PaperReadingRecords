# Objective Bayesian Methods for Model Selection: Introduction and Comparison

## Bayes Factors
### Basic Framework
Comparing q models for the data $x$,
$$
M_{i} : \mathbf{X} \text { has density } f_{i}\left(\mathbf{x} | \boldsymbol{\theta}_{i}\right), \quad i=1, \ldots, \boldsymbol{q}
$$
The marginal or predictive densities of $\mathbf X$,
$$
m_{i}(\mathbf{x})=\int f_{i}\left(\mathbf{x} | \theta_{i}\right) \pi_{i}\left(\theta_{i}\right) d \theta_{i}
$$
The Bayes factor of $M_j$ to $M_i$ is given by
$$
B_{j i}=\frac{m_{j}(\mathbf{x})}{m_{i}(\mathbf{x})}=\frac{\int f_{j}\left(\mathbf{x} | \boldsymbol{\theta}_{j}\right) \pi_{j}\left(\boldsymbol{\theta}_{j}\right) d \boldsymbol{\theta}_{j}}{\int f_{i}\left(\mathbf{x} | \boldsymbol{\theta}_{i}\right) \pi_{i}\left(\boldsymbol{\theta}_{i}\right) d \boldsymbol{\theta}_{i}}
$$
Bayes factor is the "odds provided by the data for $M_j$ versus $M_i$."
Also called the "weighted likelihood ratio of $M_j$ to $M_i$ with priors being theh "weighting functions."

If we involve the model prior $P\left(M_{j}\right)$, then the posterior probability of model $M_i$, given the data $x$, is
$$
P\left(M_{i} | \mathbf{x}\right)=\frac{P\left(M_{i}\right) m_{i}(\mathbf{x})}{\sum_{j=1}^{q} P\left(M_{j}\right) m_{j}(\mathbf{x})}=\left[\sum_{j=1}^{q} \frac{P\left(M_{j}\right)}{P\left(M_{i}\right)} B_{j i}\right]^{-1}
$$
that is,
$$
      P\left(M_{k} | \boldsymbol{X}_{n}\right)=\frac{P\left(M_{k}\right) \int f_{k}\left(\boldsymbol{X}_{n} | \boldsymbol{\theta}_{k}\right) \pi_{k}\left(\boldsymbol{\theta}_{k}\right) d \boldsymbol{\theta}_{k}}{\sum_{\alpha=1}^{r} P\left(M_{\alpha}\right) \int f_{\alpha}\left(\boldsymbol{X}_{n} | \boldsymbol{\theta}_{\alpha}\right) \pi_{\alpha}\left(\boldsymbol{\theta}_{\alpha}\right) d \boldsymbol{\theta}_{\alpha}}
$$
which is equivelent to renormalized marginal probabilities, given by
$$
\overline{m}_{i}(\mathbf{x})=\frac{m_{i}(\mathbf{x})}{\sum_{j=1}^{q} m_{j}(\mathbf{x})}
$$

### Motivation for the Bayesian Approach to Model Selection

- Reason 1: Bayes factors and posterior model probabilities are easy to understand.
The interpretation of Bayes factors as odds and directly probabilities. Not the indirectly on p-value.

- Reason 2: Bayesian model selection is consistent.
Rather surprisingly, use of most classical model selection tools, such as p-values, $C_p$ statistics, and AIC does not guarantee consistency. If true model is not included in entertain models, Bayesian model selection will choose that model among the candidates that is closest to the true model in terms of Kullback_Leibler divergence.

- Reason 3: Bayesian model selectio procedures are automatic Ockham's razors.Bayesian procedures naturally penalize model complexity, and need no introduction of a penalty term.

- Reason 4. The Bayesian approach to model selection is conceptually the same, regardless of the number of models under consideration. 传统的frequency方法对两个模型和多个模型的比较方法不一样，比如两个模型之间的选取使用hypothesis testing，而多个则不太一样。Bayesian方法则比较一致。

- Reason 5. The Bayesian approach does not require nested models, standard distributions, or regular asymptotics. 

- Reason 6. The Bayesian approach can account for model uncertainty.
Classical approach would yield overoptimistic estimates of accuracy. So it is often recommend to use part of data to select model and rest to make estimation and prediction. Bayesian can take all the models into account rather than choose one. This is known as 'Bayesian model averaging' to handle model uncertainty.

- Reason 7. The Bayesian approach can yield optimal conditional frequentist procedures.

### Utility Functions and Prediction
Approach model selection from the perspective of decision analysis.
Bayesian model averaging is an important methodology in prediction.



Select one specific model method: the *largest posterior probability* 

### Motivation for Objective Bayesian Model Selection

Subjective & Objective Bayesian Analysis.

Subjective Bayesian analysis is attractive, but needed elicitations from subject experts.

In the case that one often initially entertains a wide variety of models, and careful subjective specification of prior distributions for all the parameters of all the models is essentially impossible.

### Difficulties in Objective Bayesian Model Selection

- Difficulty 1. Computation can be difficult. Hard to calculate Bayes factor. Need integral, could be difficult in high dimension circumstance. Total number of model under consideration can be enormous.

- Difficulty 2. When the models have parameter space of differing dimensions, use of improper noninformative priors yields indeterminate answers. 假设两个improper无信息先验$\pi_i^N$ 和 $\pi_j^N$ are entertained for models $M_i$ and $M_j$. Formal Bayes factor in this case is $B_{ji}$. If use another improper prior like $c_{i} \pi_{i}^{N} \text { and } c_{j} \pi_{j}^{N}$, then in this case Bayes factor would be $\left(c_{j} / c_{i}\right) B_{j i}$. Since the choice of $c_{j} / c_{i}$ is arbitrary, the Bayes factor is clearly indeterminate.

- Difficulty 3. Use of 'vague proper priors' usually gives bad answers in Bayesian model selection.

这里需要把例子展开写一下。

Example: Suppose we observe $\mathbf{X}=\left(X_{1}, \dots, X_{n}\right)$ where the $X_i$ are iid $\mathcal{N}(0,1)$ prior, with variance $K$ largel, this is the usual vague proper prior for a normal mean. An easy calculation using the definition of Bayes factor yields:
$$
B_{21}=(n K+1)^{-1 / 2} \exp \left(\frac{K n^{2}}{2(1+K n)} \bar{x}^{2}\right)
$$
计算过程如下：
由Bayes factor的定义：
$$
B_{j i}=\frac{m_{j}(\mathbf{x})}{m_{i}(\mathbf{x})}=\frac{\int f_{j}\left(\mathbf{x} | \boldsymbol{\theta}_{j}\right) \pi_{j}\left(\boldsymbol{\theta}_{j}\right) d \boldsymbol{\theta}_{j}}{\int f_{i}\left(\mathbf{x} | \boldsymbol{\theta}_{i}\right) \pi_{i}\left(\boldsymbol{\theta}_{i}\right) d \boldsymbol{\theta}_{i}}
$$
在这个模型中,分母是模型$M_1$,所以就是
$$
\begin{align}
f_1(x)&=\prod _{i=1}^n\frac{1}{\sqrt{2\pi}}exp(-\frac{x_i^2}{2})\\
&=\frac{1}{(2\pi)^{\frac{n}{2}}}exp(-\frac{1}{2}\sum_{i=1}^n x_i^2)
\end{align}
$$

,分子则是

$$
\begin{align}
&\int \prod_{i=1}^n \left [\frac{1}{\sqrt{2\pi}}exp(-\frac{(x_i-\theta)^2}{2})\right ]\frac{1}{\sqrt{2\pi K}}exp(-\frac{\theta^2}{2K})d\theta\\
=& \int \frac{1}{2\pi\sqrt K} exp(- \frac{K\sum _{i=1}^nx_i^2-2\theta n\overline xK+nK\theta^2+\theta^2}{2K})d\theta\\
\end{align}
$$
由于有gaussian积分的形式
$$
\int exp(-ay^2+xy)dy=\sqrt{\frac{\pi}{a}}exp(\frac{1}{4a}x^2)
$$
所以分子的积分形式有
$$
\begin{align}
& \int  \frac{1}{(2\pi)^{\frac{n+1}{2}}\sqrt K} exp(- \frac{K\sum _{i=1}^nx_i^2-2\theta n\overline xK+nK\theta^2+\theta^2}{2K})d\theta\\
=& \frac{1}{(2\pi)^{\frac{n+1}{2}}\sqrt K} exp(-\frac{1}{2} \sum _{i=1}^nx_i^2) \int exp(-\frac{-2\theta n\overline x K+(nK+1)\theta^2}{2K})d\theta\\
=& \frac{1}{(2\pi)^{\frac{n+1}{2}}  \sqrt K} exp(-\frac{1}{2} \sum _{i=1}^nx_i^2) \sqrt{\frac{2\pi K}{nK+1}}exp(\frac{Kn^2}{2(1+Kn)}\overline x^2)\\
=&  \frac{1}{(2\pi)^{\frac{n}{2}}}  exp(-\frac{1}{2} \sum _{i=1}^nx_i^2) (nK+1)^{-1/2}exp(\frac{Kn^2}{2(1+Kn)}\overline x^2)
\end{align}
$$
则Bayes factor是
$$
B_{21}=(n K+1)^{-1 / 2} \exp \left(\frac{K n^{2}}{2(1+K n)} \bar{x}^{2}\right)
$$

For large K (or large n), this is roughly $(nK)^{-1/2exp(z^2/2)}$, where $z=\sqrt n \overline x$. So $B_{21}$ depends very strongly on K, which is chosen arbitrarily.

In contrast, the usual noninformative prior for $\theta$ in this situation is $\pi^N_2(\theta)=1$. The resulting Bayes factor is $B_{21}=\sqrt{n / 2 \pi} \exp \left(z^{2} / 2\right)$, which is reasonable value. In this case, never use 'arbitrary' vague proper priors for model selection, but improper noninformative priors may give reasonable results. 

However, this is violate the basic principle of basic criteria. The "proper" prior is essential for the Bayesian factor for its scale and normalising constant problem.

- Difficulty 4. Even 'common parameters' can change meaning from one model to another, so that prior distributions must change in a corresponding fashion. Here is an example.

这个难点主要是在实践中遇到的设计阵一般都不是正交设计，有些解释变量会有相关性，这样模型的coefficient就会有不同的scale of coefficient。

比如说对于如下模型:
$$
\begin{aligned} M_{1}: Y=X_{1} \beta_{1}+\varepsilon_{1}, & \varepsilon_{1} \sim \mathcal{N}\left(0, \sigma_{1}^{2}\right) \\ M_{2}: Y=X_{1} \beta_{1}+X_{2} \beta_{2}+\varepsilon_{2}, & \varepsilon_{2} \sim \mathcal{N}\left(0, \sigma_{2}^{2}\right) \end{aligned}
$$
Y是汽油消费量，$X_1$是weight，$X_2$是engine size。
Prior density is of the form $\pi_2(\beta_1,\beta_2,\sigma_2)=\pi_{21}(\beta_1)\pi_{22}(\beta_2)\pi_{23}(\sigma_2)$. Since $\beta_1$ is 'common' to the two models, one frequently sees the same prior, $\pi_{21}(\beta_1)$ also used for this parameter in $M_1$.

However, this is not reasonable, since $\beta_1$ has a very different meaning (and value) under $M_1$ than under $M_2$. It is unreasonable to set the same prior to $beta_1$ in both model. This happens because of the considerable positive correlation between weight and engine size.

Although the design matrix is orthogonal design, it could be an issue for variance estimation.
For $\sigma_1^2$ and $\sigma_2^2$, one often sees the variances being equated and assigned the same prior, even though it is clear that $\sigma^2_1$ will typically be larger than $\sigma^2_2$.

### Preview

























### 2.3 The Fractional Bayes Factor (FBF) Approach

由于这个方法这篇文章的内容一时半会没看懂，于是去看 O'Hagan (1995)的了，为了避免未来（明后天？）写整理的时候忘掉，于是直接先写这部分的内容。

这个方法是从上一节所描述的The Intrinsic Bayes Factor approach发展而来的。为了使minimal dataset构造出的proper prior性质比较好，所以最好用一些方法取遍$x(\ell)$。比如说平均，或者中位数等等方法。但是为了简化这个选取过程 Let $b=m/n$. If both m and n are large, the likelihood $f_i(y|\theta_i)$ based only n the training sample y will approximate to the full likelihood $f_i(y|\theta_i)$ raised to the power b. 
$$
\begin{array}{l}{\qquad B_{b}(\mathbf{x})=q_{1}(b, \mathbf{x}) / q_{2}(b, \mathbf{x})} \\ {\text { where }} \\ {\qquad q_{i}(b, \mathbf{x})=\frac{\int \pi_{i}\left(\theta_{i}\right) f_{i}\left(\mathbf{x} | \theta_{i}\right) \mathrm{d} \theta_{i}}{\int \pi_{i}\left(\theta_{i}\right) f_{i}\left(\mathbf{x} | \theta_{i}\right)^{b} \mathrm{d} \theta_{i}}}\end{array} 
$$
if the improper prior have the form $\pi_{i}\left(\boldsymbol{\theta}_{i}\right)=c_{i} h_{i}\left(\boldsymbol{\theta}_{i}\right)$, the indeterminate constant $c_i$ cancels out, leaving 
$$
q_{i}(b | \mathbf{x})=\frac{\int h_{i}\left(\boldsymbol{\theta}_{i}\right) f_{i}\left(\mathbf{x} | \boldsymbol{\theta}_{i}\right) \mathrm{d} \boldsymbol{\theta}_{i}}{\int h_{i}\left(\boldsymbol{\theta}_{i}\right) f_{i}\left(\mathbf{x} | \boldsymbol{\theta}_{i}\right)^{b} \mathrm{d} \boldsymbol{\theta}_{i}}
$$

所以上标$^b$就是字面意思power b，而不是标识。





